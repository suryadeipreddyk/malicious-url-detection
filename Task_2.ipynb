{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQ0vKIp_XzkC"
      },
      "source": [
        "### **Installing Librarires**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9KcgDvvX7FM",
        "outputId": "5f138fab-d3b3-457e-e7c8-1d3cf5c242eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tld\n",
            "  Downloading tld-0.13-py2.py3-none-any.whl (263 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/263.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.7/263.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m263.8/263.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tld\n",
            "Successfully installed tld-0.13\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Installing collected packages: colorama\n",
            "Successfully installed colorama-0.4.6\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.3.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.1)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.25.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.5.3)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.10/dist-packages (from seaborn) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (0.9.0)\n"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries if not already installed\n",
        "!pip install tld\n",
        "!pip install colorama\n",
        "!pip install pandas\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install scikit-learn\n",
        "!pip install seaborn\n",
        "!pip install matplotlib\n",
        "!pip install tabulate\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPRrAzy0VuG1"
      },
      "source": [
        "### **Importing Necessary Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uaz2x0WvV1gN"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import tree\n",
        "from colorama import Fore\n",
        "from urllib.parse import urlparse\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from tld import get_tld, is_tld\n",
        "\n",
        "# Function to extract TLD from a URL\n",
        "def extract_tld(url):\n",
        "    try:\n",
        "        return get_tld(url, as_object=True).fld\n",
        "    except:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVRiyEStVHMg"
      },
      "source": [
        "### **Loading the Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0FySIe-VN27",
        "outputId": "b0ee58a8-32ed-4a13-af2b-1326073604a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "╒════╤════════════════════════════════════════════════════╤════════════╕\n",
            "│    │ url                                                │ type       │\n",
            "╞════╪════════════════════════════════════════════════════╪════════════╡\n",
            "│  0 │ br-icloud.com.br                                   │ phishing   │\n",
            "├────┼────────────────────────────────────────────────────┼────────────┤\n",
            "│  1 │ mp3raid.com/music/krizz_kaliko.html                │ benign     │\n",
            "├────┼────────────────────────────────────────────────────┼────────────┤\n",
            "│  2 │ bopsecrets.org/rexroth/cr/1.htm                    │ benign     │\n",
            "├────┼────────────────────────────────────────────────────┼────────────┤\n",
            "│  3 │ http://www.garage-pirenne.be/index.php?option=com_ │ defacement │\n",
            "├────┼────────────────────────────────────────────────────┼────────────┤\n",
            "│  4 │ http://adventure-nicaragua.net/index.php?option=co │ defacement │\n",
            "├────┼────────────────────────────────────────────────────┼────────────┤\n",
            "│  5 │ http://buzzfil.net/m/show-art/ils-etaient-loin-de- │ benign     │\n",
            "├────┼────────────────────────────────────────────────────┼────────────┤\n",
            "│  6 │ espn.go.com/nba/player/_/id/3457/brandon-rush      │ benign     │\n",
            "├────┼────────────────────────────────────────────────────┼────────────┤\n",
            "│  7 │ yourbittorrent.com/?q=anthony-hamilton-soulife     │ benign     │\n",
            "├────┼────────────────────────────────────────────────────┼────────────┤\n",
            "│  8 │ http://www.pashminaonline.com/pure-pashminas       │ defacement │\n",
            "├────┼────────────────────────────────────────────────────┼────────────┤\n",
            "│  9 │ allmusic.com/album/crazy-from-the-heat-r16990      │ benign     │\n",
            "├────┼────────────────────────────────────────────────────┼────────────┤\n",
            "│ 10 │ corporationwiki.com/Ohio/Columbus/frank-s-benson-P │ benign     │\n",
            "├────┼────────────────────────────────────────────────────┼────────────┤\n",
            "│ 11 │ http://www.ikenmijnkunst.nl/index.php/exposities/e │ defacement │\n",
            "├────┼────────────────────────────────────────────────────┼────────────┤\n",
            "│ 12 │ myspace.com/video/vid/30602581                     │ benign     │\n",
            "├────┼────────────────────────────────────────────────────┼────────────┤\n",
            "│ 13 │ http://www.lebensmittel-ueberwachung.de/index.php/ │ defacement │\n",
            "├────┼────────────────────────────────────────────────────┼────────────┤\n",
            "│ 14 │ http://www.szabadmunkaero.hu/cimoldal.html?start=1 │ defacement │\n",
            "├────┼────────────────────────────────────────────────────┼────────────┤\n",
            "│ 15 │ http://larcadelcarnevale.com/catalogo/palloncini   │ defacement │\n",
            "├────┼────────────────────────────────────────────────────┼────────────┤\n",
            "│ 16 │ quickfacts.census.gov/qfd/maps/iowa_map.html       │ benign     │\n",
            "├────┼────────────────────────────────────────────────────┼────────────┤\n",
            "│ 17 │ nugget.ca/ArticleDisplay.aspx?archive=true&e=11609 │ benign     │\n",
            "├────┼────────────────────────────────────────────────────┼────────────┤\n",
            "│ 18 │ uk.linkedin.com/pub/steve-rubenstein/8/718/755     │ benign     │\n",
            "├────┼────────────────────────────────────────────────────┼────────────┤\n",
            "│ 19 │ http://www.vnic.co/khach-hang.html                 │ defacement │\n",
            "╘════╧════════════════════════════════════════════════════╧════════════╛\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "\n",
        "'''\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "file = 'malicious_phish.csv'\n",
        "'''\n",
        "\n",
        "# Assuming the CSV file is in the specified path\n",
        "file_path = 'https://drive.google.com/uc?id=1hVTzUkdLfLcAAO7iV8EtWMduy0HrOP5b'\n",
        "\n",
        "# Read the CSV file into a Pandas DataFrame\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first 20 rows of the DataFrame\n",
        "#print(data.head(20))\n",
        "\n",
        "# Display the first 20 rows of the DataFrame with borders\n",
        "data_truncated = data.head(20).apply(lambda x: x.str[:50])  # Truncate each column to 50 characters\n",
        "print(tabulate(data_truncated, headers='keys', tablefmt='fancy_grid'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PS-sxsp399j"
      },
      "source": [
        "### **Meta information of Dataframe**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRFF9_mB4MdU",
        "outputId": "098f9a83-6f06-4f0a-eb33-ee94c14a3a82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 651191 entries, 0 to 651190\n",
            "Data columns (total 5 columns):\n",
            " #   Column      Non-Null Count   Dtype \n",
            "---  ------      --------------   ----- \n",
            " 0   url         651191 non-null  object\n",
            " 1   type        651191 non-null  object\n",
            " 2   num_dots    651191 non-null  int64 \n",
            " 3   is_bad_url  651191 non-null  bool  \n",
            " 4   url_len     651191 non-null  int64 \n",
            "dtypes: bool(1), int64(2), object(2)\n",
            "memory usage: 20.5+ MB\n"
          ]
        }
      ],
      "source": [
        "#Providing the information about the dataset we are currently using\n",
        "data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zJmwfoT8Z3e"
      },
      "source": [
        "### **Checking for Null Values**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iO7mwwmN8etU",
        "outputId": "b77bb0a2-0e06-430e-fcb1-659e17a6f10e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "url     0\n",
              "type    0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check for missing values in each column of the DataFrame and sum them up\n",
        "data.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zh3Rthr-82AL",
        "outputId": "ad30cd23-4b5f-4310-e939-1af51734f648"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "benign        428103\n",
              "defacement     96457\n",
              "phishing       94111\n",
              "malware        32520\n",
              "Name: type, dtype: int64"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Count the occurrences of each unique value in the 'type' column and store the result in the 'count' variable\n",
        "count = data.type.value_counts()\n",
        "count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1SQURYv89-m",
        "outputId": "350593a3-9767-49a7-d843-d1082f135d6c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['benign', 'defacement', 'phishing', 'malware'], dtype='object')"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Extracting the index of the 'count' Series, which represents categories or types\n",
        "x=count.index\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        },
        "id": "SaxPOWPw9CF2",
        "outputId": "35253cd2-b5ea-4b22-be74-c677dcbc9044"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-9-e13863ea6258>:5: FutureWarning: \n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n",
            "  sns.barplot(x=count.index, y=count, palette=custom_palette)\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Custom color palette with bright colors\n",
        "custom_palette = [\"#2ecc71\", \"#3498db\", \"#e74c3c\", \"#f39c12\"]\n",
        "\n",
        "# Plotting the bar plot with different colors for each category\n",
        "sns.barplot(x=count.index, y=count, palette=custom_palette)\n",
        "\n",
        "\n",
        "# Adding labels to the plot\n",
        "plt.xlabel('Types')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Bar Plot of Count for Each Category')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        },
        "id": "aU-E1S74_IJy",
        "outputId": "11286d04-99a8-4329-8cd1-8c549ab39fb2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-a868ed09-9c9e-4cc4-b3d9-d15c3cfac699\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>url</th>\n",
              "      <th>type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>br-icloud.com.br</td>\n",
              "      <td>phishing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>mp3raid.com/music/krizz_kaliko.html</td>\n",
              "      <td>benign</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>bopsecrets.org/rexroth/cr/1.htm</td>\n",
              "      <td>benign</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>http://garage-pirenne.be/index.php?option=com_...</td>\n",
              "      <td>defacement</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>http://adventure-nicaragua.net/index.php?optio...</td>\n",
              "      <td>defacement</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>651186</th>\n",
              "      <td>xbox360.ign.com/objects/850/850402.html</td>\n",
              "      <td>phishing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>651187</th>\n",
              "      <td>games.teamxbox.com/xbox-360/1860/Dead-Space/</td>\n",
              "      <td>phishing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>651188</th>\n",
              "      <td>gamespot.com/xbox360/action/deadspace/</td>\n",
              "      <td>phishing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>651189</th>\n",
              "      <td>en.wikipedia.org/wiki/Dead_Space_(video_game)</td>\n",
              "      <td>phishing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>651190</th>\n",
              "      <td>angelfire.com/goth/devilmaycrytonite/</td>\n",
              "      <td>phishing</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>651191 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a868ed09-9c9e-4cc4-b3d9-d15c3cfac699')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a868ed09-9c9e-4cc4-b3d9-d15c3cfac699 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a868ed09-9c9e-4cc4-b3d9-d15c3cfac699');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f93f968e-2522-4914-af03-2a73b8fc9a44\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f93f968e-2522-4914-af03-2a73b8fc9a44')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f93f968e-2522-4914-af03-2a73b8fc9a44 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                                      url        type\n",
              "0                                        br-icloud.com.br    phishing\n",
              "1                     mp3raid.com/music/krizz_kaliko.html      benign\n",
              "2                         bopsecrets.org/rexroth/cr/1.htm      benign\n",
              "3       http://garage-pirenne.be/index.php?option=com_...  defacement\n",
              "4       http://adventure-nicaragua.net/index.php?optio...  defacement\n",
              "...                                                   ...         ...\n",
              "651186            xbox360.ign.com/objects/850/850402.html    phishing\n",
              "651187       games.teamxbox.com/xbox-360/1860/Dead-Space/    phishing\n",
              "651188             gamespot.com/xbox360/action/deadspace/    phishing\n",
              "651189      en.wikipedia.org/wiki/Dead_Space_(video_game)    phishing\n",
              "651190              angelfire.com/goth/devilmaycrytonite/    phishing\n",
              "\n",
              "[651191 rows x 2 columns]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Remove 'www.' from the 'url' column using the replace method and a regular expression\n",
        "data['url'] = data['url'].replace('www.', '', regex=True)\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "uYweKqmWCT50",
        "outputId": "1e17c215-eb73-470a-d20b-05aa2e1fa3a9"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-40f601fb-9098-4e6e-92cd-e007f491fe55\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>url</th>\n",
              "      <th>type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>br-icloud.com.br</td>\n",
              "      <td>phishing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>mp3raid.com/music/krizz_kaliko.html</td>\n",
              "      <td>benign</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>bopsecrets.org/rexroth/cr/1.htm</td>\n",
              "      <td>benign</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>http://garage-pirenne.be/index.php?option=com_...</td>\n",
              "      <td>defacement</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>http://adventure-nicaragua.net/index.php?optio...</td>\n",
              "      <td>defacement</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>http://buzzfil.net/m/show-art/ils-etaient-loin...</td>\n",
              "      <td>benign</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>espn.go.com/nba/player/_/id/3457/brandon-rush</td>\n",
              "      <td>benign</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>yourbittorrent.com/?q=anthony-hamilton-soulife</td>\n",
              "      <td>benign</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>http://pashminaonline.com/pure-pashminas</td>\n",
              "      <td>defacement</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>allmusic.com/album/crazy-from-the-heat-r16990</td>\n",
              "      <td>benign</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-40f601fb-9098-4e6e-92cd-e007f491fe55')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-40f601fb-9098-4e6e-92cd-e007f491fe55 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-40f601fb-9098-4e6e-92cd-e007f491fe55');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-aa1bace8-fef6-415e-a949-e018dc4cc827\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-aa1bace8-fef6-415e-a949-e018dc4cc827')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-aa1bace8-fef6-415e-a949-e018dc4cc827 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                                 url        type\n",
              "0                                   br-icloud.com.br    phishing\n",
              "1                mp3raid.com/music/krizz_kaliko.html      benign\n",
              "2                    bopsecrets.org/rexroth/cr/1.htm      benign\n",
              "3  http://garage-pirenne.be/index.php?option=com_...  defacement\n",
              "4  http://adventure-nicaragua.net/index.php?optio...  defacement\n",
              "5  http://buzzfil.net/m/show-art/ils-etaient-loin...      benign\n",
              "6      espn.go.com/nba/player/_/id/3457/brandon-rush      benign\n",
              "7     yourbittorrent.com/?q=anthony-hamilton-soulife      benign\n",
              "8           http://pashminaonline.com/pure-pashminas  defacement\n",
              "9      allmusic.com/album/crazy-from-the-heat-r16990      benign"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Display the first 10 rows of the DataFrame 'data'\n",
        "data.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 825
        },
        "id": "k97ZHqfZCWHN",
        "outputId": "adc23c27-2812-423f-f8bb-7ce6db234b7c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-b57f999e-f3a3-4957-8a3f-a55b376b4697\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>url</th>\n",
              "      <th>type</th>\n",
              "      <th>Category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>br-icloud.com.br</td>\n",
              "      <td>phishing</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>mp3raid.com/music/krizz_kaliko.html</td>\n",
              "      <td>benign</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>bopsecrets.org/rexroth/cr/1.htm</td>\n",
              "      <td>benign</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>http://garage-pirenne.be/index.php?option=com_...</td>\n",
              "      <td>defacement</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>http://adventure-nicaragua.net/index.php?optio...</td>\n",
              "      <td>defacement</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>http://buzzfil.net/m/show-art/ils-etaient-loin...</td>\n",
              "      <td>benign</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>espn.go.com/nba/player/_/id/3457/brandon-rush</td>\n",
              "      <td>benign</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>yourbittorrent.com/?q=anthony-hamilton-soulife</td>\n",
              "      <td>benign</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>http://pashminaonline.com/pure-pashminas</td>\n",
              "      <td>defacement</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>allmusic.com/album/crazy-from-the-heat-r16990</td>\n",
              "      <td>benign</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>corporationwiki.com/Ohio/Columbus/frank-s-bens...</td>\n",
              "      <td>benign</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>http://ikenmijnkunst.nl/index.php/exposities/e...</td>\n",
              "      <td>defacement</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>myspace.com/video/vid/30602581</td>\n",
              "      <td>benign</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>http://lebensmittel-ueberwachung.de/index.php/...</td>\n",
              "      <td>defacement</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>http://szabadmunkaero.hu/cimoldal.html?start=12</td>\n",
              "      <td>defacement</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>http://larcadelcarnevale.com/catalogo/palloncini</td>\n",
              "      <td>defacement</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>quickfacts.census.gov/qfd/maps/iowa_map.html</td>\n",
              "      <td>benign</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>nugget.ca/ArticleDisplay.aspx?archive=true&amp;e=1...</td>\n",
              "      <td>benign</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>uk.linkedin.com/pub/steve-rubenstein/8/718/755</td>\n",
              "      <td>benign</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>http://vnic.co/khach-hang.html</td>\n",
              "      <td>defacement</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b57f999e-f3a3-4957-8a3f-a55b376b4697')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b57f999e-f3a3-4957-8a3f-a55b376b4697 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b57f999e-f3a3-4957-8a3f-a55b376b4697');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ece6a0ca-d649-416b-b0e9-e1332f7877c0\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ece6a0ca-d649-416b-b0e9-e1332f7877c0')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ece6a0ca-d649-416b-b0e9-e1332f7877c0 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                                  url        type  Category\n",
              "0                                    br-icloud.com.br    phishing         2\n",
              "1                 mp3raid.com/music/krizz_kaliko.html      benign         0\n",
              "2                     bopsecrets.org/rexroth/cr/1.htm      benign         0\n",
              "3   http://garage-pirenne.be/index.php?option=com_...  defacement         1\n",
              "4   http://adventure-nicaragua.net/index.php?optio...  defacement         1\n",
              "5   http://buzzfil.net/m/show-art/ils-etaient-loin...      benign         0\n",
              "6       espn.go.com/nba/player/_/id/3457/brandon-rush      benign         0\n",
              "7      yourbittorrent.com/?q=anthony-hamilton-soulife      benign         0\n",
              "8            http://pashminaonline.com/pure-pashminas  defacement         1\n",
              "9       allmusic.com/album/crazy-from-the-heat-r16990      benign         0\n",
              "10  corporationwiki.com/Ohio/Columbus/frank-s-bens...      benign         0\n",
              "11  http://ikenmijnkunst.nl/index.php/exposities/e...  defacement         1\n",
              "12                     myspace.com/video/vid/30602581      benign         0\n",
              "13  http://lebensmittel-ueberwachung.de/index.php/...  defacement         1\n",
              "14    http://szabadmunkaero.hu/cimoldal.html?start=12  defacement         1\n",
              "15   http://larcadelcarnevale.com/catalogo/palloncini  defacement         1\n",
              "16       quickfacts.census.gov/qfd/maps/iowa_map.html      benign         0\n",
              "17  nugget.ca/ArticleDisplay.aspx?archive=true&e=1...      benign         0\n",
              "18     uk.linkedin.com/pub/steve-rubenstein/8/718/755      benign         0\n",
              "19                     http://vnic.co/khach-hang.html  defacement         1"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Creating a mapping dictionary for the 'Category' column, assigning numerical values to corresponding categories\n",
        "rem = {\"Category\": {\"benign\": 0, \"defacement\": 1, \"phishing\": 2, \"malware\": 3}}\n",
        "\n",
        "# Creating a new column 'Category' and replacing its values based on the mapping defined in the 'rem' dictionary\n",
        "data['Category'] = data['type']\n",
        "\n",
        "# Applying the replacement using the 'replace' method\n",
        "data = data.replace(rem)\n",
        "\n",
        "# Displaying the first 20 rows of the DataFrame with the updated 'Category' column\n",
        "data.head(20)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "369S9ZD12Zhh"
      },
      "source": [
        "### **TFIDF Vectoriser**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTa11aE_Ujyh",
        "outputId": "6cef9483-0b4c-450c-b277-2a0170767f96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                 url        type  Category  \\\n",
            "0                                   br-icloud.com.br    phishing         2   \n",
            "1                mp3raid.com/music/krizz_kaliko.html      benign         0   \n",
            "2                    bopsecrets.org/rexroth/cr/1.htm      benign         0   \n",
            "3  http://garage-pirenne.be/index.php?option=com_...  defacement         1   \n",
            "4  http://adventure-nicaragua.net/index.php?optio...  defacement         1   \n",
            "5  http://buzzfil.net/m/show-art/ils-etaient-loin...      benign         0   \n",
            "6      espn.go.com/nba/player/_/id/3457/brandon-rush      benign         0   \n",
            "7     yourbittorrent.com/?q=anthony-hamilton-soulife      benign         0   \n",
            "8           http://pashminaonline.com/pure-pashminas  defacement         1   \n",
            "9      allmusic.com/album/crazy-from-the-heat-r16990      benign         0   \n",
            "\n",
            "   000webhostapp   01   02   03   04   05   06  ...  wikipedia   wn  \\\n",
            "0            0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...        0.0  0.0   \n",
            "1            0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...        0.0  0.0   \n",
            "2            0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...        0.0  0.0   \n",
            "3            0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...        0.0  0.0   \n",
            "4            0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...        0.0  0.0   \n",
            "5            0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...        0.0  0.0   \n",
            "6            0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...        0.0  0.0   \n",
            "7            0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...        0.0  0.0   \n",
            "8            0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...        0.0  0.0   \n",
            "9            0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...        0.0  0.0   \n",
            "\n",
            "   wordpress  world   wp   ws  yahoo  year  youtube   za  \n",
            "0        0.0    0.0  0.0  0.0    0.0   0.0      0.0  0.0  \n",
            "1        0.0    0.0  0.0  0.0    0.0   0.0      0.0  0.0  \n",
            "2        0.0    0.0  0.0  0.0    0.0   0.0      0.0  0.0  \n",
            "3        0.0    0.0  0.0  0.0    0.0   0.0      0.0  0.0  \n",
            "4        0.0    0.0  0.0  0.0    0.0   0.0      0.0  0.0  \n",
            "5        0.0    0.0  0.0  0.0    0.0   0.0      0.0  0.0  \n",
            "6        0.0    0.0  0.0  0.0    0.0   0.0      0.0  0.0  \n",
            "7        0.0    0.0  0.0  0.0    0.0   0.0      0.0  0.0  \n",
            "8        0.0    0.0  0.0  0.0    0.0   0.0      0.0  0.0  \n",
            "9        0.0    0.0  0.0  0.0    0.0   0.0      0.0  0.0  \n",
            "\n",
            "[10 rows x 503 columns]\n",
            "╒════╤════════════════════════════════════════════════════╤════════════╕\n",
            "│    │ url                                                │ type       │\n",
            "╞════╪════════════════════════════════════════════════════╪════════════╡\n",
            "│  0 │ br-icloud.com.br                                   │ phishing   │\n",
            "├────┼────────────────────────────────────────────────────┼────────────┤\n",
            "│  1 │ mp3raid.com/music/krizz_kaliko.html                │ benign     │\n",
            "├────┼────────────────────────────────────────────────────┼────────────┤\n",
            "│  2 │ bopsecrets.org/rexroth/cr/1.htm                    │ benign     │\n",
            "├────┼────────────────────────────────────────────────────┼────────────┤\n",
            "│  3 │ http://www.garage-pirenne.be/index.php?option=com_ │ defacement │\n",
            "├────┼────────────────────────────────────────────────────┼────────────┤\n",
            "│  4 │ http://adventure-nicaragua.net/index.php?option=co │ defacement │\n",
            "├────┼────────────────────────────────────────────────────┼────────────┤\n",
            "│  5 │ http://buzzfil.net/m/show-art/ils-etaient-loin-de- │ benign     │\n",
            "├────┼────────────────────────────────────────────────────┼────────────┤\n",
            "│  6 │ espn.go.com/nba/player/_/id/3457/brandon-rush      │ benign     │\n",
            "├────┼────────────────────────────────────────────────────┼────────────┤\n",
            "│  7 │ yourbittorrent.com/?q=anthony-hamilton-soulife     │ benign     │\n",
            "├────┼────────────────────────────────────────────────────┼────────────┤\n",
            "│  8 │ http://www.pashminaonline.com/pure-pashminas       │ defacement │\n",
            "├────┼────────────────────────────────────────────────────┼────────────┤\n",
            "│  9 │ allmusic.com/album/crazy-from-the-heat-r16990      │ benign     │\n",
            "╘════╧════════════════════════════════════════════════════╧════════════╛\n"
          ]
        }
      ],
      "source": [
        "# Importing necessary libraries for TF-IDF vectorization\n",
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Creating an instance of TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(analyzer='word', stop_words='english', max_features=500)\n",
        "\n",
        "# Applying TF-IDF vectorization to the 'url' column and converting it to a dense matrix\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(data['url']).todense()\n",
        "\n",
        "# Creating a DataFrame from the TF-IDF matrix\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix, columns=tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "# Concatenating the TF-IDF DataFrame with the original dataset\n",
        "data_with_tfidf = pd.concat([data, tfidf_df], axis=1)\n",
        "\n",
        "# Displaying the first 10 rows of the DataFrame with TF-IDF features\n",
        "print(data_with_tfidf.head(10))\n",
        "\n",
        "#This line of code is used to display the same 10 url values in a tabulated format\n",
        "print(tabulate(data_truncated.head(10), headers='keys', tablefmt='fancy_grid'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZZJQj8zpz6T",
        "outputId": "5fa92db7-4da8-4eb0-bceb-d8a04b976990"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "╒════╤════════════╤════════════╤════════════╤═════════╤══════════╤═════════╤═════════════╤════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════╤══════════╤══════════╤════════════╤═══════════╤══════════╤═══════════╤══════════════╤══════════╤═══════════╤═══════════╤══════════╤══════════╤═══════════════╤══════════════╤═════════════╤══════════╤══════════╤══════════╤═══════════╤══════════╤═════════════╤══════════╤════════════╤══════════╤══════════╤══════════╤══════════╤══════════╤══════════╤══════════╤══════════╤════════════╤══════════╤════════════════╤══════════╤══════════╤══════════╤═══════════╤══════════╤══════════╤══════════╤═════════════╤══════════╤══════════╤══════════════════╤═════════════╤══════════╤═══════════╤══════════╤══════════╤══════════╤══════════╤══════════╤═══════════╤══════════╤═══════════╤══════════╤═════════╤════════════╤══════════╤══════════════════╤══════╤════════╤══════╤═════════════╤════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════╤═════════╤══════════╤════════════╤═══════════╤═══════╤═══════════╤══════╤══════════════╤══════╤═══════════╤═══════════╤════════╤═══════╤═══════════════╤══════════════╤═════════════╤══════╤═════════╤══════╤════════╤═══════════╤═════════╤═════════════╤════════╤══════════╤══════╤════════════╤════════╤═════════╤═══════╤════════╤════════╤══════════╤══════╤═══════╤════════════╤═════════╤════════════════╤══════╤════════╤════════╤═══════════╤═════════╤═══════╤═══════╤═════════════╤══════════╤═══════╤══════════════════╤═════════════╤═══════╤═══════════╤══════════╤════════╤═════════╤═══════╤══════════╤═══════════╤════════╤════════╤═══════════╤═══════╤════════╤════════╤════════════╤═══════╤══════════════════╕\n",
            "│    │ url        │ type       │ lem_url    │      15 │     3457 │      70 │   adventure │   ahr0cdovl2fkdmvudhvyzs1uawnhcmfndweubmv0l2luzgv4lnbocd9vchrpb249y29tx2nvbnrlbnqmdmlldz1hcnrpy2xljmlkptq3omfib3v0jmnhdglkptm2omrlbw8tyxj0awnszxmmsxrlbwlkptu0 │    album │   allait │   allmusic │   anthony │      art │   article │   bopsecrets │       br │   brandon │   buzzfil │     ceci │      com │   com_content │   com_mailto │   component │       cr │    crazy │     espn │   etaient │    faire │   filmaient │   garage │   hamilton │     heat │    hibou │      htm │     html │     http │   icloud │       id │      ils │   imaginer │    index │   krizz_kaliko │       le │     link │     loin │   mp3raid │    music │      nba │      net │   nicaragua │   option │      org │   pashminaonline │   pashminas │      php │   pirenne │   player │     pure │    quand │      que │   r16990 │   rexroth │     rush │   soulife │     tmpl │    view │   vsig70_0 │      www │   yourbittorrent │   15 │   3457 │   70 │   adventure │   ahr0cdovl2fkdmvudhvyzs1uawnhcmfndweubmv0l2luzgv4lnbocd9vchrpb249y29tx2nvbnrlbnqmdmlldz1hcnrpy2xljmlkptq3omfib3v0jmnhdglkptm2omrlbw8tyxj0awnszxmmsxrlbwlkptu0 │   album │   allait │   allmusic │   anthony │   art │   article │   be │   bopsecrets │   br │   brandon │   buzzfil │   ceci │   com │   com_content │   com_mailto │   component │   cr │   crazy │   de │   espn │   etaient │   faire │   filmaient │   from │   garage │   go │   hamilton │   heat │   hibou │   htm │   html │   http │   icloud │   id │   ils │   imaginer │   index │   krizz_kaliko │   le │   link │   loin │   mp3raid │   music │   nba │   net │   nicaragua │   option │   org │   pashminaonline │   pashminas │   php │   pirenne │   player │   pure │   quand │   que │   r16990 │   rexroth │   rush │   show │   soulife │   the │   tmpl │   view │   vsig70_0 │   www │   yourbittorrent │\n",
            "╞════╪════════════╪════════════╪════════════╪═════════╪══════════╪═════════╪═════════════╪════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════╪══════════╪══════════╪════════════╪═══════════╪══════════╪═══════════╪══════════════╪══════════╪═══════════╪═══════════╪══════════╪══════════╪═══════════════╪══════════════╪═════════════╪══════════╪══════════╪══════════╪═══════════╪══════════╪═════════════╪══════════╪════════════╪══════════╪══════════╪══════════╪══════════╪══════════╪══════════╪══════════╪══════════╪════════════╪══════════╪════════════════╪══════════╪══════════╪══════════╪═══════════╪══════════╪══════════╪══════════╪═════════════╪══════════╪══════════╪══════════════════╪═════════════╪══════════╪═══════════╪══════════╪══════════╪══════════╪══════════╪══════════╪═══════════╪══════════╪═══════════╪══════════╪═════════╪════════════╪══════════╪══════════════════╪══════╪════════╪══════╪═════════════╪════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════╪═════════╪══════════╪════════════╪═══════════╪═══════╪═══════════╪══════╪══════════════╪══════╪═══════════╪═══════════╪════════╪═══════╪═══════════════╪══════════════╪═════════════╪══════╪═════════╪══════╪════════╪═══════════╪═════════╪═════════════╪════════╪══════════╪══════╪════════════╪════════╪═════════╪═══════╪════════╪════════╪══════════╪══════╪═══════╪════════════╪═════════╪════════════════╪══════╪════════╪════════╪═══════════╪═════════╪═══════╪═══════╪═════════════╪══════════╪═══════╪══════════════════╪═════════════╪═══════╪═══════════╪══════════╪════════╪═════════╪═══════╪══════════╪═══════════╪════════╪════════╪═══════════╪═══════╪════════╪════════╪════════════╪═══════╪══════════════════╡\n",
            "│  0 │ br-icloud. │ phishing   │ br-icloud. │ 0       │ 0        │ 0       │    0        │                                                                                                                                                       0        │ 0        │ 0        │   0        │  0        │ 0        │   0       │     0        │ 0.869714 │  0        │  0        │ 0        │ 0.233444 │       0       │     0        │    0        │ 0        │ 0        │ 0        │  0        │ 0        │    0        │  0       │   0        │ 0        │ 0        │ 0        │ 0        │ 0        │ 0.434857 │ 0        │ 0        │   0        │ 0        │       0        │ 0        │ 0        │ 0        │  0        │ 0        │ 0        │ 0        │    0        │ 0        │ 0        │         0        │    0        │ 0        │   0       │ 0        │ 0        │ 0        │ 0        │ 0        │  0        │ 0        │  0        │ 0        │ 0       │    0       │ 0        │         0        │    0 │      0 │    0 │           0 │                                                                                                                                                              0 │       0 │        0 │          0 │         0 │     0 │         0 │    0 │            0 │    1 │         0 │         0 │      0 │     1 │             0 │            0 │           0 │    0 │       0 │    0 │      0 │         0 │       0 │           0 │      0 │        0 │    0 │          0 │      0 │       0 │     0 │      0 │      0 │        1 │    0 │     0 │          0 │       0 │              0 │    0 │      0 │      0 │         0 │       0 │     0 │     0 │           0 │        0 │     0 │                0 │           0 │     0 │         0 │        0 │      0 │       0 │     0 │        0 │         0 │      0 │      0 │         0 │     0 │      0 │      0 │          0 │     0 │                0 │\n",
            "├────┼────────────┼────────────┼────────────┼─────────┼──────────┼─────────┼─────────────┼────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┼──────────┼──────────┼────────────┼───────────┼──────────┼───────────┼──────────────┼──────────┼───────────┼───────────┼──────────┼──────────┼───────────────┼──────────────┼─────────────┼──────────┼──────────┼──────────┼───────────┼──────────┼─────────────┼──────────┼────────────┼──────────┼──────────┼──────────┼──────────┼──────────┼──────────┼──────────┼──────────┼────────────┼──────────┼────────────────┼──────────┼──────────┼──────────┼───────────┼──────────┼──────────┼──────────┼─────────────┼──────────┼──────────┼──────────────────┼─────────────┼──────────┼───────────┼──────────┼──────────┼──────────┼──────────┼──────────┼───────────┼──────────┼───────────┼──────────┼─────────┼────────────┼──────────┼──────────────────┼──────┼────────┼──────┼─────────────┼────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┼─────────┼──────────┼────────────┼───────────┼───────┼───────────┼──────┼──────────────┼──────┼───────────┼───────────┼────────┼───────┼───────────────┼──────────────┼─────────────┼──────┼─────────┼──────┼────────┼───────────┼─────────┼─────────────┼────────┼──────────┼──────┼────────────┼────────┼─────────┼───────┼────────┼────────┼──────────┼──────┼───────┼────────────┼─────────┼────────────────┼──────┼────────┼────────┼───────────┼─────────┼───────┼───────┼─────────────┼──────────┼───────┼──────────────────┼─────────────┼───────┼───────────┼──────────┼────────┼─────────┼───────┼──────────┼───────────┼────────┼────────┼───────────┼───────┼────────┼────────┼────────────┼───────┼──────────────────┤\n",
            "│  1 │ mp3raid.co │ benign     │ mp3raid.co │ 0       │ 0        │ 0       │    0        │                                                                                                                                                       0        │ 0        │ 0        │   0        │  0        │ 0        │   0       │     0        │ 0        │  0        │  0        │ 0        │ 0.268051 │       0       │     0        │    0        │ 0        │ 0        │ 0        │  0        │ 0        │    0        │  0       │   0        │ 0        │ 0        │ 0        │ 0.424471 │ 0        │ 0        │ 0        │ 0        │   0        │ 0        │       0.499324 │ 0        │ 0        │ 0        │  0.499324 │ 0.499324 │ 0        │ 0        │    0        │ 0        │ 0        │         0        │    0        │ 0        │   0       │ 0        │ 0        │ 0        │ 0        │ 0        │  0        │ 0        │  0        │ 0        │ 0       │    0       │ 0        │         0        │    0 │      0 │    0 │           0 │                                                                                                                                                              0 │       0 │        0 │          0 │         0 │     0 │         0 │    0 │            0 │    0 │         0 │         0 │      0 │     1 │             0 │            0 │           0 │    0 │       0 │    0 │      0 │         0 │       0 │           0 │      0 │        0 │    0 │          0 │      0 │       0 │     0 │      1 │      0 │        0 │    0 │     0 │          0 │       0 │              1 │    0 │      0 │      0 │         1 │       1 │     0 │     0 │           0 │        0 │     0 │                0 │           0 │     0 │         0 │        0 │      0 │       0 │     0 │        0 │         0 │      0 │      0 │         0 │     0 │      0 │      0 │          0 │     0 │                0 │\n",
            "├────┼────────────┼────────────┼────────────┼─────────┼──────────┼─────────┼─────────────┼────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┼──────────┼──────────┼────────────┼───────────┼──────────┼───────────┼──────────────┼──────────┼───────────┼───────────┼──────────┼──────────┼───────────────┼──────────────┼─────────────┼──────────┼──────────┼──────────┼───────────┼──────────┼─────────────┼──────────┼────────────┼──────────┼──────────┼──────────┼──────────┼──────────┼──────────┼──────────┼──────────┼────────────┼──────────┼────────────────┼──────────┼──────────┼──────────┼───────────┼──────────┼──────────┼──────────┼─────────────┼──────────┼──────────┼──────────────────┼─────────────┼──────────┼───────────┼──────────┼──────────┼──────────┼──────────┼──────────┼───────────┼──────────┼───────────┼──────────┼─────────┼────────────┼──────────┼──────────────────┼──────┼────────┼──────┼─────────────┼────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┼─────────┼──────────┼────────────┼───────────┼───────┼───────────┼──────┼──────────────┼──────┼───────────┼───────────┼────────┼───────┼───────────────┼──────────────┼─────────────┼──────┼─────────┼──────┼────────┼───────────┼─────────┼─────────────┼────────┼──────────┼──────┼────────────┼────────┼─────────┼───────┼────────┼────────┼──────────┼──────┼───────┼────────────┼─────────┼────────────────┼──────┼────────┼────────┼───────────┼─────────┼───────┼───────┼─────────────┼──────────┼───────┼──────────────────┼─────────────┼───────┼───────────┼──────────┼────────┼─────────┼───────┼──────────┼───────────┼────────┼────────┼───────────┼───────┼────────┼────────┼────────────┼───────┼──────────────────┤\n",
            "│  2 │ bopsecrets │ benign     │ bopsecrets │ 0       │ 0        │ 0       │    0        │                                                                                                                                                       0        │ 0        │ 0        │   0        │  0        │ 0        │   0       │     0.447214 │ 0        │  0        │  0        │ 0        │ 0        │       0       │     0        │    0        │ 0.447214 │ 0        │ 0        │  0        │ 0        │    0        │  0       │   0        │ 0        │ 0        │ 0.447214 │ 0        │ 0        │ 0        │ 0        │ 0        │   0        │ 0        │       0        │ 0        │ 0        │ 0        │  0        │ 0        │ 0        │ 0        │    0        │ 0        │ 0.447214 │         0        │    0        │ 0        │   0       │ 0        │ 0        │ 0        │ 0        │ 0        │  0.447214 │ 0        │  0        │ 0        │ 0       │    0       │ 0        │         0        │    0 │      0 │    0 │           0 │                                                                                                                                                              0 │       0 │        0 │          0 │         0 │     0 │         0 │    0 │            1 │    0 │         0 │         0 │      0 │     0 │             0 │            0 │           0 │    1 │       0 │    0 │      0 │         0 │       0 │           0 │      0 │        0 │    0 │          0 │      0 │       0 │     1 │      0 │      0 │        0 │    0 │     0 │          0 │       0 │              0 │    0 │      0 │      0 │         0 │       0 │     0 │     0 │           0 │        0 │     1 │                0 │           0 │     0 │         0 │        0 │      0 │       0 │     0 │        0 │         1 │      0 │      0 │         0 │     0 │      0 │      0 │          0 │     0 │                0 │\n",
            "├────┼────────────┼────────────┼────────────┼─────────┼──────────┼─────────┼─────────────┼────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┼──────────┼──────────┼────────────┼───────────┼──────────┼───────────┼──────────────┼──────────┼───────────┼───────────┼──────────┼──────────┼───────────────┼──────────────┼─────────────┼──────────┼──────────┼──────────┼───────────┼──────────┼─────────────┼──────────┼────────────┼──────────┼──────────┼──────────┼──────────┼──────────┼──────────┼──────────┼──────────┼────────────┼──────────┼────────────────┼──────────┼──────────┼──────────┼───────────┼──────────┼──────────┼──────────┼─────────────┼──────────┼──────────┼──────────────────┼─────────────┼──────────┼───────────┼──────────┼──────────┼──────────┼──────────┼──────────┼───────────┼──────────┼───────────┼──────────┼─────────┼────────────┼──────────┼──────────────────┼──────┼────────┼──────┼─────────────┼────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┼─────────┼──────────┼────────────┼───────────┼───────┼───────────┼──────┼──────────────┼──────┼───────────┼───────────┼────────┼───────┼───────────────┼──────────────┼─────────────┼──────┼─────────┼──────┼────────┼───────────┼─────────┼─────────────┼────────┼──────────┼──────┼────────────┼────────┼─────────┼───────┼────────┼────────┼──────────┼──────┼───────┼────────────┼─────────┼────────────────┼──────┼────────┼────────┼───────────┼─────────┼───────┼───────┼─────────────┼──────────┼───────┼──────────────────┼─────────────┼───────┼───────────┼──────────┼────────┼─────────┼───────┼──────────┼───────────┼────────┼────────┼───────────┼───────┼────────┼────────┼────────────┼───────┼──────────────────┤\n",
            "│  3 │ http://www │ defacement │ http://www │ 0.28807 │ 0        │ 0.28807 │    0        │                                                                                                                                                       0        │ 0        │ 0        │   0        │  0        │ 0        │   0.28807 │     0        │ 0        │  0        │  0        │ 0        │ 0        │       0.28807 │     0        │    0        │ 0        │ 0        │ 0        │  0        │ 0        │    0        │  0.28807 │   0        │ 0        │ 0        │ 0        │ 0        │ 0.19048  │ 0        │ 0.244885 │ 0        │   0        │ 0.244885 │       0        │ 0        │ 0        │ 0        │  0        │ 0        │ 0        │ 0        │    0        │ 0.244885 │ 0        │         0        │    0        │ 0.244885 │   0.28807 │ 0        │ 0        │ 0        │ 0        │ 0        │  0        │ 0        │  0        │ 0        │ 0.28807 │    0.28807 │ 0.244885 │         0        │    1 │      0 │    1 │           0 │                                                                                                                                                              0 │       0 │        0 │          0 │         0 │     0 │         1 │    1 │            0 │    0 │         0 │         0 │      0 │     0 │             1 │            0 │           0 │    0 │       0 │    0 │      0 │         0 │       0 │           0 │      0 │        1 │    0 │          0 │      0 │       0 │     0 │      0 │      1 │        0 │    1 │     0 │          0 │       1 │              0 │    0 │      0 │      0 │         0 │       0 │     0 │     0 │           0 │        1 │     0 │                0 │           0 │     1 │         1 │        0 │      0 │       0 │     0 │        0 │         0 │      0 │      0 │         0 │     0 │      0 │      1 │          1 │     1 │                0 │\n",
            "├────┼────────────┼────────────┼────────────┼─────────┼──────────┼─────────┼─────────────┼────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┼──────────┼──────────┼────────────┼───────────┼──────────┼───────────┼──────────────┼──────────┼───────────┼───────────┼──────────┼──────────┼───────────────┼──────────────┼─────────────┼──────────┼──────────┼──────────┼───────────┼──────────┼─────────────┼──────────┼────────────┼──────────┼──────────┼──────────┼──────────┼──────────┼──────────┼──────────┼──────────┼────────────┼──────────┼────────────────┼──────────┼──────────┼──────────┼───────────┼──────────┼──────────┼──────────┼─────────────┼──────────┼──────────┼──────────────────┼─────────────┼──────────┼───────────┼──────────┼──────────┼──────────┼──────────┼──────────┼───────────┼──────────┼───────────┼──────────┼─────────┼────────────┼──────────┼──────────────────┼──────┼────────┼──────┼─────────────┼────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┼─────────┼──────────┼────────────┼───────────┼───────┼───────────┼──────┼──────────────┼──────┼───────────┼───────────┼────────┼───────┼───────────────┼──────────────┼─────────────┼──────┼─────────┼──────┼────────┼───────────┼─────────┼─────────────┼────────┼──────────┼──────┼────────────┼────────┼─────────┼───────┼────────┼────────┼──────────┼──────┼───────┼────────────┼─────────┼────────────────┼──────┼────────┼────────┼───────────┼─────────┼───────┼───────┼─────────────┼──────────┼───────┼──────────────────┼─────────────┼───────┼───────────┼──────────┼────────┼─────────┼───────┼──────────┼───────────┼────────┼────────┼───────────┼───────┼────────┼────────┼────────────┼───────┼──────────────────┤\n",
            "│  4 │ http://adv │ defacement │ http://adv │ 0       │ 0        │ 0       │    0.311168 │                                                                                                                                                       0.311168 │ 0        │ 0        │   0        │  0        │ 0        │   0       │     0        │ 0        │  0        │  0        │ 0        │ 0        │       0       │     0.311168 │    0.311168 │ 0        │ 0        │ 0        │  0        │ 0        │    0        │  0       │   0        │ 0        │ 0        │ 0        │ 0        │ 0.205753 │ 0        │ 0        │ 0        │   0        │ 0.264521 │       0        │ 0        │ 0.311168 │ 0        │  0        │ 0        │ 0        │ 0.264521 │    0.311168 │ 0.264521 │ 0        │         0        │    0        │ 0.264521 │   0       │ 0        │ 0        │ 0        │ 0        │ 0        │  0        │ 0        │  0        │ 0.311168 │ 0       │    0       │ 0        │         0        │    0 │      0 │    0 │           1 │                                                                                                                                                              1 │       0 │        0 │          0 │         0 │     0 │         0 │    0 │            0 │    0 │         0 │         0 │      0 │     0 │             0 │            1 │           1 │    0 │       0 │    0 │      0 │         0 │       0 │           0 │      0 │        0 │    0 │          0 │      0 │       0 │     0 │      0 │      1 │        0 │    0 │     0 │          0 │       1 │              0 │    0 │      1 │      0 │         0 │       0 │     0 │     1 │           1 │        1 │     0 │                0 │           0 │     1 │         0 │        0 │      0 │       0 │     0 │        0 │         0 │      0 │      0 │         0 │     0 │      1 │      0 │          0 │     0 │                0 │\n",
            "├────┼────────────┼────────────┼────────────┼─────────┼──────────┼─────────┼─────────────┼────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┼──────────┼──────────┼────────────┼───────────┼──────────┼───────────┼──────────────┼──────────┼───────────┼───────────┼──────────┼──────────┼───────────────┼──────────────┼─────────────┼──────────┼──────────┼──────────┼───────────┼──────────┼─────────────┼──────────┼────────────┼──────────┼──────────┼──────────┼──────────┼──────────┼──────────┼──────────┼──────────┼────────────┼──────────┼────────────────┼──────────┼──────────┼──────────┼───────────┼──────────┼──────────┼──────────┼─────────────┼──────────┼──────────┼──────────────────┼─────────────┼──────────┼───────────┼──────────┼──────────┼──────────┼──────────┼──────────┼───────────┼──────────┼───────────┼──────────┼─────────┼────────────┼──────────┼──────────────────┼──────┼────────┼──────┼─────────────┼────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┼─────────┼──────────┼────────────┼───────────┼───────┼───────────┼──────┼──────────────┼──────┼───────────┼───────────┼────────┼───────┼───────────────┼──────────────┼─────────────┼──────┼─────────┼──────┼────────┼───────────┼─────────┼─────────────┼────────┼──────────┼──────┼────────────┼────────┼─────────┼───────┼────────┼────────┼──────────┼──────┼───────┼────────────┼─────────┼────────────────┼──────┼────────┼────────┼───────────┼─────────┼───────┼───────┼─────────────┼──────────┼───────┼──────────────────┼─────────────┼───────┼───────────┼──────────┼────────┼─────────┼───────┼──────────┼───────────┼────────┼────────┼───────────┼───────┼────────┼────────┼────────────┼───────┼──────────────────┤\n",
            "│  5 │ http://buz │ benign     │ http://buz │ 0       │ 0        │ 0       │    0        │                                                                                                                                                       0        │ 0        │ 0.230128 │   0        │  0        │ 0.230128 │   0       │     0        │ 0        │  0        │  0.230128 │ 0.230128 │ 0        │       0       │     0        │    0        │ 0        │ 0        │ 0        │  0.230128 │ 0.230128 │    0.230128 │  0       │   0        │ 0        │ 0.230128 │ 0        │ 0.19563  │ 0.152167 │ 0        │ 0        │ 0.460256 │   0.230128 │ 0        │       0        │ 0.230128 │ 0        │ 0.230128 │  0        │ 0        │ 0        │ 0.19563  │    0        │ 0        │ 0        │         0        │    0        │ 0        │   0       │ 0        │ 0        │ 0.230128 │ 0.230128 │ 0        │  0        │ 0        │  0        │ 0        │ 0       │    0       │ 0        │         0        │    0 │      0 │    0 │           0 │                                                                                                                                                              0 │       0 │        1 │          0 │         0 │     1 │         0 │    0 │            0 │    0 │         0 │         1 │      1 │     0 │             0 │            0 │           0 │    0 │       0 │    1 │      0 │         1 │       1 │           1 │      0 │        0 │    0 │          0 │      0 │       1 │     0 │      1 │      1 │        0 │    0 │     1 │          1 │       0 │              0 │    1 │      0 │      1 │         0 │       0 │     0 │     1 │           0 │        0 │     0 │                0 │           0 │     0 │         0 │        0 │      0 │       1 │     1 │        0 │         0 │      0 │      1 │         0 │     0 │      0 │      0 │          0 │     0 │                0 │\n",
            "├────┼────────────┼────────────┼────────────┼─────────┼──────────┼─────────┼─────────────┼────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┼──────────┼──────────┼────────────┼───────────┼──────────┼───────────┼──────────────┼──────────┼───────────┼───────────┼──────────┼──────────┼───────────────┼──────────────┼─────────────┼──────────┼──────────┼──────────┼───────────┼──────────┼─────────────┼──────────┼────────────┼──────────┼──────────┼──────────┼──────────┼──────────┼──────────┼──────────┼──────────┼────────────┼──────────┼────────────────┼──────────┼──────────┼──────────┼───────────┼──────────┼──────────┼──────────┼─────────────┼──────────┼──────────┼──────────────────┼─────────────┼──────────┼───────────┼──────────┼──────────┼──────────┼──────────┼──────────┼───────────┼──────────┼───────────┼──────────┼─────────┼────────────┼──────────┼──────────────────┼──────┼────────┼──────┼─────────────┼────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┼─────────┼──────────┼────────────┼───────────┼───────┼───────────┼──────┼──────────────┼──────┼───────────┼───────────┼────────┼───────┼───────────────┼──────────────┼─────────────┼──────┼─────────┼──────┼────────┼───────────┼─────────┼─────────────┼────────┼──────────┼──────┼────────────┼────────┼─────────┼───────┼────────┼────────┼──────────┼──────┼───────┼────────────┼─────────┼────────────────┼──────┼────────┼────────┼───────────┼─────────┼───────┼───────┼─────────────┼──────────┼───────┼──────────────────┼─────────────┼───────┼───────────┼──────────┼────────┼─────────┼───────┼──────────┼───────────┼────────┼────────┼───────────┼───────┼────────┼────────┼────────────┼───────┼──────────────────┤\n",
            "│  6 │ espn.go.co │ benign     │ espn.go.co │ 0       │ 0.377672 │ 0       │    0        │                                                                                                                                                       0        │ 0        │ 0        │   0        │  0        │ 0        │   0       │     0        │ 0        │  0.377672 │  0        │ 0        │ 0.202745 │       0       │     0        │    0        │ 0        │ 0        │ 0.377672 │  0        │ 0        │    0        │  0       │   0        │ 0        │ 0        │ 0        │ 0        │ 0        │ 0        │ 0.321056 │ 0        │   0        │ 0        │       0        │ 0        │ 0        │ 0        │  0        │ 0        │ 0.377672 │ 0        │    0        │ 0        │ 0        │         0        │    0        │ 0        │   0       │ 0.377672 │ 0        │ 0        │ 0        │ 0        │  0        │ 0.377672 │  0        │ 0        │ 0       │    0       │ 0        │         0        │    0 │      1 │    0 │           0 │                                                                                                                                                              0 │       0 │        0 │          0 │         0 │     0 │         0 │    0 │            0 │    0 │         1 │         0 │      0 │     1 │             0 │            0 │           0 │    0 │       0 │    0 │      1 │         0 │       0 │           0 │      0 │        0 │    1 │          0 │      0 │       0 │     0 │      0 │      0 │        0 │    1 │     0 │          0 │       0 │              0 │    0 │      0 │      0 │         0 │       0 │     1 │     0 │           0 │        0 │     0 │                0 │           0 │     0 │         0 │        1 │      0 │       0 │     0 │        0 │         0 │      1 │      0 │         0 │     0 │      0 │      0 │          0 │     0 │                0 │\n",
            "├────┼────────────┼────────────┼────────────┼─────────┼──────────┼─────────┼─────────────┼────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┼──────────┼──────────┼────────────┼───────────┼──────────┼───────────┼──────────────┼──────────┼───────────┼───────────┼──────────┼──────────┼───────────────┼──────────────┼─────────────┼──────────┼──────────┼──────────┼───────────┼──────────┼─────────────┼──────────┼────────────┼──────────┼──────────┼──────────┼──────────┼──────────┼──────────┼──────────┼──────────┼────────────┼──────────┼────────────────┼──────────┼──────────┼──────────┼───────────┼──────────┼──────────┼──────────┼─────────────┼──────────┼──────────┼──────────────────┼─────────────┼──────────┼───────────┼──────────┼──────────┼──────────┼──────────┼──────────┼───────────┼──────────┼───────────┼──────────┼─────────┼────────────┼──────────┼──────────────────┼──────┼────────┼──────┼─────────────┼────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┼─────────┼──────────┼────────────┼───────────┼───────┼───────────┼──────┼──────────────┼──────┼───────────┼───────────┼────────┼───────┼───────────────┼──────────────┼─────────────┼──────┼─────────┼──────┼────────┼───────────┼─────────┼─────────────┼────────┼──────────┼──────┼────────────┼────────┼─────────┼───────┼────────┼────────┼──────────┼──────┼───────┼────────────┼─────────┼────────────────┼──────┼────────┼────────┼───────────┼─────────┼───────┼───────┼─────────────┼──────────┼───────┼──────────────────┼─────────────┼───────┼───────────┼──────────┼────────┼─────────┼───────┼──────────┼───────────┼────────┼────────┼───────────┼───────┼────────┼────────┼────────────┼───────┼──────────────────┤\n",
            "│  7 │ yourbittor │ benign     │ yourbittor │ 0       │ 0        │ 0       │    0        │                                                                                                                                                       0        │ 0        │ 0        │   0        │  0.482907 │ 0        │   0       │     0        │ 0        │  0        │  0        │ 0        │ 0.259238 │       0       │     0        │    0        │ 0        │ 0        │ 0        │  0        │ 0        │    0        │  0       │   0.482907 │ 0        │ 0        │ 0        │ 0        │ 0        │ 0        │ 0        │ 0        │   0        │ 0        │       0        │ 0        │ 0        │ 0        │  0        │ 0        │ 0        │ 0        │    0        │ 0        │ 0        │         0        │    0        │ 0        │   0       │ 0        │ 0        │ 0        │ 0        │ 0        │  0        │ 0        │  0.482907 │ 0        │ 0       │    0       │ 0        │         0.482907 │    0 │      0 │    0 │           0 │                                                                                                                                                              0 │       0 │        0 │          0 │         1 │     0 │         0 │    0 │            0 │    0 │         0 │         0 │      0 │     1 │             0 │            0 │           0 │    0 │       0 │    0 │      0 │         0 │       0 │           0 │      0 │        0 │    0 │          1 │      0 │       0 │     0 │      0 │      0 │        0 │    0 │     0 │          0 │       0 │              0 │    0 │      0 │      0 │         0 │       0 │     0 │     0 │           0 │        0 │     0 │                0 │           0 │     0 │         0 │        0 │      0 │       0 │     0 │        0 │         0 │      0 │      0 │         1 │     0 │      0 │      0 │          0 │     0 │                1 │\n",
            "├────┼────────────┼────────────┼────────────┼─────────┼──────────┼─────────┼─────────────┼────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┼──────────┼──────────┼────────────┼───────────┼──────────┼───────────┼──────────────┼──────────┼───────────┼───────────┼──────────┼──────────┼───────────────┼──────────────┼─────────────┼──────────┼──────────┼──────────┼───────────┼──────────┼─────────────┼──────────┼────────────┼──────────┼──────────┼──────────┼──────────┼──────────┼──────────┼──────────┼──────────┼────────────┼──────────┼────────────────┼──────────┼──────────┼──────────┼───────────┼──────────┼──────────┼──────────┼─────────────┼──────────┼──────────┼──────────────────┼─────────────┼──────────┼───────────┼──────────┼──────────┼──────────┼──────────┼──────────┼───────────┼──────────┼───────────┼──────────┼─────────┼────────────┼──────────┼──────────────────┼──────┼────────┼──────┼─────────────┼────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┼─────────┼──────────┼────────────┼───────────┼───────┼───────────┼──────┼──────────────┼──────┼───────────┼───────────┼────────┼───────┼───────────────┼──────────────┼─────────────┼──────┼─────────┼──────┼────────┼───────────┼─────────┼─────────────┼────────┼──────────┼──────┼────────────┼────────┼─────────┼───────┼────────┼────────┼──────────┼──────┼───────┼────────────┼─────────┼────────────────┼──────┼────────┼────────┼───────────┼─────────┼───────┼───────┼─────────────┼──────────┼───────┼──────────────────┼─────────────┼───────┼───────────┼──────────┼────────┼─────────┼───────┼──────────┼───────────┼────────┼────────┼───────────┼───────┼────────┼────────┼────────────┼───────┼──────────────────┤\n",
            "│  8 │ http://www │ defacement │ http://www │ 0       │ 0        │ 0       │    0        │                                                                                                                                                       0        │ 0        │ 0        │   0        │  0        │ 0        │   0       │     0        │ 0        │  0        │  0        │ 0        │ 0.254536 │       0       │     0        │    0        │ 0        │ 0        │ 0        │  0        │ 0        │    0        │  0       │   0        │ 0        │ 0        │ 0        │ 0        │ 0.313521 │ 0        │ 0        │ 0        │   0        │ 0        │       0        │ 0        │ 0        │ 0        │  0        │ 0        │ 0        │ 0        │    0        │ 0        │ 0        │         0.474149 │    0.474149 │ 0        │   0       │ 0        │ 0.474149 │ 0        │ 0        │ 0        │  0        │ 0        │  0        │ 0        │ 0       │    0       │ 0.40307  │         0        │    0 │      0 │    0 │           0 │                                                                                                                                                              0 │       0 │        0 │          0 │         0 │     0 │         0 │    0 │            0 │    0 │         0 │         0 │      0 │     1 │             0 │            0 │           0 │    0 │       0 │    0 │      0 │         0 │       0 │           0 │      0 │        0 │    0 │          0 │      0 │       0 │     0 │      0 │      1 │        0 │    0 │     0 │          0 │       0 │              0 │    0 │      0 │      0 │         0 │       0 │     0 │     0 │           0 │        0 │     0 │                1 │           1 │     0 │         0 │        0 │      1 │       0 │     0 │        0 │         0 │      0 │      0 │         0 │     0 │      0 │      0 │          0 │     1 │                0 │\n",
            "├────┼────────────┼────────────┼────────────┼─────────┼──────────┼─────────┼─────────────┼────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┼──────────┼──────────┼────────────┼───────────┼──────────┼───────────┼──────────────┼──────────┼───────────┼───────────┼──────────┼──────────┼───────────────┼──────────────┼─────────────┼──────────┼──────────┼──────────┼───────────┼──────────┼─────────────┼──────────┼────────────┼──────────┼──────────┼──────────┼──────────┼──────────┼──────────┼──────────┼──────────┼────────────┼──────────┼────────────────┼──────────┼──────────┼──────────┼───────────┼──────────┼──────────┼──────────┼─────────────┼──────────┼──────────┼──────────────────┼─────────────┼──────────┼───────────┼──────────┼──────────┼──────────┼──────────┼──────────┼───────────┼──────────┼───────────┼──────────┼─────────┼────────────┼──────────┼──────────────────┼──────┼────────┼──────┼─────────────┼────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┼─────────┼──────────┼────────────┼───────────┼───────┼───────────┼──────┼──────────────┼──────┼───────────┼───────────┼────────┼───────┼───────────────┼──────────────┼─────────────┼──────┼─────────┼──────┼────────┼───────────┼─────────┼─────────────┼────────┼──────────┼──────┼────────────┼────────┼─────────┼───────┼────────┼────────┼──────────┼──────┼───────┼────────────┼─────────┼────────────────┼──────┼────────┼────────┼───────────┼─────────┼───────┼───────┼─────────────┼──────────┼───────┼──────────────────┼─────────────┼───────┼───────────┼──────────┼────────┼─────────┼───────┼──────────┼───────────┼────────┼────────┼───────────┼───────┼────────┼────────┼────────────┼───────┼──────────────────┤\n",
            "│  9 │ allmusic.c │ benign     │ allmusic.c │ 0       │ 0        │ 0       │    0        │                                                                                                                                                       0        │ 0.434857 │ 0        │   0.434857 │  0        │ 0        │   0       │     0        │ 0        │  0        │  0        │ 0        │ 0.233444 │       0       │     0        │    0        │ 0        │ 0.434857 │ 0        │  0        │ 0        │    0        │  0       │   0        │ 0.434857 │ 0        │ 0        │ 0        │ 0        │ 0        │ 0        │ 0        │   0        │ 0        │       0        │ 0        │ 0        │ 0        │  0        │ 0        │ 0        │ 0        │    0        │ 0        │ 0        │         0        │    0        │ 0        │   0       │ 0        │ 0        │ 0        │ 0        │ 0.434857 │  0        │ 0        │  0        │ 0        │ 0       │    0       │ 0        │         0        │    0 │      0 │    0 │           0 │                                                                                                                                                              0 │       1 │        0 │          1 │         0 │     0 │         0 │    0 │            0 │    0 │         0 │         0 │      0 │     1 │             0 │            0 │           0 │    0 │       1 │    0 │      0 │         0 │       0 │           0 │      1 │        0 │    0 │          0 │      1 │       0 │     0 │      0 │      0 │        0 │    0 │     0 │          0 │       0 │              0 │    0 │      0 │      0 │         0 │       0 │     0 │     0 │           0 │        0 │     0 │                0 │           0 │     0 │         0 │        0 │      0 │       0 │     0 │        1 │         0 │      0 │      0 │         0 │     1 │      0 │      0 │          0 │     0 │                0 │\n",
            "╘════╧════════════╧════════════╧════════════╧═════════╧══════════╧═════════╧═════════════╧════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════╧══════════╧══════════╧════════════╧═══════════╧══════════╧═══════════╧══════════════╧══════════╧═══════════╧═══════════╧══════════╧══════════╧═══════════════╧══════════════╧═════════════╧══════════╧══════════╧══════════╧═══════════╧══════════╧═════════════╧══════════╧════════════╧══════════╧══════════╧══════════╧══════════╧══════════╧══════════╧══════════╧══════════╧════════════╧══════════╧════════════════╧══════════╧══════════╧══════════╧═══════════╧══════════╧══════════╧══════════╧═════════════╧══════════╧══════════╧══════════════════╧═════════════╧══════════╧═══════════╧══════════╧══════════╧══════════╧══════════╧══════════╧═══════════╧══════════╧═══════════╧══════════╧═════════╧════════════╧══════════╧══════════════════╧══════╧════════╧══════╧═════════════╧════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════╧═════════╧══════════╧════════════╧═══════════╧═══════╧═══════════╧══════╧══════════════╧══════╧═══════════╧═══════════╧════════╧═══════╧═══════════════╧══════════════╧═════════════╧══════╧═════════╧══════╧════════╧═══════════╧═════════╧═════════════╧════════╧══════════╧══════╧════════════╧════════╧═════════╧═══════╧════════╧════════╧══════════╧══════╧═══════╧════════════╧═════════╧════════════════╧══════╧════════╧════════╧═══════════╧═════════╧═══════╧═══════╧═════════════╧══════════╧═══════╧══════════════════╧═════════════╧═══════╧═══════════╧══════════╧════════╧═════════╧═══════╧══════════╧═══════════╧════════╧════════╧═══════════╧═══════╧════════╧════════╧════════════╧═══════╧══════════════════╛\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "from scipy.sparse import csr_matrix\n",
        "from tabulate import tabulate  # Import the tabulate function\n",
        "\n",
        "# Sample data (replace this with your actual data)\n",
        "data_url = 'https://drive.google.com/uc?id=1hVTzUkdLfLcAAO7iV8EtWMduy0HrOP5b'\n",
        "df = pd.read_csv(data_url)\n",
        "\n",
        "# Download the WordNet resource\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Instantiate WordNetLemmatizer\n",
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "# Process only the first 10 URLs\n",
        "batch_df = df.head(10).copy()\n",
        "\n",
        "# Lemmatize the 'url' column\n",
        "batch_df['lem_url'] = batch_df['url'].apply(lambda x: ' '.join([wnl.lemmatize(word) for word in x.split()]))\n",
        "\n",
        "# Applying TF-IDF vectorization to the 'url' column\n",
        "url_vectorizer = TfidfVectorizer(analyzer='word', stop_words='english', max_features=500)\n",
        "url_tfidf_matrix = url_vectorizer.fit_transform(batch_df['url'])\n",
        "url_tfidf_matrix = csr_matrix(url_tfidf_matrix)  # Convert to sparse matrix\n",
        "url_tfidf_df = pd.DataFrame(url_tfidf_matrix.toarray(), columns=url_vectorizer.get_feature_names_out())\n",
        "\n",
        "# Applying TF-IDF vectorization to the 'lem_url' column\n",
        "word_vectorizer = TfidfVectorizer(ngram_range=(1, 1), max_features=1000)\n",
        "lem_url_tfidf_matrix = word_vectorizer.fit_transform(batch_df['lem_url'])\n",
        "lem_url_tfidf_matrix = csr_matrix(lem_url_tfidf_matrix)  # Convert to sparse matrix\n",
        "lem_url_tfidf_df = pd.DataFrame(lem_url_tfidf_matrix.toarray(), columns=word_vectorizer.get_feature_names_out())\n",
        "lem_url_tfidf_df[lem_url_tfidf_df > 0] = 1  # Convert non-zero values to 1\n",
        "\n",
        "# Free up memory\n",
        "del url_tfidf_matrix, lem_url_tfidf_matrix\n",
        "\n",
        "# Concatenate the TF-IDF DataFrames with the original dataset\n",
        "batch_df_with_tfidf = pd.concat([batch_df, url_tfidf_df, lem_url_tfidf_df], axis=1)\n",
        "\n",
        "# Truncate each column to 20 characters\n",
        "data_truncated = batch_df_with_tfidf.apply(lambda x: x.astype(str).str[:10])\n",
        "\n",
        "# Display the resulting DataFrame using tabulate\n",
        "print(tabulate(data_truncated, headers='keys', tablefmt='fancy_grid'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LxPIbiDU4i4"
      },
      "source": [
        "### **Training and tesing the Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-zAAmmXDr6jd",
        "outputId": "90e41a5b-4459-4518-9f18-1dbf487e761f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch [2/5000], Training Loss: 0.9441, Training Accuracy: 17.45%\n",
            "Epoch [3/5000], Training Loss: 0.9148, Training Accuracy: 17.05%\n",
            "Epoch [4/5000], Training Loss: 0.8868, Training Accuracy: 17.66%\n",
            "Epoch [5/5000], Training Loss: 0.8602, Training Accuracy: 18.89%\n",
            "Epoch [6/5000], Training Loss: 0.8348, Training Accuracy: 19.80%\n",
            "Epoch [7/5000], Training Loss: 0.8107, Training Accuracy: 20.97%\n",
            "Epoch [8/5000], Training Loss: 0.7878, Training Accuracy: 22.96%\n",
            "Epoch [9/5000], Training Loss: 0.7660, Training Accuracy: 26.32%\n",
            "Epoch [10/5000], Training Loss: 0.7453, Training Accuracy: 35.63%\n",
            "Epoch [11/5000], Training Loss: 0.7257, Training Accuracy: 48.19%\n",
            "Epoch [12/5000], Training Loss: 0.7070, Training Accuracy: 48.38%\n",
            "Epoch [13/5000], Training Loss: 0.6893, Training Accuracy: 48.80%\n",
            "Epoch [14/5000], Training Loss: 0.6725, Training Accuracy: 49.06%\n",
            "Epoch [15/5000], Training Loss: 0.6565, Training Accuracy: 49.45%\n",
            "Epoch [16/5000], Training Loss: 0.6414, Training Accuracy: 49.48%\n",
            "Epoch [17/5000], Training Loss: 0.6269, Training Accuracy: 71.32%\n",
            "Epoch [18/5000], Training Loss: 0.6132, Training Accuracy: 78.68%\n",
            "Epoch [19/5000], Training Loss: 0.6002, Training Accuracy: 80.94%\n",
            "Epoch [20/5000], Training Loss: 0.5878, Training Accuracy: 80.38%\n",
            "Epoch [21/5000], Training Loss: 0.5760, Training Accuracy: 79.90%\n",
            "Epoch [22/5000], Training Loss: 0.5647, Training Accuracy: 79.73%\n",
            "Epoch [23/5000], Training Loss: 0.5540, Training Accuracy: 79.55%\n",
            "Epoch [24/5000], Training Loss: 0.5438, Training Accuracy: 79.37%\n",
            "Epoch [25/5000], Training Loss: 0.5341, Training Accuracy: 79.26%\n",
            "Epoch [26/5000], Training Loss: 0.5247, Training Accuracy: 79.20%\n",
            "Epoch [27/5000], Training Loss: 0.5159, Training Accuracy: 79.84%\n",
            "Epoch [28/5000], Training Loss: 0.5074, Training Accuracy: 80.88%\n",
            "Epoch [29/5000], Training Loss: 0.4992, Training Accuracy: 82.40%\n",
            "Epoch [30/5000], Training Loss: 0.4915, Training Accuracy: 83.66%\n",
            "Epoch [31/5000], Training Loss: 0.4840, Training Accuracy: 85.04%\n",
            "Epoch [32/5000], Training Loss: 0.4769, Training Accuracy: 85.92%\n",
            "Epoch [33/5000], Training Loss: 0.4701, Training Accuracy: 86.53%\n",
            "Epoch [34/5000], Training Loss: 0.4635, Training Accuracy: 87.44%\n",
            "Epoch [35/5000], Training Loss: 0.4572, Training Accuracy: 87.90%\n",
            "Epoch [36/5000], Training Loss: 0.4512, Training Accuracy: 88.36%\n",
            "Epoch [37/5000], Training Loss: 0.4454, Training Accuracy: 88.58%\n",
            "Epoch [38/5000], Training Loss: 0.4398, Training Accuracy: 88.99%\n",
            "Epoch [39/5000], Training Loss: 0.4344, Training Accuracy: 89.08%\n",
            "Epoch [40/5000], Training Loss: 0.4293, Training Accuracy: 89.47%\n",
            "Epoch [41/5000], Training Loss: 0.4243, Training Accuracy: 89.66%\n",
            "Epoch [42/5000], Training Loss: 0.4195, Training Accuracy: 89.85%\n",
            "Epoch [43/5000], Training Loss: 0.4149, Training Accuracy: 90.01%\n",
            "Epoch [44/5000], Training Loss: 0.4105, Training Accuracy: 90.20%\n",
            "Epoch [45/5000], Training Loss: 0.4062, Training Accuracy: 90.38%\n",
            "Epoch [46/5000], Training Loss: 0.4020, Training Accuracy: 90.54%\n",
            "Epoch [47/5000], Training Loss: 0.3980, Training Accuracy: 90.52%\n",
            "Epoch [48/5000], Training Loss: 0.3942, Training Accuracy: 90.69%\n",
            "Epoch [49/5000], Training Loss: 0.3904, Training Accuracy: 90.72%\n",
            "Epoch [50/5000], Training Loss: 0.3868, Training Accuracy: 90.65%\n",
            "Epoch [51/5000], Training Loss: 0.3833, Training Accuracy: 90.71%\n",
            "Epoch [52/5000], Training Loss: 0.3799, Training Accuracy: 90.64%\n",
            "Epoch [53/5000], Training Loss: 0.3766, Training Accuracy: 90.53%\n",
            "Epoch [54/5000], Training Loss: 0.3735, Training Accuracy: 90.24%\n",
            "Epoch [55/5000], Training Loss: 0.3704, Training Accuracy: 90.21%\n",
            "Epoch [56/5000], Training Loss: 0.3674, Training Accuracy: 90.12%\n",
            "Epoch [57/5000], Training Loss: 0.3645, Training Accuracy: 90.14%\n",
            "Epoch [58/5000], Training Loss: 0.3617, Training Accuracy: 90.03%\n",
            "Epoch [59/5000], Training Loss: 0.3590, Training Accuracy: 90.03%\n",
            "Epoch [60/5000], Training Loss: 0.3563, Training Accuracy: 90.11%\n",
            "Epoch [61/5000], Training Loss: 0.3537, Training Accuracy: 90.02%\n",
            "Epoch [62/5000], Training Loss: 0.3512, Training Accuracy: 90.02%\n",
            "Epoch [63/5000], Training Loss: 0.3488, Training Accuracy: 89.95%\n",
            "Epoch [64/5000], Training Loss: 0.3464, Training Accuracy: 90.16%\n",
            "Epoch [65/5000], Training Loss: 0.3441, Training Accuracy: 90.17%\n",
            "Epoch [66/5000], Training Loss: 0.3419, Training Accuracy: 90.09%\n",
            "Epoch [67/5000], Training Loss: 0.3397, Training Accuracy: 90.17%\n",
            "Epoch [68/5000], Training Loss: 0.3376, Training Accuracy: 90.25%\n",
            "Epoch [69/5000], Training Loss: 0.3355, Training Accuracy: 90.25%\n",
            "Epoch [70/5000], Training Loss: 0.3335, Training Accuracy: 90.15%\n",
            "Epoch [71/5000], Training Loss: 0.3315, Training Accuracy: 90.17%\n",
            "Epoch [72/5000], Training Loss: 0.3296, Training Accuracy: 90.17%\n",
            "Epoch [73/5000], Training Loss: 0.3277, Training Accuracy: 90.08%\n",
            "Epoch [74/5000], Training Loss: 0.3259, Training Accuracy: 90.23%\n",
            "Epoch [75/5000], Training Loss: 0.3241, Training Accuracy: 90.15%\n",
            "Epoch [76/5000], Training Loss: 0.3223, Training Accuracy: 90.15%\n",
            "Epoch [77/5000], Training Loss: 0.3206, Training Accuracy: 90.16%\n",
            "Epoch [78/5000], Training Loss: 0.3190, Training Accuracy: 90.09%\n",
            "Epoch [79/5000], Training Loss: 0.3173, Training Accuracy: 90.17%\n",
            "Epoch [80/5000], Training Loss: 0.3157, Training Accuracy: 90.17%\n",
            "Epoch [81/5000], Training Loss: 0.3142, Training Accuracy: 90.18%\n",
            "Epoch [82/5000], Training Loss: 0.3127, Training Accuracy: 90.12%\n",
            "Epoch [83/5000], Training Loss: 0.3112, Training Accuracy: 90.27%\n",
            "Epoch [84/5000], Training Loss: 0.3097, Training Accuracy: 90.29%\n",
            "Epoch [85/5000], Training Loss: 0.3083, Training Accuracy: 90.29%\n",
            "Epoch [86/5000], Training Loss: 0.3069, Training Accuracy: 90.22%\n",
            "Epoch [87/5000], Training Loss: 0.3055, Training Accuracy: 90.22%\n",
            "Epoch [88/5000], Training Loss: 0.3042, Training Accuracy: 90.24%\n",
            "Epoch [89/5000], Training Loss: 0.3029, Training Accuracy: 90.24%\n",
            "Epoch [90/5000], Training Loss: 0.3016, Training Accuracy: 90.18%\n",
            "Epoch [91/5000], Training Loss: 0.3003, Training Accuracy: 90.28%\n",
            "Epoch [92/5000], Training Loss: 0.2991, Training Accuracy: 90.28%\n",
            "Epoch [93/5000], Training Loss: 0.2979, Training Accuracy: 90.30%\n",
            "Epoch [94/5000], Training Loss: 0.2967, Training Accuracy: 90.30%\n",
            "Epoch [95/5000], Training Loss: 0.2955, Training Accuracy: 90.23%\n",
            "Epoch [96/5000], Training Loss: 0.2944, Training Accuracy: 90.23%\n",
            "Epoch [97/5000], Training Loss: 0.2933, Training Accuracy: 90.23%\n",
            "Epoch [98/5000], Training Loss: 0.2922, Training Accuracy: 90.25%\n",
            "Epoch [99/5000], Training Loss: 0.2911, Training Accuracy: 90.25%\n",
            "Epoch [100/5000], Training Loss: 0.2900, Training Accuracy: 90.47%\n",
            "Epoch [101/5000], Training Loss: 0.2890, Training Accuracy: 90.47%\n",
            "Epoch [102/5000], Training Loss: 0.2880, Training Accuracy: 90.40%\n",
            "Epoch [103/5000], Training Loss: 0.2870, Training Accuracy: 90.54%\n",
            "Epoch [104/5000], Training Loss: 0.2860, Training Accuracy: 90.54%\n",
            "Epoch [105/5000], Training Loss: 0.2850, Training Accuracy: 90.54%\n",
            "Epoch [106/5000], Training Loss: 0.2841, Training Accuracy: 90.54%\n",
            "Epoch [107/5000], Training Loss: 0.2831, Training Accuracy: 90.54%\n",
            "Epoch [108/5000], Training Loss: 0.2822, Training Accuracy: 90.55%\n",
            "Epoch [109/5000], Training Loss: 0.2813, Training Accuracy: 90.55%\n",
            "Epoch [110/5000], Training Loss: 0.2804, Training Accuracy: 90.49%\n",
            "Epoch [111/5000], Training Loss: 0.2795, Training Accuracy: 90.48%\n",
            "Epoch [112/5000], Training Loss: 0.2787, Training Accuracy: 90.48%\n",
            "Epoch [113/5000], Training Loss: 0.2778, Training Accuracy: 90.48%\n",
            "Epoch [114/5000], Training Loss: 0.2770, Training Accuracy: 90.50%\n",
            "Epoch [115/5000], Training Loss: 0.2761, Training Accuracy: 90.50%\n",
            "Epoch [116/5000], Training Loss: 0.2753, Training Accuracy: 90.63%\n",
            "Epoch [117/5000], Training Loss: 0.2745, Training Accuracy: 90.63%\n",
            "Epoch [118/5000], Training Loss: 0.2738, Training Accuracy: 90.63%\n",
            "Epoch [119/5000], Training Loss: 0.2730, Training Accuracy: 90.63%\n",
            "Epoch [120/5000], Training Loss: 0.2722, Training Accuracy: 90.63%\n",
            "Epoch [121/5000], Training Loss: 0.2715, Training Accuracy: 90.58%\n",
            "Epoch [122/5000], Training Loss: 0.2707, Training Accuracy: 90.58%\n",
            "Epoch [123/5000], Training Loss: 0.2700, Training Accuracy: 90.58%\n",
            "Epoch [124/5000], Training Loss: 0.2693, Training Accuracy: 90.58%\n",
            "Epoch [125/5000], Training Loss: 0.2686, Training Accuracy: 90.58%\n",
            "Epoch [126/5000], Training Loss: 0.2679, Training Accuracy: 90.58%\n",
            "Epoch [127/5000], Training Loss: 0.2672, Training Accuracy: 90.58%\n",
            "Epoch [128/5000], Training Loss: 0.2665, Training Accuracy: 90.60%\n",
            "Epoch [129/5000], Training Loss: 0.2658, Training Accuracy: 90.60%\n",
            "Epoch [130/5000], Training Loss: 0.2652, Training Accuracy: 90.70%\n",
            "Epoch [131/5000], Training Loss: 0.2645, Training Accuracy: 90.69%\n",
            "Epoch [132/5000], Training Loss: 0.2639, Training Accuracy: 90.69%\n",
            "Epoch [133/5000], Training Loss: 0.2633, Training Accuracy: 90.69%\n",
            "Epoch [134/5000], Training Loss: 0.2626, Training Accuracy: 90.69%\n",
            "Epoch [135/5000], Training Loss: 0.2620, Training Accuracy: 90.69%\n",
            "Epoch [136/5000], Training Loss: 0.2614, Training Accuracy: 90.71%\n",
            "Epoch [137/5000], Training Loss: 0.2608, Training Accuracy: 90.71%\n",
            "Epoch [138/5000], Training Loss: 0.2602, Training Accuracy: 90.71%\n",
            "Epoch [139/5000], Training Loss: 0.2596, Training Accuracy: 90.65%\n",
            "Epoch [140/5000], Training Loss: 0.2591, Training Accuracy: 90.65%\n",
            "Epoch [141/5000], Training Loss: 0.2585, Training Accuracy: 90.64%\n",
            "Epoch [142/5000], Training Loss: 0.2579, Training Accuracy: 90.64%\n",
            "Epoch [143/5000], Training Loss: 0.2574, Training Accuracy: 90.64%\n",
            "Epoch [144/5000], Training Loss: 0.2568, Training Accuracy: 90.67%\n",
            "Epoch [145/5000], Training Loss: 0.2563, Training Accuracy: 90.77%\n",
            "Epoch [146/5000], Training Loss: 0.2557, Training Accuracy: 90.77%\n",
            "Epoch [147/5000], Training Loss: 0.2552, Training Accuracy: 90.77%\n",
            "Epoch [148/5000], Training Loss: 0.2547, Training Accuracy: 90.77%\n",
            "Epoch [149/5000], Training Loss: 0.2542, Training Accuracy: 90.77%\n",
            "Epoch [150/5000], Training Loss: 0.2537, Training Accuracy: 90.77%\n",
            "Epoch [151/5000], Training Loss: 0.2532, Training Accuracy: 90.77%\n",
            "Epoch [152/5000], Training Loss: 0.2527, Training Accuracy: 90.77%\n",
            "Epoch [153/5000], Training Loss: 0.2522, Training Accuracy: 90.79%\n",
            "Epoch [154/5000], Training Loss: 0.2517, Training Accuracy: 90.79%\n",
            "Epoch [155/5000], Training Loss: 0.2512, Training Accuracy: 90.57%\n",
            "Epoch [156/5000], Training Loss: 0.2507, Training Accuracy: 90.57%\n",
            "Epoch [157/5000], Training Loss: 0.2503, Training Accuracy: 90.57%\n",
            "Epoch [158/5000], Training Loss: 0.2498, Training Accuracy: 90.57%\n",
            "Epoch [159/5000], Training Loss: 0.2493, Training Accuracy: 90.57%\n",
            "Epoch [160/5000], Training Loss: 0.2489, Training Accuracy: 90.57%\n",
            "Epoch [161/5000], Training Loss: 0.2484, Training Accuracy: 90.57%\n",
            "Epoch [162/5000], Training Loss: 0.2480, Training Accuracy: 90.67%\n",
            "Epoch [163/5000], Training Loss: 0.2476, Training Accuracy: 90.67%\n",
            "Epoch [164/5000], Training Loss: 0.2471, Training Accuracy: 90.70%\n",
            "Epoch [165/5000], Training Loss: 0.2467, Training Accuracy: 90.70%\n",
            "Epoch [166/5000], Training Loss: 0.2463, Training Accuracy: 90.70%\n",
            "Epoch [167/5000], Training Loss: 0.2459, Training Accuracy: 90.70%\n",
            "Epoch [168/5000], Training Loss: 0.2454, Training Accuracy: 90.70%\n",
            "Epoch [169/5000], Training Loss: 0.2450, Training Accuracy: 90.68%\n",
            "Epoch [170/5000], Training Loss: 0.2446, Training Accuracy: 90.68%\n",
            "Epoch [171/5000], Training Loss: 0.2442, Training Accuracy: 90.68%\n",
            "Epoch [172/5000], Training Loss: 0.2438, Training Accuracy: 90.68%\n",
            "Epoch [173/5000], Training Loss: 0.2434, Training Accuracy: 90.68%\n",
            "Epoch [174/5000], Training Loss: 0.2430, Training Accuracy: 90.68%\n",
            "Epoch [175/5000], Training Loss: 0.2427, Training Accuracy: 90.70%\n",
            "Epoch [176/5000], Training Loss: 0.2423, Training Accuracy: 90.70%\n",
            "Epoch [177/5000], Training Loss: 0.2419, Training Accuracy: 90.66%\n",
            "Epoch [178/5000], Training Loss: 0.2415, Training Accuracy: 90.66%\n",
            "Epoch [179/5000], Training Loss: 0.2411, Training Accuracy: 90.66%\n",
            "Epoch [180/5000], Training Loss: 0.2408, Training Accuracy: 90.66%\n",
            "Epoch [181/5000], Training Loss: 0.2404, Training Accuracy: 90.66%\n",
            "Epoch [182/5000], Training Loss: 0.2401, Training Accuracy: 90.76%\n",
            "Epoch [183/5000], Training Loss: 0.2397, Training Accuracy: 90.76%\n",
            "Epoch [184/5000], Training Loss: 0.2393, Training Accuracy: 90.76%\n",
            "Epoch [185/5000], Training Loss: 0.2390, Training Accuracy: 90.76%\n",
            "Epoch [186/5000], Training Loss: 0.2386, Training Accuracy: 90.76%\n",
            "Epoch [187/5000], Training Loss: 0.2383, Training Accuracy: 90.76%\n",
            "Epoch [188/5000], Training Loss: 0.2380, Training Accuracy: 90.78%\n",
            "Epoch [189/5000], Training Loss: 0.2376, Training Accuracy: 90.78%\n",
            "Epoch [190/5000], Training Loss: 0.2373, Training Accuracy: 90.78%\n",
            "Epoch [191/5000], Training Loss: 0.2370, Training Accuracy: 90.78%\n",
            "Epoch [192/5000], Training Loss: 0.2366, Training Accuracy: 90.78%\n",
            "Epoch [193/5000], Training Loss: 0.2363, Training Accuracy: 90.77%\n",
            "Epoch [194/5000], Training Loss: 0.2360, Training Accuracy: 90.77%\n",
            "Epoch [195/5000], Training Loss: 0.2357, Training Accuracy: 90.77%\n",
            "Epoch [196/5000], Training Loss: 0.2354, Training Accuracy: 90.77%\n",
            "Epoch [197/5000], Training Loss: 0.2350, Training Accuracy: 90.77%\n",
            "Epoch [198/5000], Training Loss: 0.2347, Training Accuracy: 90.77%\n",
            "Epoch [199/5000], Training Loss: 0.2344, Training Accuracy: 90.77%\n",
            "Epoch [200/5000], Training Loss: 0.2341, Training Accuracy: 90.77%\n",
            "Epoch [201/5000], Training Loss: 0.2338, Training Accuracy: 90.77%\n",
            "Epoch [202/5000], Training Loss: 0.2335, Training Accuracy: 90.80%\n",
            "Epoch [203/5000], Training Loss: 0.2332, Training Accuracy: 90.80%\n",
            "Epoch [204/5000], Training Loss: 0.2329, Training Accuracy: 90.90%\n",
            "Epoch [205/5000], Training Loss: 0.2326, Training Accuracy: 90.90%\n",
            "Epoch [206/5000], Training Loss: 0.2324, Training Accuracy: 90.90%\n",
            "Epoch [207/5000], Training Loss: 0.2321, Training Accuracy: 90.90%\n",
            "Epoch [208/5000], Training Loss: 0.2318, Training Accuracy: 90.90%\n",
            "Epoch [209/5000], Training Loss: 0.2315, Training Accuracy: 90.74%\n",
            "Epoch [210/5000], Training Loss: 0.2312, Training Accuracy: 90.74%\n",
            "Epoch [211/5000], Training Loss: 0.2310, Training Accuracy: 90.74%\n",
            "Epoch [212/5000], Training Loss: 0.2307, Training Accuracy: 90.74%\n",
            "Epoch [213/5000], Training Loss: 0.2304, Training Accuracy: 90.74%\n",
            "Epoch [214/5000], Training Loss: 0.2301, Training Accuracy: 90.74%\n",
            "Epoch [215/5000], Training Loss: 0.2299, Training Accuracy: 90.74%\n",
            "Epoch [216/5000], Training Loss: 0.2296, Training Accuracy: 90.74%\n",
            "Epoch [217/5000], Training Loss: 0.2293, Training Accuracy: 90.74%\n",
            "Epoch [218/5000], Training Loss: 0.2291, Training Accuracy: 90.78%\n",
            "Epoch [219/5000], Training Loss: 0.2288, Training Accuracy: 90.78%\n",
            "Epoch [220/5000], Training Loss: 0.2286, Training Accuracy: 90.78%\n",
            "Epoch [221/5000], Training Loss: 0.2283, Training Accuracy: 90.78%\n",
            "Epoch [222/5000], Training Loss: 0.2281, Training Accuracy: 90.78%\n",
            "Epoch [223/5000], Training Loss: 0.2278, Training Accuracy: 90.78%\n",
            "Epoch [224/5000], Training Loss: 0.2276, Training Accuracy: 90.78%\n",
            "Epoch [225/5000], Training Loss: 0.2273, Training Accuracy: 90.78%\n",
            "Epoch [226/5000], Training Loss: 0.2271, Training Accuracy: 90.78%\n",
            "Epoch [227/5000], Training Loss: 0.2268, Training Accuracy: 90.78%\n",
            "Epoch [228/5000], Training Loss: 0.2266, Training Accuracy: 90.78%\n",
            "Epoch [229/5000], Training Loss: 0.2264, Training Accuracy: 90.78%\n",
            "Epoch [230/5000], Training Loss: 0.2261, Training Accuracy: 90.87%\n",
            "Epoch [231/5000], Training Loss: 0.2259, Training Accuracy: 90.87%\n",
            "Epoch [232/5000], Training Loss: 0.2257, Training Accuracy: 90.87%\n",
            "Epoch [233/5000], Training Loss: 0.2254, Training Accuracy: 90.87%\n",
            "Epoch [234/5000], Training Loss: 0.2252, Training Accuracy: 90.87%\n",
            "Epoch [235/5000], Training Loss: 0.2250, Training Accuracy: 90.87%\n",
            "Epoch [236/5000], Training Loss: 0.2247, Training Accuracy: 90.87%\n",
            "Epoch [237/5000], Training Loss: 0.2245, Training Accuracy: 90.92%\n",
            "Epoch [238/5000], Training Loss: 0.2243, Training Accuracy: 90.92%\n",
            "Epoch [239/5000], Training Loss: 0.2241, Training Accuracy: 90.92%\n",
            "Epoch [240/5000], Training Loss: 0.2239, Training Accuracy: 90.90%\n",
            "Epoch [241/5000], Training Loss: 0.2236, Training Accuracy: 90.90%\n",
            "Epoch [242/5000], Training Loss: 0.2234, Training Accuracy: 90.90%\n",
            "Epoch [243/5000], Training Loss: 0.2232, Training Accuracy: 90.90%\n",
            "Epoch [244/5000], Training Loss: 0.2230, Training Accuracy: 90.90%\n",
            "Epoch [245/5000], Training Loss: 0.2228, Training Accuracy: 90.90%\n",
            "Epoch [246/5000], Training Loss: 0.2226, Training Accuracy: 90.90%\n",
            "Epoch [247/5000], Training Loss: 0.2224, Training Accuracy: 90.90%\n",
            "Epoch [248/5000], Training Loss: 0.2222, Training Accuracy: 90.90%\n",
            "Epoch [249/5000], Training Loss: 0.2220, Training Accuracy: 90.90%\n",
            "Epoch [250/5000], Training Loss: 0.2218, Training Accuracy: 90.90%\n",
            "Epoch [251/5000], Training Loss: 0.2216, Training Accuracy: 90.90%\n",
            "Epoch [252/5000], Training Loss: 0.2214, Training Accuracy: 90.90%\n",
            "Epoch [253/5000], Training Loss: 0.2212, Training Accuracy: 90.90%\n",
            "Epoch [254/5000], Training Loss: 0.2210, Training Accuracy: 90.90%\n",
            "Epoch [255/5000], Training Loss: 0.2208, Training Accuracy: 90.90%\n",
            "Epoch [256/5000], Training Loss: 0.2206, Training Accuracy: 90.90%\n",
            "Epoch [257/5000], Training Loss: 0.2204, Training Accuracy: 90.90%\n",
            "Epoch [258/5000], Training Loss: 0.2202, Training Accuracy: 90.90%\n",
            "Epoch [259/5000], Training Loss: 0.2200, Training Accuracy: 90.94%\n",
            "Epoch [260/5000], Training Loss: 0.2198, Training Accuracy: 91.06%\n",
            "Epoch [261/5000], Training Loss: 0.2196, Training Accuracy: 91.06%\n",
            "Epoch [262/5000], Training Loss: 0.2194, Training Accuracy: 91.06%\n",
            "Epoch [263/5000], Training Loss: 0.2192, Training Accuracy: 91.06%\n",
            "Epoch [264/5000], Training Loss: 0.2191, Training Accuracy: 91.06%\n",
            "Epoch [265/5000], Training Loss: 0.2189, Training Accuracy: 90.92%\n",
            "Epoch [266/5000], Training Loss: 0.2187, Training Accuracy: 90.92%\n",
            "Epoch [267/5000], Training Loss: 0.2185, Training Accuracy: 90.92%\n",
            "Epoch [268/5000], Training Loss: 0.2183, Training Accuracy: 90.92%\n",
            "Epoch [269/5000], Training Loss: 0.2182, Training Accuracy: 90.92%\n",
            "Epoch [270/5000], Training Loss: 0.2180, Training Accuracy: 90.92%\n",
            "Epoch [271/5000], Training Loss: 0.2178, Training Accuracy: 90.92%\n",
            "Epoch [272/5000], Training Loss: 0.2176, Training Accuracy: 90.92%\n",
            "Epoch [273/5000], Training Loss: 0.2175, Training Accuracy: 90.92%\n",
            "Epoch [274/5000], Training Loss: 0.2173, Training Accuracy: 90.92%\n",
            "Epoch [275/5000], Training Loss: 0.2171, Training Accuracy: 90.92%\n",
            "Epoch [276/5000], Training Loss: 0.2169, Training Accuracy: 90.92%\n",
            "Epoch [277/5000], Training Loss: 0.2168, Training Accuracy: 90.92%\n",
            "Epoch [278/5000], Training Loss: 0.2166, Training Accuracy: 90.92%\n",
            "Epoch [279/5000], Training Loss: 0.2164, Training Accuracy: 90.92%\n",
            "Epoch [280/5000], Training Loss: 0.2163, Training Accuracy: 90.92%\n",
            "Epoch [281/5000], Training Loss: 0.2161, Training Accuracy: 90.92%\n",
            "Epoch [282/5000], Training Loss: 0.2159, Training Accuracy: 90.92%\n",
            "Epoch [283/5000], Training Loss: 0.2158, Training Accuracy: 90.92%\n",
            "Epoch [284/5000], Training Loss: 0.2156, Training Accuracy: 90.96%\n",
            "Epoch [285/5000], Training Loss: 0.2155, Training Accuracy: 90.96%\n",
            "Epoch [286/5000], Training Loss: 0.2153, Training Accuracy: 90.96%\n",
            "Epoch [287/5000], Training Loss: 0.2151, Training Accuracy: 90.96%\n",
            "Epoch [288/5000], Training Loss: 0.2150, Training Accuracy: 90.96%\n",
            "Epoch [289/5000], Training Loss: 0.2148, Training Accuracy: 91.00%\n",
            "Epoch [290/5000], Training Loss: 0.2147, Training Accuracy: 91.00%\n",
            "Epoch [291/5000], Training Loss: 0.2145, Training Accuracy: 91.00%\n",
            "Epoch [292/5000], Training Loss: 0.2144, Training Accuracy: 91.00%\n",
            "Epoch [293/5000], Training Loss: 0.2142, Training Accuracy: 91.00%\n",
            "Epoch [294/5000], Training Loss: 0.2141, Training Accuracy: 91.00%\n",
            "Epoch [295/5000], Training Loss: 0.2139, Training Accuracy: 91.00%\n",
            "Epoch [296/5000], Training Loss: 0.2138, Training Accuracy: 91.00%\n",
            "Epoch [297/5000], Training Loss: 0.2136, Training Accuracy: 91.00%\n",
            "Epoch [298/5000], Training Loss: 0.2135, Training Accuracy: 91.13%\n",
            "Epoch [299/5000], Training Loss: 0.2133, Training Accuracy: 91.13%\n",
            "Epoch [300/5000], Training Loss: 0.2132, Training Accuracy: 91.13%\n",
            "Epoch [301/5000], Training Loss: 0.2130, Training Accuracy: 91.13%\n",
            "Epoch [302/5000], Training Loss: 0.2129, Training Accuracy: 91.13%\n",
            "Epoch [303/5000], Training Loss: 0.2127, Training Accuracy: 91.13%\n",
            "Epoch [304/5000], Training Loss: 0.2126, Training Accuracy: 91.13%\n",
            "Epoch [305/5000], Training Loss: 0.2125, Training Accuracy: 91.13%\n",
            "Epoch [306/5000], Training Loss: 0.2123, Training Accuracy: 91.13%\n",
            "Epoch [307/5000], Training Loss: 0.2122, Training Accuracy: 91.13%\n",
            "Epoch [308/5000], Training Loss: 0.2120, Training Accuracy: 91.13%\n",
            "Epoch [309/5000], Training Loss: 0.2119, Training Accuracy: 91.13%\n",
            "Epoch [310/5000], Training Loss: 0.2118, Training Accuracy: 91.13%\n",
            "Epoch [311/5000], Training Loss: 0.2116, Training Accuracy: 91.13%\n",
            "Epoch [312/5000], Training Loss: 0.2115, Training Accuracy: 91.13%\n",
            "Epoch [313/5000], Training Loss: 0.2114, Training Accuracy: 91.13%\n",
            "Epoch [314/5000], Training Loss: 0.2112, Training Accuracy: 91.13%\n",
            "Epoch [315/5000], Training Loss: 0.2111, Training Accuracy: 91.18%\n",
            "Epoch [316/5000], Training Loss: 0.2110, Training Accuracy: 91.18%\n",
            "Epoch [317/5000], Training Loss: 0.2108, Training Accuracy: 91.18%\n",
            "Epoch [318/5000], Training Loss: 0.2107, Training Accuracy: 91.18%\n",
            "Epoch [319/5000], Training Loss: 0.2106, Training Accuracy: 91.18%\n",
            "Epoch [320/5000], Training Loss: 0.2104, Training Accuracy: 91.18%\n",
            "Epoch [321/5000], Training Loss: 0.2103, Training Accuracy: 91.18%\n",
            "Epoch [322/5000], Training Loss: 0.2102, Training Accuracy: 91.18%\n",
            "Epoch [323/5000], Training Loss: 0.2101, Training Accuracy: 91.18%\n",
            "Epoch [324/5000], Training Loss: 0.2099, Training Accuracy: 91.18%\n",
            "Epoch [325/5000], Training Loss: 0.2098, Training Accuracy: 91.18%\n",
            "Epoch [326/5000], Training Loss: 0.2097, Training Accuracy: 91.18%\n",
            "Epoch [327/5000], Training Loss: 0.2096, Training Accuracy: 91.18%\n",
            "Epoch [328/5000], Training Loss: 0.2094, Training Accuracy: 91.18%\n",
            "Epoch [329/5000], Training Loss: 0.2093, Training Accuracy: 91.18%\n",
            "Epoch [330/5000], Training Loss: 0.2092, Training Accuracy: 91.18%\n",
            "Epoch [331/5000], Training Loss: 0.2091, Training Accuracy: 91.18%\n",
            "Epoch [332/5000], Training Loss: 0.2089, Training Accuracy: 91.02%\n",
            "Epoch [333/5000], Training Loss: 0.2088, Training Accuracy: 91.02%\n",
            "Epoch [334/5000], Training Loss: 0.2087, Training Accuracy: 91.02%\n",
            "Epoch [335/5000], Training Loss: 0.2086, Training Accuracy: 91.02%\n",
            "Epoch [336/5000], Training Loss: 0.2085, Training Accuracy: 91.02%\n",
            "Epoch [337/5000], Training Loss: 0.2083, Training Accuracy: 91.02%\n",
            "Epoch [338/5000], Training Loss: 0.2082, Training Accuracy: 91.02%\n",
            "Epoch [339/5000], Training Loss: 0.2081, Training Accuracy: 91.02%\n",
            "Epoch [340/5000], Training Loss: 0.2080, Training Accuracy: 91.02%\n",
            "Epoch [341/5000], Training Loss: 0.2079, Training Accuracy: 91.02%\n",
            "Epoch [342/5000], Training Loss: 0.2078, Training Accuracy: 91.02%\n",
            "Epoch [343/5000], Training Loss: 0.2077, Training Accuracy: 91.02%\n",
            "Epoch [344/5000], Training Loss: 0.2075, Training Accuracy: 91.02%\n",
            "Epoch [345/5000], Training Loss: 0.2074, Training Accuracy: 91.02%\n",
            "Epoch [346/5000], Training Loss: 0.2073, Training Accuracy: 91.13%\n",
            "Epoch [347/5000], Training Loss: 0.2072, Training Accuracy: 91.13%\n",
            "Epoch [348/5000], Training Loss: 0.2071, Training Accuracy: 91.13%\n",
            "Epoch [349/5000], Training Loss: 0.2070, Training Accuracy: 91.13%\n",
            "Epoch [350/5000], Training Loss: 0.2069, Training Accuracy: 91.13%\n",
            "Epoch [351/5000], Training Loss: 0.2068, Training Accuracy: 91.13%\n",
            "Epoch [352/5000], Training Loss: 0.2067, Training Accuracy: 91.13%\n",
            "Epoch [353/5000], Training Loss: 0.2066, Training Accuracy: 91.18%\n",
            "Epoch [354/5000], Training Loss: 0.2064, Training Accuracy: 91.18%\n",
            "Epoch [355/5000], Training Loss: 0.2063, Training Accuracy: 91.18%\n",
            "Epoch [356/5000], Training Loss: 0.2062, Training Accuracy: 91.18%\n",
            "Epoch [357/5000], Training Loss: 0.2061, Training Accuracy: 91.18%\n",
            "Epoch [358/5000], Training Loss: 0.2060, Training Accuracy: 91.18%\n",
            "Epoch [359/5000], Training Loss: 0.2059, Training Accuracy: 91.18%\n",
            "Epoch [360/5000], Training Loss: 0.2058, Training Accuracy: 91.18%\n",
            "Epoch [361/5000], Training Loss: 0.2057, Training Accuracy: 91.18%\n",
            "Epoch [362/5000], Training Loss: 0.2056, Training Accuracy: 91.18%\n",
            "Epoch [363/5000], Training Loss: 0.2055, Training Accuracy: 91.18%\n",
            "Epoch [364/5000], Training Loss: 0.2054, Training Accuracy: 91.18%\n",
            "Epoch [365/5000], Training Loss: 0.2053, Training Accuracy: 91.18%\n",
            "Epoch [366/5000], Training Loss: 0.2052, Training Accuracy: 91.18%\n",
            "Epoch [367/5000], Training Loss: 0.2051, Training Accuracy: 91.18%\n",
            "Epoch [368/5000], Training Loss: 0.2050, Training Accuracy: 91.18%\n",
            "Epoch [369/5000], Training Loss: 0.2049, Training Accuracy: 91.18%\n",
            "Epoch [370/5000], Training Loss: 0.2048, Training Accuracy: 91.18%\n",
            "Epoch [371/5000], Training Loss: 0.2047, Training Accuracy: 91.18%\n",
            "Epoch [372/5000], Training Loss: 0.2046, Training Accuracy: 91.18%\n",
            "Epoch [373/5000], Training Loss: 0.2045, Training Accuracy: 91.18%\n",
            "Epoch [374/5000], Training Loss: 0.2044, Training Accuracy: 91.18%\n",
            "Epoch [375/5000], Training Loss: 0.2043, Training Accuracy: 91.18%\n",
            "Epoch [376/5000], Training Loss: 0.2042, Training Accuracy: 91.18%\n",
            "Epoch [377/5000], Training Loss: 0.2041, Training Accuracy: 91.18%\n",
            "Epoch [378/5000], Training Loss: 0.2040, Training Accuracy: 91.18%\n",
            "Epoch [379/5000], Training Loss: 0.2039, Training Accuracy: 91.18%\n",
            "Epoch [380/5000], Training Loss: 0.2038, Training Accuracy: 91.18%\n",
            "Epoch [381/5000], Training Loss: 0.2038, Training Accuracy: 91.18%\n",
            "Epoch [382/5000], Training Loss: 0.2037, Training Accuracy: 91.18%\n",
            "Epoch [383/5000], Training Loss: 0.2036, Training Accuracy: 91.18%\n",
            "Epoch [384/5000], Training Loss: 0.2035, Training Accuracy: 91.18%\n",
            "Epoch [385/5000], Training Loss: 0.2034, Training Accuracy: 91.18%\n",
            "Epoch [386/5000], Training Loss: 0.2033, Training Accuracy: 91.18%\n",
            "Epoch [387/5000], Training Loss: 0.2032, Training Accuracy: 91.18%\n",
            "Epoch [388/5000], Training Loss: 0.2031, Training Accuracy: 91.18%\n",
            "Epoch [389/5000], Training Loss: 0.2030, Training Accuracy: 91.18%\n",
            "Epoch [390/5000], Training Loss: 0.2029, Training Accuracy: 91.18%\n",
            "Epoch [391/5000], Training Loss: 0.2028, Training Accuracy: 91.18%\n",
            "Epoch [392/5000], Training Loss: 0.2028, Training Accuracy: 91.18%\n",
            "Epoch [393/5000], Training Loss: 0.2027, Training Accuracy: 91.18%\n",
            "Epoch [394/5000], Training Loss: 0.2026, Training Accuracy: 91.18%\n",
            "Epoch [395/5000], Training Loss: 0.2025, Training Accuracy: 91.18%\n",
            "Epoch [396/5000], Training Loss: 0.2024, Training Accuracy: 91.18%\n",
            "Epoch [397/5000], Training Loss: 0.2023, Training Accuracy: 91.18%\n",
            "Epoch [398/5000], Training Loss: 0.2022, Training Accuracy: 91.18%\n",
            "Epoch [399/5000], Training Loss: 0.2021, Training Accuracy: 91.18%\n",
            "Epoch [400/5000], Training Loss: 0.2021, Training Accuracy: 91.18%\n",
            "Epoch [401/5000], Training Loss: 0.2020, Training Accuracy: 91.18%\n",
            "Epoch [402/5000], Training Loss: 0.2019, Training Accuracy: 91.24%\n",
            "Epoch [403/5000], Training Loss: 0.2018, Training Accuracy: 91.24%\n",
            "Epoch [404/5000], Training Loss: 0.2017, Training Accuracy: 91.24%\n",
            "Epoch [405/5000], Training Loss: 0.2016, Training Accuracy: 91.24%\n",
            "Epoch [406/5000], Training Loss: 0.2016, Training Accuracy: 91.24%\n",
            "Epoch [407/5000], Training Loss: 0.2015, Training Accuracy: 91.24%\n",
            "Epoch [408/5000], Training Loss: 0.2014, Training Accuracy: 91.24%\n",
            "Epoch [409/5000], Training Loss: 0.2013, Training Accuracy: 91.37%\n",
            "Epoch [410/5000], Training Loss: 0.2012, Training Accuracy: 91.37%\n",
            "Epoch [411/5000], Training Loss: 0.2012, Training Accuracy: 91.37%\n",
            "Epoch [412/5000], Training Loss: 0.2011, Training Accuracy: 91.37%\n",
            "Epoch [413/5000], Training Loss: 0.2010, Training Accuracy: 91.37%\n",
            "Epoch [414/5000], Training Loss: 0.2009, Training Accuracy: 91.37%\n",
            "Epoch [415/5000], Training Loss: 0.2008, Training Accuracy: 91.37%\n",
            "Epoch [416/5000], Training Loss: 0.2008, Training Accuracy: 91.37%\n",
            "Epoch [417/5000], Training Loss: 0.2007, Training Accuracy: 91.37%\n",
            "Epoch [418/5000], Training Loss: 0.2006, Training Accuracy: 91.37%\n",
            "Epoch [419/5000], Training Loss: 0.2005, Training Accuracy: 91.37%\n",
            "Epoch [420/5000], Training Loss: 0.2004, Training Accuracy: 91.37%\n",
            "Epoch [421/5000], Training Loss: 0.2004, Training Accuracy: 91.37%\n",
            "Epoch [422/5000], Training Loss: 0.2003, Training Accuracy: 91.08%\n",
            "Epoch [423/5000], Training Loss: 0.2002, Training Accuracy: 91.08%\n",
            "Epoch [424/5000], Training Loss: 0.2001, Training Accuracy: 91.08%\n",
            "Epoch [425/5000], Training Loss: 0.2001, Training Accuracy: 91.08%\n",
            "Epoch [426/5000], Training Loss: 0.2000, Training Accuracy: 91.08%\n",
            "Epoch [427/5000], Training Loss: 0.1999, Training Accuracy: 91.08%\n",
            "Epoch [428/5000], Training Loss: 0.1998, Training Accuracy: 91.08%\n",
            "Epoch [429/5000], Training Loss: 0.1998, Training Accuracy: 91.08%\n",
            "Epoch [430/5000], Training Loss: 0.1997, Training Accuracy: 91.08%\n",
            "Epoch [431/5000], Training Loss: 0.1996, Training Accuracy: 91.08%\n",
            "Epoch [432/5000], Training Loss: 0.1995, Training Accuracy: 91.08%\n",
            "Epoch [433/5000], Training Loss: 0.1995, Training Accuracy: 91.08%\n",
            "Epoch [434/5000], Training Loss: 0.1994, Training Accuracy: 91.08%\n",
            "Epoch [435/5000], Training Loss: 0.1993, Training Accuracy: 91.08%\n",
            "Epoch [436/5000], Training Loss: 0.1992, Training Accuracy: 91.08%\n",
            "Epoch [437/5000], Training Loss: 0.1992, Training Accuracy: 91.08%\n",
            "Epoch [438/5000], Training Loss: 0.1991, Training Accuracy: 91.08%\n",
            "Epoch [439/5000], Training Loss: 0.1990, Training Accuracy: 91.08%\n",
            "Epoch [440/5000], Training Loss: 0.1990, Training Accuracy: 91.08%\n",
            "Epoch [441/5000], Training Loss: 0.1989, Training Accuracy: 91.08%\n",
            "Epoch [442/5000], Training Loss: 0.1988, Training Accuracy: 91.08%\n",
            "Epoch [443/5000], Training Loss: 0.1988, Training Accuracy: 91.08%\n",
            "Epoch [444/5000], Training Loss: 0.1987, Training Accuracy: 91.08%\n",
            "Epoch [445/5000], Training Loss: 0.1986, Training Accuracy: 91.08%\n",
            "Epoch [446/5000], Training Loss: 0.1985, Training Accuracy: 91.08%\n",
            "Epoch [447/5000], Training Loss: 0.1985, Training Accuracy: 91.08%\n",
            "Epoch [448/5000], Training Loss: 0.1984, Training Accuracy: 91.08%\n",
            "Epoch [449/5000], Training Loss: 0.1983, Training Accuracy: 91.08%\n",
            "Epoch [450/5000], Training Loss: 0.1983, Training Accuracy: 91.15%\n",
            "Epoch [451/5000], Training Loss: 0.1982, Training Accuracy: 91.15%\n",
            "Epoch [452/5000], Training Loss: 0.1981, Training Accuracy: 91.15%\n",
            "Epoch [453/5000], Training Loss: 0.1981, Training Accuracy: 91.15%\n",
            "Epoch [454/5000], Training Loss: 0.1980, Training Accuracy: 91.15%\n",
            "Epoch [455/5000], Training Loss: 0.1979, Training Accuracy: 91.15%\n",
            "Epoch [456/5000], Training Loss: 0.1979, Training Accuracy: 91.15%\n",
            "Epoch [457/5000], Training Loss: 0.1978, Training Accuracy: 91.15%\n",
            "Epoch [458/5000], Training Loss: 0.1977, Training Accuracy: 91.15%\n",
            "Epoch [459/5000], Training Loss: 0.1977, Training Accuracy: 91.15%\n",
            "Epoch [460/5000], Training Loss: 0.1976, Training Accuracy: 91.15%\n",
            "Epoch [461/5000], Training Loss: 0.1975, Training Accuracy: 91.15%\n",
            "Epoch [462/5000], Training Loss: 0.1975, Training Accuracy: 91.15%\n",
            "Epoch [463/5000], Training Loss: 0.1974, Training Accuracy: 91.15%\n",
            "Epoch [464/5000], Training Loss: 0.1974, Training Accuracy: 91.15%\n",
            "Epoch [465/5000], Training Loss: 0.1973, Training Accuracy: 91.15%\n",
            "Epoch [466/5000], Training Loss: 0.1972, Training Accuracy: 91.15%\n",
            "Epoch [467/5000], Training Loss: 0.1972, Training Accuracy: 91.15%\n",
            "Epoch [468/5000], Training Loss: 0.1971, Training Accuracy: 91.20%\n",
            "Epoch [469/5000], Training Loss: 0.1970, Training Accuracy: 91.20%\n",
            "Epoch [470/5000], Training Loss: 0.1970, Training Accuracy: 91.20%\n",
            "Epoch [471/5000], Training Loss: 0.1969, Training Accuracy: 91.20%\n",
            "Epoch [472/5000], Training Loss: 0.1968, Training Accuracy: 91.20%\n",
            "Epoch [473/5000], Training Loss: 0.1968, Training Accuracy: 91.20%\n",
            "Epoch [474/5000], Training Loss: 0.1967, Training Accuracy: 91.20%\n",
            "Epoch [475/5000], Training Loss: 0.1967, Training Accuracy: 91.20%\n",
            "Epoch [476/5000], Training Loss: 0.1966, Training Accuracy: 91.20%\n",
            "Epoch [477/5000], Training Loss: 0.1965, Training Accuracy: 91.20%\n",
            "Epoch [478/5000], Training Loss: 0.1965, Training Accuracy: 91.20%\n",
            "Epoch [479/5000], Training Loss: 0.1964, Training Accuracy: 91.20%\n",
            "Epoch [480/5000], Training Loss: 0.1964, Training Accuracy: 91.20%\n",
            "Epoch [481/5000], Training Loss: 0.1963, Training Accuracy: 91.20%\n",
            "Epoch [482/5000], Training Loss: 0.1962, Training Accuracy: 91.20%\n",
            "Epoch [483/5000], Training Loss: 0.1962, Training Accuracy: 91.20%\n",
            "Epoch [484/5000], Training Loss: 0.1961, Training Accuracy: 91.20%\n",
            "Epoch [485/5000], Training Loss: 0.1961, Training Accuracy: 91.20%\n",
            "Epoch [486/5000], Training Loss: 0.1960, Training Accuracy: 91.20%\n",
            "Epoch [487/5000], Training Loss: 0.1960, Training Accuracy: 91.20%\n",
            "Epoch [488/5000], Training Loss: 0.1959, Training Accuracy: 91.20%\n",
            "Epoch [489/5000], Training Loss: 0.1958, Training Accuracy: 91.20%\n",
            "Epoch [490/5000], Training Loss: 0.1958, Training Accuracy: 91.20%\n",
            "Epoch [491/5000], Training Loss: 0.1957, Training Accuracy: 91.20%\n",
            "Epoch [492/5000], Training Loss: 0.1957, Training Accuracy: 91.20%\n",
            "Epoch [493/5000], Training Loss: 0.1956, Training Accuracy: 91.20%\n",
            "Epoch [494/5000], Training Loss: 0.1955, Training Accuracy: 91.20%\n",
            "Epoch [495/5000], Training Loss: 0.1955, Training Accuracy: 91.20%\n",
            "Epoch [496/5000], Training Loss: 0.1954, Training Accuracy: 91.20%\n",
            "Epoch [497/5000], Training Loss: 0.1954, Training Accuracy: 91.20%\n",
            "Epoch [498/5000], Training Loss: 0.1953, Training Accuracy: 91.20%\n",
            "Epoch [499/5000], Training Loss: 0.1953, Training Accuracy: 91.37%\n",
            "Epoch [500/5000], Training Loss: 0.1952, Training Accuracy: 91.37%\n",
            "Epoch [501/5000], Training Loss: 0.1952, Training Accuracy: 91.37%\n",
            "Epoch [502/5000], Training Loss: 0.1951, Training Accuracy: 91.37%\n",
            "Epoch [503/5000], Training Loss: 0.1951, Training Accuracy: 91.37%\n",
            "Epoch [504/5000], Training Loss: 0.1950, Training Accuracy: 91.37%\n",
            "Epoch [505/5000], Training Loss: 0.1949, Training Accuracy: 91.37%\n",
            "Epoch [506/5000], Training Loss: 0.1949, Training Accuracy: 91.37%\n",
            "Epoch [507/5000], Training Loss: 0.1948, Training Accuracy: 91.37%\n",
            "Epoch [508/5000], Training Loss: 0.1948, Training Accuracy: 91.37%\n",
            "Epoch [509/5000], Training Loss: 0.1947, Training Accuracy: 91.37%\n",
            "Epoch [510/5000], Training Loss: 0.1947, Training Accuracy: 91.37%\n",
            "Epoch [511/5000], Training Loss: 0.1946, Training Accuracy: 91.37%\n",
            "Epoch [512/5000], Training Loss: 0.1946, Training Accuracy: 91.37%\n",
            "Epoch [513/5000], Training Loss: 0.1945, Training Accuracy: 91.37%\n",
            "Epoch [514/5000], Training Loss: 0.1945, Training Accuracy: 91.37%\n",
            "Epoch [515/5000], Training Loss: 0.1944, Training Accuracy: 91.37%\n",
            "Epoch [516/5000], Training Loss: 0.1944, Training Accuracy: 91.37%\n",
            "Epoch [517/5000], Training Loss: 0.1943, Training Accuracy: 91.37%\n",
            "Epoch [518/5000], Training Loss: 0.1943, Training Accuracy: 91.37%\n",
            "Epoch [519/5000], Training Loss: 0.1942, Training Accuracy: 91.37%\n",
            "Epoch [520/5000], Training Loss: 0.1942, Training Accuracy: 91.37%\n",
            "Epoch [521/5000], Training Loss: 0.1941, Training Accuracy: 91.37%\n",
            "Epoch [522/5000], Training Loss: 0.1941, Training Accuracy: 91.37%\n",
            "Epoch [523/5000], Training Loss: 0.1940, Training Accuracy: 91.37%\n",
            "Epoch [524/5000], Training Loss: 0.1940, Training Accuracy: 91.37%\n",
            "Epoch [525/5000], Training Loss: 0.1939, Training Accuracy: 91.39%\n",
            "Epoch [526/5000], Training Loss: 0.1939, Training Accuracy: 91.39%\n",
            "Epoch [527/5000], Training Loss: 0.1938, Training Accuracy: 91.39%\n",
            "Epoch [528/5000], Training Loss: 0.1938, Training Accuracy: 91.39%\n",
            "Epoch [529/5000], Training Loss: 0.1937, Training Accuracy: 91.39%\n",
            "Epoch [530/5000], Training Loss: 0.1937, Training Accuracy: 91.39%\n",
            "Epoch [531/5000], Training Loss: 0.1936, Training Accuracy: 91.39%\n",
            "Epoch [532/5000], Training Loss: 0.1936, Training Accuracy: 91.39%\n",
            "Epoch [533/5000], Training Loss: 0.1935, Training Accuracy: 91.39%\n",
            "Epoch [534/5000], Training Loss: 0.1935, Training Accuracy: 91.39%\n",
            "Epoch [535/5000], Training Loss: 0.1934, Training Accuracy: 91.39%\n",
            "Epoch [536/5000], Training Loss: 0.1934, Training Accuracy: 91.39%\n",
            "Epoch [537/5000], Training Loss: 0.1933, Training Accuracy: 91.39%\n",
            "Epoch [538/5000], Training Loss: 0.1933, Training Accuracy: 91.39%\n",
            "Epoch [539/5000], Training Loss: 0.1932, Training Accuracy: 91.39%\n",
            "Epoch [540/5000], Training Loss: 0.1932, Training Accuracy: 91.39%\n",
            "Epoch [541/5000], Training Loss: 0.1931, Training Accuracy: 91.39%\n",
            "Epoch [542/5000], Training Loss: 0.1931, Training Accuracy: 91.39%\n",
            "Epoch [543/5000], Training Loss: 0.1930, Training Accuracy: 91.39%\n",
            "Epoch [544/5000], Training Loss: 0.1930, Training Accuracy: 91.39%\n",
            "Epoch [545/5000], Training Loss: 0.1929, Training Accuracy: 91.39%\n",
            "Epoch [546/5000], Training Loss: 0.1929, Training Accuracy: 91.39%\n",
            "Epoch [547/5000], Training Loss: 0.1928, Training Accuracy: 91.39%\n",
            "Epoch [548/5000], Training Loss: 0.1928, Training Accuracy: 91.39%\n",
            "Epoch [549/5000], Training Loss: 0.1928, Training Accuracy: 91.39%\n",
            "Epoch [550/5000], Training Loss: 0.1927, Training Accuracy: 91.39%\n",
            "Epoch [551/5000], Training Loss: 0.1927, Training Accuracy: 91.39%\n",
            "Epoch [552/5000], Training Loss: 0.1926, Training Accuracy: 91.39%\n",
            "Epoch [553/5000], Training Loss: 0.1926, Training Accuracy: 91.23%\n",
            "Epoch [554/5000], Training Loss: 0.1925, Training Accuracy: 91.23%\n",
            "Epoch [555/5000], Training Loss: 0.1925, Training Accuracy: 91.23%\n",
            "Epoch [556/5000], Training Loss: 0.1924, Training Accuracy: 91.23%\n",
            "Epoch [557/5000], Training Loss: 0.1924, Training Accuracy: 91.23%\n",
            "Epoch [558/5000], Training Loss: 0.1923, Training Accuracy: 91.23%\n",
            "Epoch [559/5000], Training Loss: 0.1923, Training Accuracy: 91.23%\n",
            "Epoch [560/5000], Training Loss: 0.1923, Training Accuracy: 91.23%\n",
            "Epoch [561/5000], Training Loss: 0.1922, Training Accuracy: 91.23%\n",
            "Epoch [562/5000], Training Loss: 0.1922, Training Accuracy: 91.23%\n",
            "Epoch [563/5000], Training Loss: 0.1921, Training Accuracy: 91.23%\n",
            "Epoch [564/5000], Training Loss: 0.1921, Training Accuracy: 91.23%\n",
            "Epoch [565/5000], Training Loss: 0.1920, Training Accuracy: 91.29%\n",
            "Epoch [566/5000], Training Loss: 0.1920, Training Accuracy: 91.29%\n",
            "Epoch [567/5000], Training Loss: 0.1920, Training Accuracy: 91.29%\n",
            "Epoch [568/5000], Training Loss: 0.1919, Training Accuracy: 91.29%\n",
            "Epoch [569/5000], Training Loss: 0.1919, Training Accuracy: 91.29%\n",
            "Epoch [570/5000], Training Loss: 0.1918, Training Accuracy: 91.29%\n",
            "Epoch [571/5000], Training Loss: 0.1918, Training Accuracy: 91.29%\n",
            "Epoch [572/5000], Training Loss: 0.1917, Training Accuracy: 91.29%\n",
            "Epoch [573/5000], Training Loss: 0.1917, Training Accuracy: 91.29%\n",
            "Epoch [574/5000], Training Loss: 0.1917, Training Accuracy: 91.29%\n",
            "Epoch [575/5000], Training Loss: 0.1916, Training Accuracy: 91.29%\n",
            "Epoch [576/5000], Training Loss: 0.1916, Training Accuracy: 91.29%\n",
            "Epoch [577/5000], Training Loss: 0.1915, Training Accuracy: 91.29%\n",
            "Epoch [578/5000], Training Loss: 0.1915, Training Accuracy: 91.29%\n",
            "Epoch [579/5000], Training Loss: 0.1914, Training Accuracy: 91.29%\n",
            "Epoch [580/5000], Training Loss: 0.1914, Training Accuracy: 91.29%\n",
            "Epoch [581/5000], Training Loss: 0.1914, Training Accuracy: 91.29%\n",
            "Epoch [582/5000], Training Loss: 0.1913, Training Accuracy: 91.29%\n",
            "Epoch [583/5000], Training Loss: 0.1913, Training Accuracy: 91.29%\n",
            "Epoch [584/5000], Training Loss: 0.1912, Training Accuracy: 91.29%\n",
            "Epoch [585/5000], Training Loss: 0.1912, Training Accuracy: 91.29%\n",
            "Epoch [586/5000], Training Loss: 0.1912, Training Accuracy: 91.29%\n",
            "Epoch [587/5000], Training Loss: 0.1911, Training Accuracy: 91.29%\n",
            "Epoch [588/5000], Training Loss: 0.1911, Training Accuracy: 91.29%\n",
            "Epoch [589/5000], Training Loss: 0.1910, Training Accuracy: 91.29%\n",
            "Epoch [590/5000], Training Loss: 0.1910, Training Accuracy: 91.29%\n",
            "Epoch [591/5000], Training Loss: 0.1910, Training Accuracy: 91.29%\n",
            "Epoch [592/5000], Training Loss: 0.1909, Training Accuracy: 91.29%\n",
            "Epoch [593/5000], Training Loss: 0.1909, Training Accuracy: 91.29%\n",
            "Epoch [594/5000], Training Loss: 0.1908, Training Accuracy: 91.29%\n",
            "Epoch [595/5000], Training Loss: 0.1908, Training Accuracy: 91.29%\n",
            "Epoch [596/5000], Training Loss: 0.1908, Training Accuracy: 91.29%\n",
            "Epoch [597/5000], Training Loss: 0.1907, Training Accuracy: 91.29%\n",
            "Epoch [598/5000], Training Loss: 0.1907, Training Accuracy: 91.29%\n",
            "Epoch [599/5000], Training Loss: 0.1907, Training Accuracy: 91.29%\n",
            "Epoch [600/5000], Training Loss: 0.1906, Training Accuracy: 91.29%\n",
            "Epoch [601/5000], Training Loss: 0.1906, Training Accuracy: 91.29%\n",
            "Epoch [602/5000], Training Loss: 0.1905, Training Accuracy: 91.29%\n",
            "Epoch [603/5000], Training Loss: 0.1905, Training Accuracy: 91.29%\n",
            "Epoch [604/5000], Training Loss: 0.1905, Training Accuracy: 91.29%\n",
            "Epoch [605/5000], Training Loss: 0.1904, Training Accuracy: 91.29%\n",
            "Epoch [606/5000], Training Loss: 0.1904, Training Accuracy: 91.29%\n",
            "Epoch [607/5000], Training Loss: 0.1904, Training Accuracy: 91.29%\n",
            "Epoch [608/5000], Training Loss: 0.1903, Training Accuracy: 91.29%\n",
            "Epoch [609/5000], Training Loss: 0.1903, Training Accuracy: 91.29%\n",
            "Epoch [610/5000], Training Loss: 0.1902, Training Accuracy: 91.29%\n",
            "Epoch [611/5000], Training Loss: 0.1902, Training Accuracy: 91.29%\n",
            "Epoch [612/5000], Training Loss: 0.1902, Training Accuracy: 91.29%\n",
            "Epoch [613/5000], Training Loss: 0.1901, Training Accuracy: 91.29%\n",
            "Epoch [614/5000], Training Loss: 0.1901, Training Accuracy: 91.29%\n",
            "Epoch [615/5000], Training Loss: 0.1901, Training Accuracy: 91.29%\n",
            "Epoch [616/5000], Training Loss: 0.1900, Training Accuracy: 91.29%\n",
            "Epoch [617/5000], Training Loss: 0.1900, Training Accuracy: 91.29%\n",
            "Epoch [618/5000], Training Loss: 0.1899, Training Accuracy: 91.29%\n",
            "Epoch [619/5000], Training Loss: 0.1899, Training Accuracy: 91.29%\n",
            "Epoch [620/5000], Training Loss: 0.1899, Training Accuracy: 91.29%\n",
            "Epoch [621/5000], Training Loss: 0.1898, Training Accuracy: 91.29%\n",
            "Epoch [622/5000], Training Loss: 0.1898, Training Accuracy: 91.29%\n",
            "Epoch [623/5000], Training Loss: 0.1898, Training Accuracy: 91.29%\n",
            "Epoch [624/5000], Training Loss: 0.1897, Training Accuracy: 91.29%\n",
            "Epoch [625/5000], Training Loss: 0.1897, Training Accuracy: 91.29%\n",
            "Epoch [626/5000], Training Loss: 0.1897, Training Accuracy: 91.29%\n",
            "Epoch [627/5000], Training Loss: 0.1896, Training Accuracy: 91.29%\n",
            "Epoch [628/5000], Training Loss: 0.1896, Training Accuracy: 91.29%\n",
            "Epoch [629/5000], Training Loss: 0.1896, Training Accuracy: 91.29%\n",
            "Epoch [630/5000], Training Loss: 0.1895, Training Accuracy: 91.29%\n",
            "Epoch [631/5000], Training Loss: 0.1895, Training Accuracy: 91.29%\n",
            "Epoch [632/5000], Training Loss: 0.1895, Training Accuracy: 91.29%\n",
            "Epoch [633/5000], Training Loss: 0.1894, Training Accuracy: 91.29%\n",
            "Epoch [634/5000], Training Loss: 0.1894, Training Accuracy: 91.29%\n",
            "Epoch [635/5000], Training Loss: 0.1894, Training Accuracy: 91.29%\n",
            "Epoch [636/5000], Training Loss: 0.1893, Training Accuracy: 91.29%\n",
            "Epoch [637/5000], Training Loss: 0.1893, Training Accuracy: 91.29%\n",
            "Epoch [638/5000], Training Loss: 0.1892, Training Accuracy: 91.29%\n",
            "Epoch [639/5000], Training Loss: 0.1892, Training Accuracy: 91.42%\n",
            "Epoch [640/5000], Training Loss: 0.1892, Training Accuracy: 91.42%\n",
            "Epoch [641/5000], Training Loss: 0.1891, Training Accuracy: 91.42%\n",
            "Epoch [642/5000], Training Loss: 0.1891, Training Accuracy: 91.42%\n",
            "Epoch [643/5000], Training Loss: 0.1891, Training Accuracy: 91.42%\n",
            "Epoch [644/5000], Training Loss: 0.1890, Training Accuracy: 91.42%\n",
            "Epoch [645/5000], Training Loss: 0.1890, Training Accuracy: 91.42%\n",
            "Epoch [646/5000], Training Loss: 0.1890, Training Accuracy: 91.42%\n",
            "Epoch [647/5000], Training Loss: 0.1889, Training Accuracy: 91.42%\n",
            "Epoch [648/5000], Training Loss: 0.1889, Training Accuracy: 91.42%\n",
            "Epoch [649/5000], Training Loss: 0.1889, Training Accuracy: 91.42%\n",
            "Epoch [650/5000], Training Loss: 0.1889, Training Accuracy: 91.42%\n",
            "Epoch [651/5000], Training Loss: 0.1888, Training Accuracy: 91.42%\n",
            "Epoch [652/5000], Training Loss: 0.1888, Training Accuracy: 91.42%\n",
            "Epoch [653/5000], Training Loss: 0.1888, Training Accuracy: 91.48%\n",
            "Epoch [654/5000], Training Loss: 0.1887, Training Accuracy: 91.48%\n",
            "Epoch [655/5000], Training Loss: 0.1887, Training Accuracy: 91.48%\n",
            "Epoch [656/5000], Training Loss: 0.1887, Training Accuracy: 91.48%\n",
            "Epoch [657/5000], Training Loss: 0.1886, Training Accuracy: 91.48%\n",
            "Epoch [658/5000], Training Loss: 0.1886, Training Accuracy: 91.48%\n",
            "Epoch [659/5000], Training Loss: 0.1886, Training Accuracy: 91.48%\n",
            "Epoch [660/5000], Training Loss: 0.1885, Training Accuracy: 91.48%\n",
            "Epoch [661/5000], Training Loss: 0.1885, Training Accuracy: 91.48%\n",
            "Epoch [662/5000], Training Loss: 0.1885, Training Accuracy: 91.48%\n",
            "Epoch [663/5000], Training Loss: 0.1884, Training Accuracy: 91.48%\n",
            "Epoch [664/5000], Training Loss: 0.1884, Training Accuracy: 91.48%\n",
            "Epoch [665/5000], Training Loss: 0.1884, Training Accuracy: 91.48%\n",
            "Epoch [666/5000], Training Loss: 0.1883, Training Accuracy: 91.48%\n",
            "Epoch [667/5000], Training Loss: 0.1883, Training Accuracy: 91.48%\n",
            "Epoch [668/5000], Training Loss: 0.1883, Training Accuracy: 91.48%\n",
            "Epoch [669/5000], Training Loss: 0.1883, Training Accuracy: 91.48%\n",
            "Epoch [670/5000], Training Loss: 0.1882, Training Accuracy: 91.48%\n",
            "Epoch [671/5000], Training Loss: 0.1882, Training Accuracy: 91.48%\n",
            "Epoch [672/5000], Training Loss: 0.1882, Training Accuracy: 91.48%\n",
            "Epoch [673/5000], Training Loss: 0.1881, Training Accuracy: 91.48%\n",
            "Epoch [674/5000], Training Loss: 0.1881, Training Accuracy: 91.48%\n",
            "Epoch [675/5000], Training Loss: 0.1881, Training Accuracy: 91.48%\n",
            "Epoch [676/5000], Training Loss: 0.1880, Training Accuracy: 91.48%\n",
            "Epoch [677/5000], Training Loss: 0.1880, Training Accuracy: 91.48%\n",
            "Epoch [678/5000], Training Loss: 0.1880, Training Accuracy: 91.48%\n",
            "Epoch [679/5000], Training Loss: 0.1880, Training Accuracy: 91.48%\n",
            "Epoch [680/5000], Training Loss: 0.1879, Training Accuracy: 91.48%\n",
            "Epoch [681/5000], Training Loss: 0.1879, Training Accuracy: 91.48%\n",
            "Epoch [682/5000], Training Loss: 0.1879, Training Accuracy: 91.48%\n",
            "Epoch [683/5000], Training Loss: 0.1878, Training Accuracy: 91.48%\n",
            "Epoch [684/5000], Training Loss: 0.1878, Training Accuracy: 91.48%\n",
            "Epoch [685/5000], Training Loss: 0.1878, Training Accuracy: 91.48%\n",
            "Epoch [686/5000], Training Loss: 0.1877, Training Accuracy: 91.48%\n",
            "Epoch [687/5000], Training Loss: 0.1877, Training Accuracy: 91.48%\n",
            "Epoch [688/5000], Training Loss: 0.1877, Training Accuracy: 91.48%\n",
            "Epoch [689/5000], Training Loss: 0.1877, Training Accuracy: 91.48%\n",
            "Epoch [690/5000], Training Loss: 0.1876, Training Accuracy: 91.48%\n",
            "Epoch [691/5000], Training Loss: 0.1876, Training Accuracy: 91.48%\n",
            "Epoch [692/5000], Training Loss: 0.1876, Training Accuracy: 91.48%\n",
            "Epoch [693/5000], Training Loss: 0.1875, Training Accuracy: 91.48%\n",
            "Epoch [694/5000], Training Loss: 0.1875, Training Accuracy: 91.48%\n",
            "Epoch [695/5000], Training Loss: 0.1875, Training Accuracy: 91.48%\n",
            "Epoch [696/5000], Training Loss: 0.1875, Training Accuracy: 91.48%\n",
            "Epoch [697/5000], Training Loss: 0.1874, Training Accuracy: 91.48%\n",
            "Epoch [698/5000], Training Loss: 0.1874, Training Accuracy: 91.48%\n",
            "Epoch [699/5000], Training Loss: 0.1874, Training Accuracy: 91.48%\n",
            "Epoch [700/5000], Training Loss: 0.1873, Training Accuracy: 91.48%\n",
            "Epoch [701/5000], Training Loss: 0.1873, Training Accuracy: 91.48%\n",
            "Epoch [702/5000], Training Loss: 0.1873, Training Accuracy: 91.48%\n",
            "Epoch [703/5000], Training Loss: 0.1873, Training Accuracy: 91.48%\n",
            "Epoch [704/5000], Training Loss: 0.1872, Training Accuracy: 91.48%\n",
            "Epoch [705/5000], Training Loss: 0.1872, Training Accuracy: 91.48%\n",
            "Epoch [706/5000], Training Loss: 0.1872, Training Accuracy: 91.48%\n",
            "Epoch [707/5000], Training Loss: 0.1872, Training Accuracy: 91.48%\n",
            "Epoch [708/5000], Training Loss: 0.1871, Training Accuracy: 91.48%\n",
            "Epoch [709/5000], Training Loss: 0.1871, Training Accuracy: 91.48%\n",
            "Epoch [710/5000], Training Loss: 0.1871, Training Accuracy: 91.48%\n",
            "Epoch [711/5000], Training Loss: 0.1870, Training Accuracy: 91.48%\n",
            "Epoch [712/5000], Training Loss: 0.1870, Training Accuracy: 91.48%\n",
            "Epoch [713/5000], Training Loss: 0.1870, Training Accuracy: 91.48%\n",
            "Epoch [714/5000], Training Loss: 0.1870, Training Accuracy: 91.48%\n",
            "Epoch [715/5000], Training Loss: 0.1869, Training Accuracy: 91.48%\n",
            "Epoch [716/5000], Training Loss: 0.1869, Training Accuracy: 91.48%\n",
            "Epoch [717/5000], Training Loss: 0.1869, Training Accuracy: 91.48%\n",
            "Epoch [718/5000], Training Loss: 0.1869, Training Accuracy: 91.48%\n",
            "Epoch [719/5000], Training Loss: 0.1868, Training Accuracy: 91.48%\n",
            "Epoch [720/5000], Training Loss: 0.1868, Training Accuracy: 91.48%\n",
            "Epoch [721/5000], Training Loss: 0.1868, Training Accuracy: 91.48%\n",
            "Epoch [722/5000], Training Loss: 0.1868, Training Accuracy: 91.48%\n",
            "Epoch [723/5000], Training Loss: 0.1867, Training Accuracy: 91.48%\n",
            "Epoch [724/5000], Training Loss: 0.1867, Training Accuracy: 91.48%\n",
            "Epoch [725/5000], Training Loss: 0.1867, Training Accuracy: 91.48%\n",
            "Epoch [726/5000], Training Loss: 0.1866, Training Accuracy: 91.48%\n",
            "Epoch [727/5000], Training Loss: 0.1866, Training Accuracy: 91.48%\n",
            "Epoch [728/5000], Training Loss: 0.1866, Training Accuracy: 91.54%\n",
            "Epoch [729/5000], Training Loss: 0.1866, Training Accuracy: 91.54%\n",
            "Epoch [730/5000], Training Loss: 0.1865, Training Accuracy: 91.54%\n",
            "Epoch [731/5000], Training Loss: 0.1865, Training Accuracy: 91.54%\n",
            "Epoch [732/5000], Training Loss: 0.1865, Training Accuracy: 91.54%\n",
            "Epoch [733/5000], Training Loss: 0.1865, Training Accuracy: 91.54%\n",
            "Epoch [734/5000], Training Loss: 0.1864, Training Accuracy: 91.54%\n",
            "Epoch [735/5000], Training Loss: 0.1864, Training Accuracy: 91.54%\n",
            "Epoch [736/5000], Training Loss: 0.1864, Training Accuracy: 91.54%\n",
            "Epoch [737/5000], Training Loss: 0.1864, Training Accuracy: 91.54%\n",
            "Epoch [738/5000], Training Loss: 0.1863, Training Accuracy: 91.54%\n",
            "Epoch [739/5000], Training Loss: 0.1863, Training Accuracy: 91.54%\n",
            "Epoch [740/5000], Training Loss: 0.1863, Training Accuracy: 91.54%\n",
            "Epoch [741/5000], Training Loss: 0.1863, Training Accuracy: 91.54%\n",
            "Epoch [742/5000], Training Loss: 0.1862, Training Accuracy: 91.54%\n",
            "Epoch [743/5000], Training Loss: 0.1862, Training Accuracy: 91.54%\n",
            "Epoch [744/5000], Training Loss: 0.1862, Training Accuracy: 91.54%\n",
            "Epoch [745/5000], Training Loss: 0.1862, Training Accuracy: 91.54%\n",
            "Epoch [746/5000], Training Loss: 0.1861, Training Accuracy: 91.54%\n",
            "Epoch [747/5000], Training Loss: 0.1861, Training Accuracy: 91.54%\n",
            "Epoch [748/5000], Training Loss: 0.1861, Training Accuracy: 91.54%\n",
            "Epoch [749/5000], Training Loss: 0.1861, Training Accuracy: 91.54%\n",
            "Epoch [750/5000], Training Loss: 0.1861, Training Accuracy: 91.54%\n",
            "Epoch [751/5000], Training Loss: 0.1860, Training Accuracy: 91.54%\n",
            "Epoch [752/5000], Training Loss: 0.1860, Training Accuracy: 91.54%\n",
            "Epoch [753/5000], Training Loss: 0.1860, Training Accuracy: 91.54%\n",
            "Epoch [754/5000], Training Loss: 0.1860, Training Accuracy: 91.54%\n",
            "Epoch [755/5000], Training Loss: 0.1859, Training Accuracy: 91.54%\n",
            "Epoch [756/5000], Training Loss: 0.1859, Training Accuracy: 91.54%\n",
            "Epoch [757/5000], Training Loss: 0.1859, Training Accuracy: 91.54%\n",
            "Epoch [758/5000], Training Loss: 0.1859, Training Accuracy: 91.54%\n",
            "Epoch [759/5000], Training Loss: 0.1858, Training Accuracy: 91.54%\n",
            "Epoch [760/5000], Training Loss: 0.1858, Training Accuracy: 91.54%\n",
            "Epoch [761/5000], Training Loss: 0.1858, Training Accuracy: 91.54%\n",
            "Epoch [762/5000], Training Loss: 0.1858, Training Accuracy: 91.54%\n",
            "Epoch [763/5000], Training Loss: 0.1857, Training Accuracy: 91.54%\n",
            "Epoch [764/5000], Training Loss: 0.1857, Training Accuracy: 91.54%\n",
            "Epoch [765/5000], Training Loss: 0.1857, Training Accuracy: 91.54%\n",
            "Epoch [766/5000], Training Loss: 0.1857, Training Accuracy: 91.39%\n",
            "Epoch [767/5000], Training Loss: 0.1857, Training Accuracy: 91.39%\n",
            "Epoch [768/5000], Training Loss: 0.1856, Training Accuracy: 91.39%\n",
            "Epoch [769/5000], Training Loss: 0.1856, Training Accuracy: 91.39%\n",
            "Epoch [770/5000], Training Loss: 0.1856, Training Accuracy: 91.39%\n",
            "Epoch [771/5000], Training Loss: 0.1856, Training Accuracy: 91.39%\n",
            "Epoch [772/5000], Training Loss: 0.1855, Training Accuracy: 91.39%\n",
            "Epoch [773/5000], Training Loss: 0.1855, Training Accuracy: 91.39%\n",
            "Epoch [774/5000], Training Loss: 0.1855, Training Accuracy: 91.39%\n",
            "Epoch [775/5000], Training Loss: 0.1855, Training Accuracy: 91.39%\n",
            "Epoch [776/5000], Training Loss: 0.1854, Training Accuracy: 91.39%\n",
            "Epoch [777/5000], Training Loss: 0.1854, Training Accuracy: 91.39%\n",
            "Epoch [778/5000], Training Loss: 0.1854, Training Accuracy: 91.39%\n",
            "Epoch [779/5000], Training Loss: 0.1854, Training Accuracy: 91.39%\n",
            "Epoch [780/5000], Training Loss: 0.1854, Training Accuracy: 91.39%\n",
            "Epoch [781/5000], Training Loss: 0.1853, Training Accuracy: 91.39%\n",
            "Epoch [782/5000], Training Loss: 0.1853, Training Accuracy: 91.39%\n",
            "Epoch [783/5000], Training Loss: 0.1853, Training Accuracy: 91.39%\n",
            "Epoch [784/5000], Training Loss: 0.1853, Training Accuracy: 91.39%\n",
            "Epoch [785/5000], Training Loss: 0.1852, Training Accuracy: 91.39%\n",
            "Epoch [786/5000], Training Loss: 0.1852, Training Accuracy: 91.39%\n",
            "Epoch [787/5000], Training Loss: 0.1852, Training Accuracy: 91.39%\n",
            "Epoch [788/5000], Training Loss: 0.1852, Training Accuracy: 91.39%\n",
            "Epoch [789/5000], Training Loss: 0.1852, Training Accuracy: 91.39%\n",
            "Epoch [790/5000], Training Loss: 0.1851, Training Accuracy: 91.39%\n",
            "Epoch [791/5000], Training Loss: 0.1851, Training Accuracy: 91.39%\n",
            "Epoch [792/5000], Training Loss: 0.1851, Training Accuracy: 91.39%\n",
            "Epoch [793/5000], Training Loss: 0.1851, Training Accuracy: 91.39%\n",
            "Epoch [794/5000], Training Loss: 0.1851, Training Accuracy: 91.39%\n",
            "Epoch [795/5000], Training Loss: 0.1850, Training Accuracy: 91.39%\n",
            "Epoch [796/5000], Training Loss: 0.1850, Training Accuracy: 91.39%\n",
            "Epoch [797/5000], Training Loss: 0.1850, Training Accuracy: 91.39%\n",
            "Epoch [798/5000], Training Loss: 0.1850, Training Accuracy: 91.39%\n",
            "Epoch [799/5000], Training Loss: 0.1849, Training Accuracy: 91.39%\n",
            "Epoch [800/5000], Training Loss: 0.1849, Training Accuracy: 91.39%\n",
            "Epoch [801/5000], Training Loss: 0.1849, Training Accuracy: 91.39%\n",
            "Epoch [802/5000], Training Loss: 0.1849, Training Accuracy: 91.39%\n",
            "Epoch [803/5000], Training Loss: 0.1849, Training Accuracy: 91.39%\n",
            "Epoch [804/5000], Training Loss: 0.1848, Training Accuracy: 91.39%\n",
            "Epoch [805/5000], Training Loss: 0.1848, Training Accuracy: 91.39%\n",
            "Epoch [806/5000], Training Loss: 0.1848, Training Accuracy: 91.39%\n",
            "Epoch [807/5000], Training Loss: 0.1848, Training Accuracy: 91.39%\n",
            "Epoch [808/5000], Training Loss: 0.1848, Training Accuracy: 91.39%\n",
            "Epoch [809/5000], Training Loss: 0.1847, Training Accuracy: 91.39%\n",
            "Epoch [810/5000], Training Loss: 0.1847, Training Accuracy: 91.39%\n",
            "Epoch [811/5000], Training Loss: 0.1847, Training Accuracy: 91.39%\n",
            "Epoch [812/5000], Training Loss: 0.1847, Training Accuracy: 91.39%\n",
            "Epoch [813/5000], Training Loss: 0.1847, Training Accuracy: 91.39%\n",
            "Epoch [814/5000], Training Loss: 0.1846, Training Accuracy: 91.39%\n",
            "Epoch [815/5000], Training Loss: 0.1846, Training Accuracy: 91.39%\n",
            "Epoch [816/5000], Training Loss: 0.1846, Training Accuracy: 91.39%\n",
            "Epoch [817/5000], Training Loss: 0.1846, Training Accuracy: 91.41%\n",
            "Epoch [818/5000], Training Loss: 0.1846, Training Accuracy: 91.41%\n",
            "Epoch [819/5000], Training Loss: 0.1845, Training Accuracy: 91.41%\n",
            "Epoch [820/5000], Training Loss: 0.1845, Training Accuracy: 91.41%\n",
            "Epoch [821/5000], Training Loss: 0.1845, Training Accuracy: 91.41%\n",
            "Epoch [822/5000], Training Loss: 0.1845, Training Accuracy: 91.41%\n",
            "Epoch [823/5000], Training Loss: 0.1845, Training Accuracy: 91.41%\n",
            "Epoch [824/5000], Training Loss: 0.1844, Training Accuracy: 91.41%\n",
            "Epoch [825/5000], Training Loss: 0.1844, Training Accuracy: 91.41%\n",
            "Epoch [826/5000], Training Loss: 0.1844, Training Accuracy: 91.41%\n",
            "Epoch [827/5000], Training Loss: 0.1844, Training Accuracy: 91.41%\n",
            "Epoch [828/5000], Training Loss: 0.1844, Training Accuracy: 91.41%\n",
            "Epoch [829/5000], Training Loss: 0.1843, Training Accuracy: 91.41%\n",
            "Epoch [830/5000], Training Loss: 0.1843, Training Accuracy: 91.41%\n",
            "Epoch [831/5000], Training Loss: 0.1843, Training Accuracy: 91.41%\n",
            "Epoch [832/5000], Training Loss: 0.1843, Training Accuracy: 91.41%\n",
            "Epoch [833/5000], Training Loss: 0.1843, Training Accuracy: 91.41%\n",
            "Epoch [834/5000], Training Loss: 0.1842, Training Accuracy: 91.41%\n",
            "Epoch [835/5000], Training Loss: 0.1842, Training Accuracy: 91.41%\n",
            "Epoch [836/5000], Training Loss: 0.1842, Training Accuracy: 91.41%\n",
            "Epoch [837/5000], Training Loss: 0.1842, Training Accuracy: 91.41%\n",
            "Epoch [838/5000], Training Loss: 0.1842, Training Accuracy: 91.41%\n",
            "Epoch [839/5000], Training Loss: 0.1841, Training Accuracy: 91.41%\n",
            "Epoch [840/5000], Training Loss: 0.1841, Training Accuracy: 91.41%\n",
            "Epoch [841/5000], Training Loss: 0.1841, Training Accuracy: 91.41%\n",
            "Epoch [842/5000], Training Loss: 0.1841, Training Accuracy: 91.41%\n",
            "Epoch [843/5000], Training Loss: 0.1841, Training Accuracy: 91.41%\n",
            "Epoch [844/5000], Training Loss: 0.1841, Training Accuracy: 91.41%\n",
            "Epoch [845/5000], Training Loss: 0.1840, Training Accuracy: 91.41%\n",
            "Epoch [846/5000], Training Loss: 0.1840, Training Accuracy: 91.41%\n",
            "Epoch [847/5000], Training Loss: 0.1840, Training Accuracy: 91.41%\n",
            "Epoch [848/5000], Training Loss: 0.1840, Training Accuracy: 91.41%\n",
            "Epoch [849/5000], Training Loss: 0.1840, Training Accuracy: 91.41%\n",
            "Epoch [850/5000], Training Loss: 0.1839, Training Accuracy: 91.41%\n",
            "Epoch [851/5000], Training Loss: 0.1839, Training Accuracy: 91.41%\n",
            "Epoch [852/5000], Training Loss: 0.1839, Training Accuracy: 91.41%\n",
            "Epoch [853/5000], Training Loss: 0.1839, Training Accuracy: 91.41%\n",
            "Epoch [854/5000], Training Loss: 0.1839, Training Accuracy: 91.41%\n",
            "Epoch [855/5000], Training Loss: 0.1839, Training Accuracy: 91.41%\n",
            "Epoch [856/5000], Training Loss: 0.1838, Training Accuracy: 91.41%\n",
            "Epoch [857/5000], Training Loss: 0.1838, Training Accuracy: 91.41%\n",
            "Epoch [858/5000], Training Loss: 0.1838, Training Accuracy: 91.41%\n",
            "Epoch [859/5000], Training Loss: 0.1838, Training Accuracy: 91.41%\n",
            "Epoch [860/5000], Training Loss: 0.1838, Training Accuracy: 91.41%\n",
            "Epoch [861/5000], Training Loss: 0.1837, Training Accuracy: 91.41%\n",
            "Epoch [862/5000], Training Loss: 0.1837, Training Accuracy: 91.41%\n",
            "Epoch [863/5000], Training Loss: 0.1837, Training Accuracy: 91.41%\n",
            "Epoch [864/5000], Training Loss: 0.1837, Training Accuracy: 91.41%\n",
            "Epoch [865/5000], Training Loss: 0.1837, Training Accuracy: 91.41%\n",
            "Epoch [866/5000], Training Loss: 0.1837, Training Accuracy: 91.41%\n",
            "Epoch [867/5000], Training Loss: 0.1836, Training Accuracy: 91.41%\n",
            "Epoch [868/5000], Training Loss: 0.1836, Training Accuracy: 91.41%\n",
            "Epoch [869/5000], Training Loss: 0.1836, Training Accuracy: 91.41%\n",
            "Epoch [870/5000], Training Loss: 0.1836, Training Accuracy: 91.41%\n",
            "Epoch [871/5000], Training Loss: 0.1836, Training Accuracy: 91.41%\n",
            "Epoch [872/5000], Training Loss: 0.1836, Training Accuracy: 91.41%\n",
            "Epoch [873/5000], Training Loss: 0.1835, Training Accuracy: 91.41%\n",
            "Epoch [874/5000], Training Loss: 0.1835, Training Accuracy: 91.41%\n",
            "Epoch [875/5000], Training Loss: 0.1835, Training Accuracy: 91.41%\n",
            "Epoch [876/5000], Training Loss: 0.1835, Training Accuracy: 91.41%\n",
            "Epoch [877/5000], Training Loss: 0.1835, Training Accuracy: 91.41%\n",
            "Epoch [878/5000], Training Loss: 0.1834, Training Accuracy: 91.41%\n",
            "Epoch [879/5000], Training Loss: 0.1834, Training Accuracy: 91.41%\n",
            "Epoch [880/5000], Training Loss: 0.1834, Training Accuracy: 91.41%\n",
            "Epoch [881/5000], Training Loss: 0.1834, Training Accuracy: 91.41%\n",
            "Epoch [882/5000], Training Loss: 0.1834, Training Accuracy: 91.41%\n",
            "Epoch [883/5000], Training Loss: 0.1834, Training Accuracy: 91.41%\n",
            "Epoch [884/5000], Training Loss: 0.1833, Training Accuracy: 91.41%\n",
            "Epoch [885/5000], Training Loss: 0.1833, Training Accuracy: 91.41%\n",
            "Epoch [886/5000], Training Loss: 0.1833, Training Accuracy: 91.41%\n",
            "Epoch [887/5000], Training Loss: 0.1833, Training Accuracy: 91.41%\n",
            "Epoch [888/5000], Training Loss: 0.1833, Training Accuracy: 91.41%\n",
            "Epoch [889/5000], Training Loss: 0.1833, Training Accuracy: 91.41%\n",
            "Epoch [890/5000], Training Loss: 0.1832, Training Accuracy: 91.41%\n",
            "Epoch [891/5000], Training Loss: 0.1832, Training Accuracy: 91.41%\n",
            "Epoch [892/5000], Training Loss: 0.1832, Training Accuracy: 91.41%\n",
            "Epoch [893/5000], Training Loss: 0.1832, Training Accuracy: 91.41%\n",
            "Epoch [894/5000], Training Loss: 0.1832, Training Accuracy: 91.41%\n",
            "Epoch [895/5000], Training Loss: 0.1832, Training Accuracy: 91.41%\n",
            "Epoch [896/5000], Training Loss: 0.1831, Training Accuracy: 91.41%\n",
            "Epoch [897/5000], Training Loss: 0.1831, Training Accuracy: 91.41%\n",
            "Epoch [898/5000], Training Loss: 0.1831, Training Accuracy: 91.41%\n",
            "Epoch [899/5000], Training Loss: 0.1831, Training Accuracy: 91.55%\n",
            "Epoch [900/5000], Training Loss: 0.1831, Training Accuracy: 91.55%\n",
            "Epoch [901/5000], Training Loss: 0.1831, Training Accuracy: 91.55%\n",
            "Epoch [902/5000], Training Loss: 0.1830, Training Accuracy: 91.55%\n",
            "Epoch [903/5000], Training Loss: 0.1830, Training Accuracy: 91.55%\n",
            "Epoch [904/5000], Training Loss: 0.1830, Training Accuracy: 91.55%\n",
            "Epoch [905/5000], Training Loss: 0.1830, Training Accuracy: 91.55%\n",
            "Epoch [906/5000], Training Loss: 0.1830, Training Accuracy: 91.55%\n",
            "Epoch [907/5000], Training Loss: 0.1830, Training Accuracy: 91.55%\n",
            "Epoch [908/5000], Training Loss: 0.1830, Training Accuracy: 91.55%\n",
            "Epoch [909/5000], Training Loss: 0.1829, Training Accuracy: 91.55%\n",
            "Epoch [910/5000], Training Loss: 0.1829, Training Accuracy: 91.55%\n",
            "Epoch [911/5000], Training Loss: 0.1829, Training Accuracy: 91.55%\n",
            "Epoch [912/5000], Training Loss: 0.1829, Training Accuracy: 91.55%\n",
            "Epoch [913/5000], Training Loss: 0.1829, Training Accuracy: 91.55%\n",
            "Epoch [914/5000], Training Loss: 0.1829, Training Accuracy: 91.55%\n",
            "Epoch [915/5000], Training Loss: 0.1828, Training Accuracy: 91.55%\n",
            "Epoch [916/5000], Training Loss: 0.1828, Training Accuracy: 91.55%\n",
            "Epoch [917/5000], Training Loss: 0.1828, Training Accuracy: 91.55%\n",
            "Epoch [918/5000], Training Loss: 0.1828, Training Accuracy: 91.55%\n",
            "Epoch [919/5000], Training Loss: 0.1828, Training Accuracy: 91.55%\n",
            "Epoch [920/5000], Training Loss: 0.1828, Training Accuracy: 91.55%\n",
            "Epoch [921/5000], Training Loss: 0.1828, Training Accuracy: 91.55%\n",
            "Epoch [922/5000], Training Loss: 0.1827, Training Accuracy: 91.55%\n",
            "Epoch [923/5000], Training Loss: 0.1827, Training Accuracy: 91.55%\n",
            "Epoch [924/5000], Training Loss: 0.1827, Training Accuracy: 91.55%\n",
            "Epoch [925/5000], Training Loss: 0.1827, Training Accuracy: 91.55%\n",
            "Epoch [926/5000], Training Loss: 0.1827, Training Accuracy: 91.55%\n",
            "Epoch [927/5000], Training Loss: 0.1827, Training Accuracy: 91.55%\n",
            "Epoch [928/5000], Training Loss: 0.1826, Training Accuracy: 91.55%\n",
            "Epoch [929/5000], Training Loss: 0.1826, Training Accuracy: 91.55%\n",
            "Epoch [930/5000], Training Loss: 0.1826, Training Accuracy: 91.55%\n",
            "Epoch [931/5000], Training Loss: 0.1826, Training Accuracy: 91.55%\n",
            "Epoch [932/5000], Training Loss: 0.1826, Training Accuracy: 91.55%\n",
            "Epoch [933/5000], Training Loss: 0.1826, Training Accuracy: 91.55%\n",
            "Epoch [934/5000], Training Loss: 0.1826, Training Accuracy: 91.55%\n",
            "Epoch [935/5000], Training Loss: 0.1825, Training Accuracy: 91.55%\n",
            "Epoch [936/5000], Training Loss: 0.1825, Training Accuracy: 91.55%\n",
            "Epoch [937/5000], Training Loss: 0.1825, Training Accuracy: 91.55%\n",
            "Epoch [938/5000], Training Loss: 0.1825, Training Accuracy: 91.55%\n",
            "Epoch [939/5000], Training Loss: 0.1825, Training Accuracy: 91.55%\n",
            "Epoch [940/5000], Training Loss: 0.1825, Training Accuracy: 91.55%\n",
            "Epoch [941/5000], Training Loss: 0.1824, Training Accuracy: 91.55%\n",
            "Epoch [942/5000], Training Loss: 0.1824, Training Accuracy: 91.55%\n",
            "Epoch [943/5000], Training Loss: 0.1824, Training Accuracy: 91.55%\n",
            "Epoch [944/5000], Training Loss: 0.1824, Training Accuracy: 91.55%\n",
            "Epoch [945/5000], Training Loss: 0.1824, Training Accuracy: 91.55%\n",
            "Epoch [946/5000], Training Loss: 0.1824, Training Accuracy: 91.55%\n",
            "Epoch [947/5000], Training Loss: 0.1824, Training Accuracy: 91.55%\n",
            "Epoch [948/5000], Training Loss: 0.1823, Training Accuracy: 91.55%\n",
            "Epoch [949/5000], Training Loss: 0.1823, Training Accuracy: 91.55%\n",
            "Epoch [950/5000], Training Loss: 0.1823, Training Accuracy: 91.55%\n",
            "Epoch [951/5000], Training Loss: 0.1823, Training Accuracy: 91.55%\n",
            "Epoch [952/5000], Training Loss: 0.1823, Training Accuracy: 91.55%\n",
            "Epoch [953/5000], Training Loss: 0.1823, Training Accuracy: 91.55%\n",
            "Epoch [954/5000], Training Loss: 0.1823, Training Accuracy: 91.55%\n",
            "Epoch [955/5000], Training Loss: 0.1822, Training Accuracy: 91.55%\n",
            "Epoch [956/5000], Training Loss: 0.1822, Training Accuracy: 91.55%\n",
            "Epoch [957/5000], Training Loss: 0.1822, Training Accuracy: 91.55%\n",
            "Epoch [958/5000], Training Loss: 0.1822, Training Accuracy: 91.55%\n",
            "Epoch [959/5000], Training Loss: 0.1822, Training Accuracy: 91.55%\n",
            "Epoch [960/5000], Training Loss: 0.1822, Training Accuracy: 91.55%\n",
            "Epoch [961/5000], Training Loss: 0.1822, Training Accuracy: 91.55%\n",
            "Epoch [962/5000], Training Loss: 0.1821, Training Accuracy: 91.55%\n",
            "Epoch [963/5000], Training Loss: 0.1821, Training Accuracy: 91.55%\n",
            "Epoch [964/5000], Training Loss: 0.1821, Training Accuracy: 91.55%\n",
            "Epoch [965/5000], Training Loss: 0.1821, Training Accuracy: 91.55%\n",
            "Epoch [966/5000], Training Loss: 0.1821, Training Accuracy: 91.55%\n",
            "Epoch [967/5000], Training Loss: 0.1821, Training Accuracy: 91.61%\n",
            "Epoch [968/5000], Training Loss: 0.1821, Training Accuracy: 91.61%\n",
            "Epoch [969/5000], Training Loss: 0.1821, Training Accuracy: 91.61%\n",
            "Epoch [970/5000], Training Loss: 0.1820, Training Accuracy: 91.61%\n",
            "Epoch [971/5000], Training Loss: 0.1820, Training Accuracy: 91.61%\n",
            "Epoch [972/5000], Training Loss: 0.1820, Training Accuracy: 91.61%\n",
            "Epoch [973/5000], Training Loss: 0.1820, Training Accuracy: 91.61%\n",
            "Epoch [974/5000], Training Loss: 0.1820, Training Accuracy: 91.61%\n",
            "Epoch [975/5000], Training Loss: 0.1820, Training Accuracy: 91.61%\n",
            "Epoch [976/5000], Training Loss: 0.1820, Training Accuracy: 91.61%\n",
            "Epoch [977/5000], Training Loss: 0.1819, Training Accuracy: 91.61%\n",
            "Epoch [978/5000], Training Loss: 0.1819, Training Accuracy: 91.61%\n",
            "Epoch [979/5000], Training Loss: 0.1819, Training Accuracy: 91.61%\n",
            "Epoch [980/5000], Training Loss: 0.1819, Training Accuracy: 91.61%\n",
            "Epoch [981/5000], Training Loss: 0.1819, Training Accuracy: 91.61%\n",
            "Epoch [982/5000], Training Loss: 0.1819, Training Accuracy: 91.61%\n",
            "Epoch [983/5000], Training Loss: 0.1819, Training Accuracy: 91.61%\n",
            "Epoch [984/5000], Training Loss: 0.1819, Training Accuracy: 91.61%\n",
            "Epoch [985/5000], Training Loss: 0.1818, Training Accuracy: 91.61%\n",
            "Epoch [986/5000], Training Loss: 0.1818, Training Accuracy: 91.61%\n",
            "Epoch [987/5000], Training Loss: 0.1818, Training Accuracy: 91.61%\n",
            "Epoch [988/5000], Training Loss: 0.1818, Training Accuracy: 91.61%\n",
            "Epoch [989/5000], Training Loss: 0.1818, Training Accuracy: 91.61%\n",
            "Epoch [990/5000], Training Loss: 0.1818, Training Accuracy: 91.61%\n",
            "Epoch [991/5000], Training Loss: 0.1818, Training Accuracy: 91.61%\n",
            "Epoch [992/5000], Training Loss: 0.1817, Training Accuracy: 91.61%\n",
            "Epoch [993/5000], Training Loss: 0.1817, Training Accuracy: 91.61%\n",
            "Epoch [994/5000], Training Loss: 0.1817, Training Accuracy: 91.61%\n",
            "Epoch [995/5000], Training Loss: 0.1817, Training Accuracy: 91.61%\n",
            "Epoch [996/5000], Training Loss: 0.1817, Training Accuracy: 91.61%\n",
            "Epoch [997/5000], Training Loss: 0.1817, Training Accuracy: 91.61%\n",
            "Epoch [998/5000], Training Loss: 0.1817, Training Accuracy: 91.61%\n",
            "Epoch [999/5000], Training Loss: 0.1817, Training Accuracy: 91.61%\n",
            "Epoch [1000/5000], Training Loss: 0.1816, Training Accuracy: 91.61%\n",
            "Epoch [1001/5000], Training Loss: 0.1816, Training Accuracy: 91.61%\n",
            "Epoch [1002/5000], Training Loss: 0.1816, Training Accuracy: 91.61%\n",
            "Epoch [1003/5000], Training Loss: 0.1816, Training Accuracy: 91.61%\n",
            "Epoch [1004/5000], Training Loss: 0.1816, Training Accuracy: 91.61%\n",
            "Epoch [1005/5000], Training Loss: 0.1816, Training Accuracy: 91.61%\n",
            "Epoch [1006/5000], Training Loss: 0.1816, Training Accuracy: 91.61%\n",
            "Epoch [1007/5000], Training Loss: 0.1816, Training Accuracy: 91.61%\n",
            "Epoch [1008/5000], Training Loss: 0.1815, Training Accuracy: 91.61%\n",
            "Epoch [1009/5000], Training Loss: 0.1815, Training Accuracy: 91.61%\n",
            "Epoch [1010/5000], Training Loss: 0.1815, Training Accuracy: 91.61%\n",
            "Epoch [1011/5000], Training Loss: 0.1815, Training Accuracy: 91.61%\n",
            "Epoch [1012/5000], Training Loss: 0.1815, Training Accuracy: 91.61%\n",
            "Epoch [1013/5000], Training Loss: 0.1815, Training Accuracy: 91.61%\n",
            "Epoch [1014/5000], Training Loss: 0.1815, Training Accuracy: 91.61%\n",
            "Epoch [1015/5000], Training Loss: 0.1815, Training Accuracy: 91.61%\n",
            "Epoch [1016/5000], Training Loss: 0.1814, Training Accuracy: 91.61%\n",
            "Epoch [1017/5000], Training Loss: 0.1814, Training Accuracy: 91.61%\n",
            "Epoch [1018/5000], Training Loss: 0.1814, Training Accuracy: 91.61%\n",
            "Epoch [1019/5000], Training Loss: 0.1814, Training Accuracy: 91.61%\n",
            "Epoch [1020/5000], Training Loss: 0.1814, Training Accuracy: 91.61%\n",
            "Epoch [1021/5000], Training Loss: 0.1814, Training Accuracy: 91.61%\n",
            "Epoch [1022/5000], Training Loss: 0.1814, Training Accuracy: 91.61%\n",
            "Epoch [1023/5000], Training Loss: 0.1814, Training Accuracy: 91.61%\n",
            "Epoch [1024/5000], Training Loss: 0.1813, Training Accuracy: 91.61%\n",
            "Epoch [1025/5000], Training Loss: 0.1813, Training Accuracy: 91.61%\n",
            "Epoch [1026/5000], Training Loss: 0.1813, Training Accuracy: 91.61%\n",
            "Epoch [1027/5000], Training Loss: 0.1813, Training Accuracy: 91.61%\n",
            "Epoch [1028/5000], Training Loss: 0.1813, Training Accuracy: 91.61%\n",
            "Epoch [1029/5000], Training Loss: 0.1813, Training Accuracy: 91.61%\n",
            "Epoch [1030/5000], Training Loss: 0.1813, Training Accuracy: 91.61%\n",
            "Epoch [1031/5000], Training Loss: 0.1813, Training Accuracy: 91.61%\n",
            "Epoch [1032/5000], Training Loss: 0.1812, Training Accuracy: 91.61%\n",
            "Epoch [1033/5000], Training Loss: 0.1812, Training Accuracy: 91.61%\n",
            "Epoch [1034/5000], Training Loss: 0.1812, Training Accuracy: 91.61%\n",
            "Epoch [1035/5000], Training Loss: 0.1812, Training Accuracy: 91.61%\n",
            "Epoch [1036/5000], Training Loss: 0.1812, Training Accuracy: 91.61%\n",
            "Epoch [1037/5000], Training Loss: 0.1812, Training Accuracy: 91.61%\n",
            "Epoch [1038/5000], Training Loss: 0.1812, Training Accuracy: 91.61%\n",
            "Epoch [1039/5000], Training Loss: 0.1812, Training Accuracy: 91.61%\n",
            "Epoch [1040/5000], Training Loss: 0.1812, Training Accuracy: 91.61%\n",
            "Epoch [1041/5000], Training Loss: 0.1811, Training Accuracy: 91.61%\n",
            "Epoch [1042/5000], Training Loss: 0.1811, Training Accuracy: 91.61%\n",
            "Epoch [1043/5000], Training Loss: 0.1811, Training Accuracy: 91.61%\n",
            "Epoch [1044/5000], Training Loss: 0.1811, Training Accuracy: 91.61%\n",
            "Epoch [1045/5000], Training Loss: 0.1811, Training Accuracy: 91.61%\n",
            "Epoch [1046/5000], Training Loss: 0.1811, Training Accuracy: 91.61%\n",
            "Epoch [1047/5000], Training Loss: 0.1811, Training Accuracy: 91.61%\n",
            "Epoch [1048/5000], Training Loss: 0.1811, Training Accuracy: 91.61%\n",
            "Epoch [1049/5000], Training Loss: 0.1811, Training Accuracy: 91.61%\n",
            "Epoch [1050/5000], Training Loss: 0.1810, Training Accuracy: 91.61%\n",
            "Epoch [1051/5000], Training Loss: 0.1810, Training Accuracy: 91.61%\n",
            "Epoch [1052/5000], Training Loss: 0.1810, Training Accuracy: 91.61%\n",
            "Epoch [1053/5000], Training Loss: 0.1810, Training Accuracy: 91.61%\n",
            "Epoch [1054/5000], Training Loss: 0.1810, Training Accuracy: 91.61%\n",
            "Epoch [1055/5000], Training Loss: 0.1810, Training Accuracy: 91.61%\n",
            "Epoch [1056/5000], Training Loss: 0.1810, Training Accuracy: 91.61%\n",
            "Epoch [1057/5000], Training Loss: 0.1810, Training Accuracy: 91.61%\n",
            "Epoch [1058/5000], Training Loss: 0.1809, Training Accuracy: 91.61%\n",
            "Epoch [1059/5000], Training Loss: 0.1809, Training Accuracy: 91.61%\n",
            "Epoch [1060/5000], Training Loss: 0.1809, Training Accuracy: 91.61%\n",
            "Epoch [1061/5000], Training Loss: 0.1809, Training Accuracy: 91.61%\n",
            "Epoch [1062/5000], Training Loss: 0.1809, Training Accuracy: 91.61%\n",
            "Epoch [1063/5000], Training Loss: 0.1809, Training Accuracy: 91.61%\n",
            "Epoch [1064/5000], Training Loss: 0.1809, Training Accuracy: 91.61%\n",
            "Epoch [1065/5000], Training Loss: 0.1809, Training Accuracy: 91.61%\n",
            "Epoch [1066/5000], Training Loss: 0.1809, Training Accuracy: 91.61%\n",
            "Epoch [1067/5000], Training Loss: 0.1808, Training Accuracy: 91.61%\n",
            "Epoch [1068/5000], Training Loss: 0.1808, Training Accuracy: 91.61%\n",
            "Epoch [1069/5000], Training Loss: 0.1808, Training Accuracy: 91.61%\n",
            "Epoch [1070/5000], Training Loss: 0.1808, Training Accuracy: 91.61%\n",
            "Epoch [1071/5000], Training Loss: 0.1808, Training Accuracy: 91.61%\n",
            "Epoch [1072/5000], Training Loss: 0.1808, Training Accuracy: 91.61%\n",
            "Epoch [1073/5000], Training Loss: 0.1808, Training Accuracy: 91.61%\n",
            "Epoch [1074/5000], Training Loss: 0.1808, Training Accuracy: 91.61%\n",
            "Epoch [1075/5000], Training Loss: 0.1808, Training Accuracy: 91.61%\n",
            "Epoch [1076/5000], Training Loss: 0.1808, Training Accuracy: 91.61%\n",
            "Epoch [1077/5000], Training Loss: 0.1807, Training Accuracy: 91.61%\n",
            "Epoch [1078/5000], Training Loss: 0.1807, Training Accuracy: 91.61%\n",
            "Epoch [1079/5000], Training Loss: 0.1807, Training Accuracy: 91.61%\n",
            "Epoch [1080/5000], Training Loss: 0.1807, Training Accuracy: 91.61%\n",
            "Epoch [1081/5000], Training Loss: 0.1807, Training Accuracy: 91.61%\n",
            "Epoch [1082/5000], Training Loss: 0.1807, Training Accuracy: 91.61%\n",
            "Epoch [1083/5000], Training Loss: 0.1807, Training Accuracy: 91.61%\n",
            "Epoch [1084/5000], Training Loss: 0.1807, Training Accuracy: 91.61%\n",
            "Epoch [1085/5000], Training Loss: 0.1807, Training Accuracy: 91.61%\n",
            "Epoch [1086/5000], Training Loss: 0.1806, Training Accuracy: 91.61%\n",
            "Epoch [1087/5000], Training Loss: 0.1806, Training Accuracy: 91.61%\n",
            "Epoch [1088/5000], Training Loss: 0.1806, Training Accuracy: 91.61%\n",
            "Epoch [1089/5000], Training Loss: 0.1806, Training Accuracy: 91.61%\n",
            "Epoch [1090/5000], Training Loss: 0.1806, Training Accuracy: 91.61%\n",
            "Epoch [1091/5000], Training Loss: 0.1806, Training Accuracy: 91.61%\n",
            "Epoch [1092/5000], Training Loss: 0.1806, Training Accuracy: 91.61%\n",
            "Epoch [1093/5000], Training Loss: 0.1806, Training Accuracy: 91.61%\n",
            "Epoch [1094/5000], Training Loss: 0.1806, Training Accuracy: 91.61%\n",
            "Epoch [1095/5000], Training Loss: 0.1805, Training Accuracy: 91.61%\n",
            "Epoch [1096/5000], Training Loss: 0.1805, Training Accuracy: 91.61%\n",
            "Epoch [1097/5000], Training Loss: 0.1805, Training Accuracy: 91.61%\n",
            "Epoch [1098/5000], Training Loss: 0.1805, Training Accuracy: 91.61%\n",
            "Epoch [1099/5000], Training Loss: 0.1805, Training Accuracy: 91.61%\n",
            "Epoch [1100/5000], Training Loss: 0.1805, Training Accuracy: 91.61%\n",
            "Epoch [1101/5000], Training Loss: 0.1805, Training Accuracy: 91.68%\n",
            "Epoch [1102/5000], Training Loss: 0.1805, Training Accuracy: 91.68%\n",
            "Epoch [1103/5000], Training Loss: 0.1805, Training Accuracy: 91.68%\n",
            "Epoch [1104/5000], Training Loss: 0.1805, Training Accuracy: 91.68%\n",
            "Epoch [1105/5000], Training Loss: 0.1804, Training Accuracy: 91.68%\n",
            "Epoch [1106/5000], Training Loss: 0.1804, Training Accuracy: 91.68%\n",
            "Epoch [1107/5000], Training Loss: 0.1804, Training Accuracy: 91.68%\n",
            "Epoch [1108/5000], Training Loss: 0.1804, Training Accuracy: 91.68%\n",
            "Epoch [1109/5000], Training Loss: 0.1804, Training Accuracy: 91.68%\n",
            "Epoch [1110/5000], Training Loss: 0.1804, Training Accuracy: 91.68%\n",
            "Epoch [1111/5000], Training Loss: 0.1804, Training Accuracy: 91.68%\n",
            "Epoch [1112/5000], Training Loss: 0.1804, Training Accuracy: 91.68%\n",
            "Epoch [1113/5000], Training Loss: 0.1804, Training Accuracy: 91.68%\n",
            "Epoch [1114/5000], Training Loss: 0.1804, Training Accuracy: 91.68%\n",
            "Epoch [1115/5000], Training Loss: 0.1803, Training Accuracy: 91.68%\n",
            "Epoch [1116/5000], Training Loss: 0.1803, Training Accuracy: 91.68%\n",
            "Epoch [1117/5000], Training Loss: 0.1803, Training Accuracy: 91.68%\n",
            "Epoch [1118/5000], Training Loss: 0.1803, Training Accuracy: 91.68%\n",
            "Epoch [1119/5000], Training Loss: 0.1803, Training Accuracy: 91.68%\n",
            "Epoch [1120/5000], Training Loss: 0.1803, Training Accuracy: 91.68%\n",
            "Epoch [1121/5000], Training Loss: 0.1803, Training Accuracy: 91.68%\n",
            "Epoch [1122/5000], Training Loss: 0.1803, Training Accuracy: 91.68%\n",
            "Epoch [1123/5000], Training Loss: 0.1803, Training Accuracy: 91.68%\n",
            "Epoch [1124/5000], Training Loss: 0.1803, Training Accuracy: 91.68%\n",
            "Epoch [1125/5000], Training Loss: 0.1802, Training Accuracy: 91.68%\n",
            "Epoch [1126/5000], Training Loss: 0.1802, Training Accuracy: 91.68%\n",
            "Epoch [1127/5000], Training Loss: 0.1802, Training Accuracy: 91.68%\n",
            "Epoch [1128/5000], Training Loss: 0.1802, Training Accuracy: 91.68%\n",
            "Epoch [1129/5000], Training Loss: 0.1802, Training Accuracy: 91.68%\n",
            "Epoch [1130/5000], Training Loss: 0.1802, Training Accuracy: 91.68%\n",
            "Epoch [1131/5000], Training Loss: 0.1802, Training Accuracy: 91.68%\n",
            "Epoch [1132/5000], Training Loss: 0.1802, Training Accuracy: 91.68%\n",
            "Epoch [1133/5000], Training Loss: 0.1802, Training Accuracy: 91.68%\n",
            "Epoch [1134/5000], Training Loss: 0.1802, Training Accuracy: 91.68%\n",
            "Epoch [1135/5000], Training Loss: 0.1802, Training Accuracy: 91.68%\n",
            "Epoch [1136/5000], Training Loss: 0.1801, Training Accuracy: 91.68%\n",
            "Epoch [1137/5000], Training Loss: 0.1801, Training Accuracy: 91.68%\n",
            "Epoch [1138/5000], Training Loss: 0.1801, Training Accuracy: 91.68%\n",
            "Epoch [1139/5000], Training Loss: 0.1801, Training Accuracy: 91.68%\n",
            "Epoch [1140/5000], Training Loss: 0.1801, Training Accuracy: 91.68%\n",
            "Epoch [1141/5000], Training Loss: 0.1801, Training Accuracy: 91.68%\n",
            "Epoch [1142/5000], Training Loss: 0.1801, Training Accuracy: 91.68%\n",
            "Epoch [1143/5000], Training Loss: 0.1801, Training Accuracy: 91.68%\n",
            "Epoch [1144/5000], Training Loss: 0.1801, Training Accuracy: 91.68%\n",
            "Epoch [1145/5000], Training Loss: 0.1801, Training Accuracy: 91.68%\n",
            "Epoch [1146/5000], Training Loss: 0.1800, Training Accuracy: 91.68%\n",
            "Epoch [1147/5000], Training Loss: 0.1800, Training Accuracy: 91.68%\n",
            "Epoch [1148/5000], Training Loss: 0.1800, Training Accuracy: 91.68%\n",
            "Epoch [1149/5000], Training Loss: 0.1800, Training Accuracy: 91.68%\n",
            "Epoch [1150/5000], Training Loss: 0.1800, Training Accuracy: 91.68%\n",
            "Epoch [1151/5000], Training Loss: 0.1800, Training Accuracy: 91.68%\n",
            "Epoch [1152/5000], Training Loss: 0.1800, Training Accuracy: 91.68%\n",
            "Epoch [1153/5000], Training Loss: 0.1800, Training Accuracy: 91.68%\n",
            "Epoch [1154/5000], Training Loss: 0.1800, Training Accuracy: 91.68%\n",
            "Epoch [1155/5000], Training Loss: 0.1800, Training Accuracy: 91.68%\n",
            "Epoch [1156/5000], Training Loss: 0.1800, Training Accuracy: 91.68%\n",
            "Epoch [1157/5000], Training Loss: 0.1799, Training Accuracy: 91.68%\n",
            "Epoch [1158/5000], Training Loss: 0.1799, Training Accuracy: 91.68%\n",
            "Epoch [1159/5000], Training Loss: 0.1799, Training Accuracy: 91.68%\n",
            "Epoch [1160/5000], Training Loss: 0.1799, Training Accuracy: 91.68%\n",
            "Epoch [1161/5000], Training Loss: 0.1799, Training Accuracy: 91.68%\n",
            "Epoch [1162/5000], Training Loss: 0.1799, Training Accuracy: 91.68%\n",
            "Epoch [1163/5000], Training Loss: 0.1799, Training Accuracy: 91.68%\n",
            "Epoch [1164/5000], Training Loss: 0.1799, Training Accuracy: 91.68%\n",
            "Epoch [1165/5000], Training Loss: 0.1799, Training Accuracy: 91.68%\n",
            "Epoch [1166/5000], Training Loss: 0.1799, Training Accuracy: 91.68%\n",
            "Epoch [1167/5000], Training Loss: 0.1799, Training Accuracy: 91.68%\n",
            "Epoch [1168/5000], Training Loss: 0.1799, Training Accuracy: 91.68%\n",
            "Epoch [1169/5000], Training Loss: 0.1798, Training Accuracy: 91.68%\n",
            "Epoch [1170/5000], Training Loss: 0.1798, Training Accuracy: 91.68%\n",
            "Epoch [1171/5000], Training Loss: 0.1798, Training Accuracy: 91.68%\n",
            "Epoch [1172/5000], Training Loss: 0.1798, Training Accuracy: 91.68%\n",
            "Epoch [1173/5000], Training Loss: 0.1798, Training Accuracy: 91.68%\n",
            "Epoch [1174/5000], Training Loss: 0.1798, Training Accuracy: 91.68%\n",
            "Epoch [1175/5000], Training Loss: 0.1798, Training Accuracy: 91.68%\n",
            "Epoch [1176/5000], Training Loss: 0.1798, Training Accuracy: 91.68%\n",
            "Epoch [1177/5000], Training Loss: 0.1798, Training Accuracy: 91.53%\n",
            "Epoch [1178/5000], Training Loss: 0.1798, Training Accuracy: 91.53%\n",
            "Epoch [1179/5000], Training Loss: 0.1798, Training Accuracy: 91.53%\n",
            "Epoch [1180/5000], Training Loss: 0.1797, Training Accuracy: 91.53%\n",
            "Epoch [1181/5000], Training Loss: 0.1797, Training Accuracy: 91.53%\n",
            "Epoch [1182/5000], Training Loss: 0.1797, Training Accuracy: 91.53%\n",
            "Epoch [1183/5000], Training Loss: 0.1797, Training Accuracy: 91.53%\n",
            "Epoch [1184/5000], Training Loss: 0.1797, Training Accuracy: 91.53%\n",
            "Epoch [1185/5000], Training Loss: 0.1797, Training Accuracy: 91.53%\n",
            "Epoch [1186/5000], Training Loss: 0.1797, Training Accuracy: 91.53%\n",
            "Epoch [1187/5000], Training Loss: 0.1797, Training Accuracy: 91.53%\n",
            "Epoch [1188/5000], Training Loss: 0.1797, Training Accuracy: 91.53%\n",
            "Epoch [1189/5000], Training Loss: 0.1797, Training Accuracy: 91.53%\n",
            "Epoch [1190/5000], Training Loss: 0.1797, Training Accuracy: 91.53%\n",
            "Epoch [1191/5000], Training Loss: 0.1797, Training Accuracy: 91.53%\n",
            "Epoch [1192/5000], Training Loss: 0.1796, Training Accuracy: 91.53%\n",
            "Epoch [1193/5000], Training Loss: 0.1796, Training Accuracy: 91.53%\n",
            "Epoch [1194/5000], Training Loss: 0.1796, Training Accuracy: 91.53%\n",
            "Epoch [1195/5000], Training Loss: 0.1796, Training Accuracy: 91.53%\n",
            "Epoch [1196/5000], Training Loss: 0.1796, Training Accuracy: 91.53%\n",
            "Epoch [1197/5000], Training Loss: 0.1796, Training Accuracy: 91.53%\n",
            "Epoch [1198/5000], Training Loss: 0.1796, Training Accuracy: 91.53%\n",
            "Epoch [1199/5000], Training Loss: 0.1796, Training Accuracy: 91.53%\n",
            "Epoch [1200/5000], Training Loss: 0.1796, Training Accuracy: 91.53%\n",
            "Epoch [1201/5000], Training Loss: 0.1796, Training Accuracy: 91.53%\n",
            "Epoch [1202/5000], Training Loss: 0.1796, Training Accuracy: 91.53%\n",
            "Epoch [1203/5000], Training Loss: 0.1796, Training Accuracy: 91.53%\n",
            "Epoch [1204/5000], Training Loss: 0.1795, Training Accuracy: 91.53%\n",
            "Epoch [1205/5000], Training Loss: 0.1795, Training Accuracy: 91.53%\n",
            "Epoch [1206/5000], Training Loss: 0.1795, Training Accuracy: 91.53%\n",
            "Epoch [1207/5000], Training Loss: 0.1795, Training Accuracy: 91.53%\n",
            "Epoch [1208/5000], Training Loss: 0.1795, Training Accuracy: 91.53%\n",
            "Epoch [1209/5000], Training Loss: 0.1795, Training Accuracy: 91.53%\n",
            "Epoch [1210/5000], Training Loss: 0.1795, Training Accuracy: 91.53%\n",
            "Epoch [1211/5000], Training Loss: 0.1795, Training Accuracy: 91.53%\n",
            "Epoch [1212/5000], Training Loss: 0.1795, Training Accuracy: 91.53%\n",
            "Epoch [1213/5000], Training Loss: 0.1795, Training Accuracy: 91.53%\n",
            "Epoch [1214/5000], Training Loss: 0.1795, Training Accuracy: 91.53%\n",
            "Epoch [1215/5000], Training Loss: 0.1795, Training Accuracy: 91.53%\n",
            "Epoch [1216/5000], Training Loss: 0.1794, Training Accuracy: 91.53%\n",
            "Epoch [1217/5000], Training Loss: 0.1794, Training Accuracy: 91.53%\n",
            "Epoch [1218/5000], Training Loss: 0.1794, Training Accuracy: 91.53%\n",
            "Epoch [1219/5000], Training Loss: 0.1794, Training Accuracy: 91.53%\n",
            "Epoch [1220/5000], Training Loss: 0.1794, Training Accuracy: 91.53%\n",
            "Epoch [1221/5000], Training Loss: 0.1794, Training Accuracy: 91.53%\n",
            "Epoch [1222/5000], Training Loss: 0.1794, Training Accuracy: 91.53%\n",
            "Epoch [1223/5000], Training Loss: 0.1794, Training Accuracy: 91.53%\n",
            "Epoch [1224/5000], Training Loss: 0.1794, Training Accuracy: 91.53%\n",
            "Epoch [1225/5000], Training Loss: 0.1794, Training Accuracy: 91.53%\n",
            "Epoch [1226/5000], Training Loss: 0.1794, Training Accuracy: 91.53%\n",
            "Epoch [1227/5000], Training Loss: 0.1794, Training Accuracy: 91.53%\n",
            "Epoch [1228/5000], Training Loss: 0.1793, Training Accuracy: 91.53%\n",
            "Epoch [1229/5000], Training Loss: 0.1793, Training Accuracy: 91.53%\n",
            "Epoch [1230/5000], Training Loss: 0.1793, Training Accuracy: 91.53%\n",
            "Epoch [1231/5000], Training Loss: 0.1793, Training Accuracy: 91.53%\n",
            "Epoch [1232/5000], Training Loss: 0.1793, Training Accuracy: 91.53%\n",
            "Epoch [1233/5000], Training Loss: 0.1793, Training Accuracy: 91.53%\n",
            "Epoch [1234/5000], Training Loss: 0.1793, Training Accuracy: 91.55%\n",
            "Epoch [1235/5000], Training Loss: 0.1793, Training Accuracy: 91.55%\n",
            "Epoch [1236/5000], Training Loss: 0.1793, Training Accuracy: 91.55%\n",
            "Epoch [1237/5000], Training Loss: 0.1793, Training Accuracy: 91.55%\n",
            "Epoch [1238/5000], Training Loss: 0.1793, Training Accuracy: 91.55%\n",
            "Epoch [1239/5000], Training Loss: 0.1793, Training Accuracy: 91.55%\n",
            "Epoch [1240/5000], Training Loss: 0.1793, Training Accuracy: 91.55%\n",
            "Epoch [1241/5000], Training Loss: 0.1792, Training Accuracy: 91.55%\n",
            "Epoch [1242/5000], Training Loss: 0.1792, Training Accuracy: 91.55%\n",
            "Epoch [1243/5000], Training Loss: 0.1792, Training Accuracy: 91.55%\n",
            "Epoch [1244/5000], Training Loss: 0.1792, Training Accuracy: 91.55%\n",
            "Epoch [1245/5000], Training Loss: 0.1792, Training Accuracy: 91.55%\n",
            "Epoch [1246/5000], Training Loss: 0.1792, Training Accuracy: 91.55%\n",
            "Epoch [1247/5000], Training Loss: 0.1792, Training Accuracy: 91.55%\n",
            "Epoch [1248/5000], Training Loss: 0.1792, Training Accuracy: 91.55%\n",
            "Epoch [1249/5000], Training Loss: 0.1792, Training Accuracy: 91.55%\n",
            "Epoch [1250/5000], Training Loss: 0.1792, Training Accuracy: 91.55%\n",
            "Epoch [1251/5000], Training Loss: 0.1792, Training Accuracy: 91.55%\n",
            "Epoch [1252/5000], Training Loss: 0.1792, Training Accuracy: 91.55%\n",
            "Epoch [1253/5000], Training Loss: 0.1792, Training Accuracy: 91.55%\n",
            "Epoch [1254/5000], Training Loss: 0.1791, Training Accuracy: 91.55%\n",
            "Epoch [1255/5000], Training Loss: 0.1791, Training Accuracy: 91.55%\n",
            "Epoch [1256/5000], Training Loss: 0.1791, Training Accuracy: 91.55%\n",
            "Epoch [1257/5000], Training Loss: 0.1791, Training Accuracy: 91.55%\n",
            "Epoch [1258/5000], Training Loss: 0.1791, Training Accuracy: 91.55%\n",
            "Epoch [1259/5000], Training Loss: 0.1791, Training Accuracy: 91.55%\n",
            "Epoch [1260/5000], Training Loss: 0.1791, Training Accuracy: 91.55%\n",
            "Epoch [1261/5000], Training Loss: 0.1791, Training Accuracy: 91.55%\n",
            "Epoch [1262/5000], Training Loss: 0.1791, Training Accuracy: 91.55%\n",
            "Epoch [1263/5000], Training Loss: 0.1791, Training Accuracy: 91.55%\n",
            "Epoch [1264/5000], Training Loss: 0.1791, Training Accuracy: 91.55%\n",
            "Epoch [1265/5000], Training Loss: 0.1791, Training Accuracy: 91.55%\n",
            "Epoch [1266/5000], Training Loss: 0.1791, Training Accuracy: 91.55%\n",
            "Epoch [1267/5000], Training Loss: 0.1791, Training Accuracy: 91.55%\n",
            "Epoch [1268/5000], Training Loss: 0.1790, Training Accuracy: 91.55%\n",
            "Epoch [1269/5000], Training Loss: 0.1790, Training Accuracy: 91.55%\n",
            "Epoch [1270/5000], Training Loss: 0.1790, Training Accuracy: 91.55%\n",
            "Epoch [1271/5000], Training Loss: 0.1790, Training Accuracy: 91.55%\n",
            "Epoch [1272/5000], Training Loss: 0.1790, Training Accuracy: 91.55%\n",
            "Epoch [1273/5000], Training Loss: 0.1790, Training Accuracy: 91.55%\n",
            "Epoch [1274/5000], Training Loss: 0.1790, Training Accuracy: 91.55%\n",
            "Epoch [1275/5000], Training Loss: 0.1790, Training Accuracy: 91.55%\n",
            "Epoch [1276/5000], Training Loss: 0.1790, Training Accuracy: 91.55%\n",
            "Epoch [1277/5000], Training Loss: 0.1790, Training Accuracy: 91.55%\n",
            "Epoch [1278/5000], Training Loss: 0.1790, Training Accuracy: 91.55%\n",
            "Epoch [1279/5000], Training Loss: 0.1790, Training Accuracy: 91.55%\n",
            "Epoch [1280/5000], Training Loss: 0.1790, Training Accuracy: 91.55%\n",
            "Epoch [1281/5000], Training Loss: 0.1790, Training Accuracy: 91.55%\n",
            "Epoch [1282/5000], Training Loss: 0.1789, Training Accuracy: 91.55%\n",
            "Epoch [1283/5000], Training Loss: 0.1789, Training Accuracy: 91.55%\n",
            "Epoch [1284/5000], Training Loss: 0.1789, Training Accuracy: 91.55%\n",
            "Epoch [1285/5000], Training Loss: 0.1789, Training Accuracy: 91.55%\n",
            "Epoch [1286/5000], Training Loss: 0.1789, Training Accuracy: 91.55%\n",
            "Epoch [1287/5000], Training Loss: 0.1789, Training Accuracy: 91.55%\n",
            "Epoch [1288/5000], Training Loss: 0.1789, Training Accuracy: 91.55%\n",
            "Epoch [1289/5000], Training Loss: 0.1789, Training Accuracy: 91.55%\n",
            "Epoch [1290/5000], Training Loss: 0.1789, Training Accuracy: 91.55%\n",
            "Epoch [1291/5000], Training Loss: 0.1789, Training Accuracy: 91.55%\n",
            "Epoch [1292/5000], Training Loss: 0.1789, Training Accuracy: 91.55%\n",
            "Epoch [1293/5000], Training Loss: 0.1789, Training Accuracy: 91.55%\n",
            "Epoch [1294/5000], Training Loss: 0.1789, Training Accuracy: 91.55%\n",
            "Epoch [1295/5000], Training Loss: 0.1789, Training Accuracy: 91.55%\n",
            "Epoch [1296/5000], Training Loss: 0.1788, Training Accuracy: 91.55%\n",
            "Epoch [1297/5000], Training Loss: 0.1788, Training Accuracy: 91.55%\n",
            "Epoch [1298/5000], Training Loss: 0.1788, Training Accuracy: 91.55%\n",
            "Epoch [1299/5000], Training Loss: 0.1788, Training Accuracy: 91.55%\n",
            "Epoch [1300/5000], Training Loss: 0.1788, Training Accuracy: 91.55%\n",
            "Epoch [1301/5000], Training Loss: 0.1788, Training Accuracy: 91.55%\n",
            "Epoch [1302/5000], Training Loss: 0.1788, Training Accuracy: 91.55%\n",
            "Epoch [1303/5000], Training Loss: 0.1788, Training Accuracy: 91.55%\n",
            "Epoch [1304/5000], Training Loss: 0.1788, Training Accuracy: 91.55%\n",
            "Epoch [1305/5000], Training Loss: 0.1788, Training Accuracy: 91.55%\n",
            "Epoch [1306/5000], Training Loss: 0.1788, Training Accuracy: 91.55%\n",
            "Epoch [1307/5000], Training Loss: 0.1788, Training Accuracy: 91.55%\n",
            "Epoch [1308/5000], Training Loss: 0.1788, Training Accuracy: 91.55%\n",
            "Epoch [1309/5000], Training Loss: 0.1788, Training Accuracy: 91.55%\n",
            "Epoch [1310/5000], Training Loss: 0.1788, Training Accuracy: 91.55%\n",
            "Epoch [1311/5000], Training Loss: 0.1787, Training Accuracy: 91.55%\n",
            "Epoch [1312/5000], Training Loss: 0.1787, Training Accuracy: 91.55%\n",
            "Epoch [1313/5000], Training Loss: 0.1787, Training Accuracy: 91.55%\n",
            "Epoch [1314/5000], Training Loss: 0.1787, Training Accuracy: 91.55%\n",
            "Epoch [1315/5000], Training Loss: 0.1787, Training Accuracy: 91.55%\n",
            "Epoch [1316/5000], Training Loss: 0.1787, Training Accuracy: 91.55%\n",
            "Epoch [1317/5000], Training Loss: 0.1787, Training Accuracy: 91.55%\n",
            "Epoch [1318/5000], Training Loss: 0.1787, Training Accuracy: 91.55%\n",
            "Epoch [1319/5000], Training Loss: 0.1787, Training Accuracy: 91.55%\n",
            "Epoch [1320/5000], Training Loss: 0.1787, Training Accuracy: 91.55%\n",
            "Epoch [1321/5000], Training Loss: 0.1787, Training Accuracy: 91.55%\n",
            "Epoch [1322/5000], Training Loss: 0.1787, Training Accuracy: 91.55%\n",
            "Epoch [1323/5000], Training Loss: 0.1787, Training Accuracy: 91.55%\n",
            "Epoch [1324/5000], Training Loss: 0.1787, Training Accuracy: 91.55%\n",
            "Epoch [1325/5000], Training Loss: 0.1787, Training Accuracy: 91.55%\n",
            "Epoch [1326/5000], Training Loss: 0.1786, Training Accuracy: 91.55%\n",
            "Epoch [1327/5000], Training Loss: 0.1786, Training Accuracy: 91.55%\n",
            "Epoch [1328/5000], Training Loss: 0.1786, Training Accuracy: 91.55%\n",
            "Epoch [1329/5000], Training Loss: 0.1786, Training Accuracy: 91.55%\n",
            "Epoch [1330/5000], Training Loss: 0.1786, Training Accuracy: 91.55%\n",
            "Epoch [1331/5000], Training Loss: 0.1786, Training Accuracy: 91.55%\n",
            "Epoch [1332/5000], Training Loss: 0.1786, Training Accuracy: 91.55%\n",
            "Epoch [1333/5000], Training Loss: 0.1786, Training Accuracy: 91.55%\n",
            "Epoch [1334/5000], Training Loss: 0.1786, Training Accuracy: 91.55%\n",
            "Epoch [1335/5000], Training Loss: 0.1786, Training Accuracy: 91.55%\n",
            "Epoch [1336/5000], Training Loss: 0.1786, Training Accuracy: 91.55%\n",
            "Epoch [1337/5000], Training Loss: 0.1786, Training Accuracy: 91.55%\n",
            "Epoch [1338/5000], Training Loss: 0.1786, Training Accuracy: 91.55%\n",
            "Epoch [1339/5000], Training Loss: 0.1786, Training Accuracy: 91.55%\n",
            "Epoch [1340/5000], Training Loss: 0.1786, Training Accuracy: 91.55%\n",
            "Epoch [1341/5000], Training Loss: 0.1786, Training Accuracy: 91.55%\n",
            "Epoch [1342/5000], Training Loss: 0.1785, Training Accuracy: 91.55%\n",
            "Epoch [1343/5000], Training Loss: 0.1785, Training Accuracy: 91.55%\n",
            "Epoch [1344/5000], Training Loss: 0.1785, Training Accuracy: 91.55%\n",
            "Epoch [1345/5000], Training Loss: 0.1785, Training Accuracy: 91.55%\n",
            "Epoch [1346/5000], Training Loss: 0.1785, Training Accuracy: 91.55%\n",
            "Epoch [1347/5000], Training Loss: 0.1785, Training Accuracy: 91.55%\n",
            "Epoch [1348/5000], Training Loss: 0.1785, Training Accuracy: 91.55%\n",
            "Epoch [1349/5000], Training Loss: 0.1785, Training Accuracy: 91.55%\n",
            "Epoch [1350/5000], Training Loss: 0.1785, Training Accuracy: 91.55%\n",
            "Epoch [1351/5000], Training Loss: 0.1785, Training Accuracy: 91.55%\n",
            "Epoch [1352/5000], Training Loss: 0.1785, Training Accuracy: 91.55%\n",
            "Epoch [1353/5000], Training Loss: 0.1785, Training Accuracy: 91.55%\n",
            "Epoch [1354/5000], Training Loss: 0.1785, Training Accuracy: 91.55%\n",
            "Epoch [1355/5000], Training Loss: 0.1785, Training Accuracy: 91.55%\n",
            "Epoch [1356/5000], Training Loss: 0.1785, Training Accuracy: 91.55%\n",
            "Epoch [1357/5000], Training Loss: 0.1785, Training Accuracy: 91.55%\n",
            "Epoch [1358/5000], Training Loss: 0.1784, Training Accuracy: 91.55%\n",
            "Epoch [1359/5000], Training Loss: 0.1784, Training Accuracy: 91.55%\n",
            "Epoch [1360/5000], Training Loss: 0.1784, Training Accuracy: 91.55%\n",
            "Epoch [1361/5000], Training Loss: 0.1784, Training Accuracy: 91.55%\n",
            "Epoch [1362/5000], Training Loss: 0.1784, Training Accuracy: 91.55%\n",
            "Epoch [1363/5000], Training Loss: 0.1784, Training Accuracy: 91.55%\n",
            "Epoch [1364/5000], Training Loss: 0.1784, Training Accuracy: 91.55%\n",
            "Epoch [1365/5000], Training Loss: 0.1784, Training Accuracy: 91.55%\n",
            "Epoch [1366/5000], Training Loss: 0.1784, Training Accuracy: 91.55%\n",
            "Epoch [1367/5000], Training Loss: 0.1784, Training Accuracy: 91.55%\n",
            "Epoch [1368/5000], Training Loss: 0.1784, Training Accuracy: 91.55%\n",
            "Epoch [1369/5000], Training Loss: 0.1784, Training Accuracy: 91.55%\n",
            "Epoch [1370/5000], Training Loss: 0.1784, Training Accuracy: 91.55%\n",
            "Epoch [1371/5000], Training Loss: 0.1784, Training Accuracy: 91.55%\n",
            "Epoch [1372/5000], Training Loss: 0.1784, Training Accuracy: 91.55%\n",
            "Epoch [1373/5000], Training Loss: 0.1784, Training Accuracy: 91.55%\n",
            "Epoch [1374/5000], Training Loss: 0.1783, Training Accuracy: 91.55%\n",
            "Epoch [1375/5000], Training Loss: 0.1783, Training Accuracy: 91.55%\n",
            "Epoch [1376/5000], Training Loss: 0.1783, Training Accuracy: 91.55%\n",
            "Epoch [1377/5000], Training Loss: 0.1783, Training Accuracy: 91.55%\n",
            "Epoch [1378/5000], Training Loss: 0.1783, Training Accuracy: 91.55%\n",
            "Epoch [1379/5000], Training Loss: 0.1783, Training Accuracy: 91.55%\n",
            "Epoch [1380/5000], Training Loss: 0.1783, Training Accuracy: 91.55%\n",
            "Epoch [1381/5000], Training Loss: 0.1783, Training Accuracy: 91.55%\n",
            "Epoch [1382/5000], Training Loss: 0.1783, Training Accuracy: 91.55%\n",
            "Epoch [1383/5000], Training Loss: 0.1783, Training Accuracy: 91.55%\n",
            "Epoch [1384/5000], Training Loss: 0.1783, Training Accuracy: 91.55%\n",
            "Epoch [1385/5000], Training Loss: 0.1783, Training Accuracy: 91.55%\n",
            "Epoch [1386/5000], Training Loss: 0.1783, Training Accuracy: 91.55%\n",
            "Epoch [1387/5000], Training Loss: 0.1783, Training Accuracy: 91.55%\n",
            "Epoch [1388/5000], Training Loss: 0.1783, Training Accuracy: 91.55%\n",
            "Epoch [1389/5000], Training Loss: 0.1783, Training Accuracy: 91.55%\n",
            "Epoch [1390/5000], Training Loss: 0.1783, Training Accuracy: 91.55%\n",
            "Epoch [1391/5000], Training Loss: 0.1783, Training Accuracy: 91.55%\n",
            "Epoch [1392/5000], Training Loss: 0.1782, Training Accuracy: 91.55%\n",
            "Epoch [1393/5000], Training Loss: 0.1782, Training Accuracy: 91.55%\n",
            "Epoch [1394/5000], Training Loss: 0.1782, Training Accuracy: 91.55%\n",
            "Epoch [1395/5000], Training Loss: 0.1782, Training Accuracy: 91.55%\n",
            "Epoch [1396/5000], Training Loss: 0.1782, Training Accuracy: 91.55%\n",
            "Epoch [1397/5000], Training Loss: 0.1782, Training Accuracy: 91.55%\n",
            "Epoch [1398/5000], Training Loss: 0.1782, Training Accuracy: 91.55%\n",
            "Epoch [1399/5000], Training Loss: 0.1782, Training Accuracy: 91.55%\n",
            "Epoch [1400/5000], Training Loss: 0.1782, Training Accuracy: 91.55%\n",
            "Epoch [1401/5000], Training Loss: 0.1782, Training Accuracy: 91.55%\n",
            "Epoch [1402/5000], Training Loss: 0.1782, Training Accuracy: 91.55%\n",
            "Epoch [1403/5000], Training Loss: 0.1782, Training Accuracy: 91.55%\n",
            "Epoch [1404/5000], Training Loss: 0.1782, Training Accuracy: 91.55%\n",
            "Epoch [1405/5000], Training Loss: 0.1782, Training Accuracy: 91.55%\n",
            "Epoch [1406/5000], Training Loss: 0.1782, Training Accuracy: 91.55%\n",
            "Epoch [1407/5000], Training Loss: 0.1782, Training Accuracy: 91.55%\n",
            "Epoch [1408/5000], Training Loss: 0.1782, Training Accuracy: 91.55%\n",
            "Epoch [1409/5000], Training Loss: 0.1781, Training Accuracy: 91.55%\n",
            "Epoch [1410/5000], Training Loss: 0.1781, Training Accuracy: 91.55%\n",
            "Epoch [1411/5000], Training Loss: 0.1781, Training Accuracy: 91.55%\n",
            "Epoch [1412/5000], Training Loss: 0.1781, Training Accuracy: 91.55%\n",
            "Epoch [1413/5000], Training Loss: 0.1781, Training Accuracy: 91.55%\n",
            "Epoch [1414/5000], Training Loss: 0.1781, Training Accuracy: 91.55%\n",
            "Epoch [1415/5000], Training Loss: 0.1781, Training Accuracy: 91.55%\n",
            "Epoch [1416/5000], Training Loss: 0.1781, Training Accuracy: 91.55%\n",
            "Epoch [1417/5000], Training Loss: 0.1781, Training Accuracy: 91.55%\n",
            "Epoch [1418/5000], Training Loss: 0.1781, Training Accuracy: 91.55%\n",
            "Epoch [1419/5000], Training Loss: 0.1781, Training Accuracy: 91.55%\n",
            "Epoch [1420/5000], Training Loss: 0.1781, Training Accuracy: 91.55%\n",
            "Epoch [1421/5000], Training Loss: 0.1781, Training Accuracy: 91.55%\n",
            "Epoch [1422/5000], Training Loss: 0.1781, Training Accuracy: 91.55%\n",
            "Epoch [1423/5000], Training Loss: 0.1781, Training Accuracy: 91.55%\n",
            "Epoch [1424/5000], Training Loss: 0.1781, Training Accuracy: 91.55%\n",
            "Epoch [1425/5000], Training Loss: 0.1781, Training Accuracy: 91.55%\n",
            "Epoch [1426/5000], Training Loss: 0.1781, Training Accuracy: 91.55%\n",
            "Epoch [1427/5000], Training Loss: 0.1780, Training Accuracy: 91.55%\n",
            "Epoch [1428/5000], Training Loss: 0.1780, Training Accuracy: 91.55%\n",
            "Epoch [1429/5000], Training Loss: 0.1780, Training Accuracy: 91.55%\n",
            "Epoch [1430/5000], Training Loss: 0.1780, Training Accuracy: 91.55%\n",
            "Epoch [1431/5000], Training Loss: 0.1780, Training Accuracy: 91.55%\n",
            "Epoch [1432/5000], Training Loss: 0.1780, Training Accuracy: 91.55%\n",
            "Epoch [1433/5000], Training Loss: 0.1780, Training Accuracy: 91.55%\n",
            "Epoch [1434/5000], Training Loss: 0.1780, Training Accuracy: 91.55%\n",
            "Epoch [1435/5000], Training Loss: 0.1780, Training Accuracy: 91.55%\n",
            "Epoch [1436/5000], Training Loss: 0.1780, Training Accuracy: 91.55%\n",
            "Epoch [1437/5000], Training Loss: 0.1780, Training Accuracy: 91.55%\n",
            "Epoch [1438/5000], Training Loss: 0.1780, Training Accuracy: 91.55%\n",
            "Epoch [1439/5000], Training Loss: 0.1780, Training Accuracy: 91.55%\n",
            "Epoch [1440/5000], Training Loss: 0.1780, Training Accuracy: 91.55%\n",
            "Epoch [1441/5000], Training Loss: 0.1780, Training Accuracy: 91.55%\n",
            "Epoch [1442/5000], Training Loss: 0.1780, Training Accuracy: 91.55%\n",
            "Epoch [1443/5000], Training Loss: 0.1780, Training Accuracy: 91.55%\n",
            "Epoch [1444/5000], Training Loss: 0.1780, Training Accuracy: 91.55%\n",
            "Epoch [1445/5000], Training Loss: 0.1780, Training Accuracy: 91.55%\n",
            "Epoch [1446/5000], Training Loss: 0.1779, Training Accuracy: 91.55%\n",
            "Epoch [1447/5000], Training Loss: 0.1779, Training Accuracy: 91.55%\n",
            "Epoch [1448/5000], Training Loss: 0.1779, Training Accuracy: 91.55%\n",
            "Epoch [1449/5000], Training Loss: 0.1779, Training Accuracy: 91.55%\n",
            "Epoch [1450/5000], Training Loss: 0.1779, Training Accuracy: 91.55%\n",
            "Epoch [1451/5000], Training Loss: 0.1779, Training Accuracy: 91.55%\n",
            "Epoch [1452/5000], Training Loss: 0.1779, Training Accuracy: 91.55%\n",
            "Epoch [1453/5000], Training Loss: 0.1779, Training Accuracy: 91.55%\n",
            "Epoch [1454/5000], Training Loss: 0.1779, Training Accuracy: 91.55%\n",
            "Epoch [1455/5000], Training Loss: 0.1779, Training Accuracy: 91.55%\n",
            "Epoch [1456/5000], Training Loss: 0.1779, Training Accuracy: 91.55%\n",
            "Epoch [1457/5000], Training Loss: 0.1779, Training Accuracy: 91.55%\n",
            "Epoch [1458/5000], Training Loss: 0.1779, Training Accuracy: 91.55%\n",
            "Epoch [1459/5000], Training Loss: 0.1779, Training Accuracy: 91.55%\n",
            "Epoch [1460/5000], Training Loss: 0.1779, Training Accuracy: 91.55%\n",
            "Epoch [1461/5000], Training Loss: 0.1779, Training Accuracy: 91.55%\n",
            "Epoch [1462/5000], Training Loss: 0.1779, Training Accuracy: 91.55%\n",
            "Epoch [1463/5000], Training Loss: 0.1779, Training Accuracy: 91.55%\n",
            "Epoch [1464/5000], Training Loss: 0.1779, Training Accuracy: 91.55%\n",
            "Epoch [1465/5000], Training Loss: 0.1779, Training Accuracy: 91.55%\n",
            "Epoch [1466/5000], Training Loss: 0.1778, Training Accuracy: 91.55%\n",
            "Epoch [1467/5000], Training Loss: 0.1778, Training Accuracy: 91.55%\n",
            "Epoch [1468/5000], Training Loss: 0.1778, Training Accuracy: 91.55%\n",
            "Epoch [1469/5000], Training Loss: 0.1778, Training Accuracy: 91.55%\n",
            "Epoch [1470/5000], Training Loss: 0.1778, Training Accuracy: 91.55%\n",
            "Epoch [1471/5000], Training Loss: 0.1778, Training Accuracy: 91.55%\n",
            "Epoch [1472/5000], Training Loss: 0.1778, Training Accuracy: 91.55%\n",
            "Epoch [1473/5000], Training Loss: 0.1778, Training Accuracy: 91.55%\n",
            "Epoch [1474/5000], Training Loss: 0.1778, Training Accuracy: 91.55%\n",
            "Epoch [1475/5000], Training Loss: 0.1778, Training Accuracy: 91.55%\n",
            "Epoch [1476/5000], Training Loss: 0.1778, Training Accuracy: 91.55%\n",
            "Epoch [1477/5000], Training Loss: 0.1778, Training Accuracy: 91.55%\n",
            "Epoch [1478/5000], Training Loss: 0.1778, Training Accuracy: 91.55%\n",
            "Epoch [1479/5000], Training Loss: 0.1778, Training Accuracy: 91.55%\n",
            "Epoch [1480/5000], Training Loss: 0.1778, Training Accuracy: 91.55%\n",
            "Epoch [1481/5000], Training Loss: 0.1778, Training Accuracy: 91.55%\n",
            "Epoch [1482/5000], Training Loss: 0.1778, Training Accuracy: 91.55%\n",
            "Epoch [1483/5000], Training Loss: 0.1778, Training Accuracy: 91.55%\n",
            "Epoch [1484/5000], Training Loss: 0.1778, Training Accuracy: 91.55%\n",
            "Epoch [1485/5000], Training Loss: 0.1778, Training Accuracy: 91.55%\n",
            "Epoch [1486/5000], Training Loss: 0.1777, Training Accuracy: 91.55%\n",
            "Epoch [1487/5000], Training Loss: 0.1777, Training Accuracy: 91.55%\n",
            "Epoch [1488/5000], Training Loss: 0.1777, Training Accuracy: 91.55%\n",
            "Epoch [1489/5000], Training Loss: 0.1777, Training Accuracy: 91.55%\n",
            "Epoch [1490/5000], Training Loss: 0.1777, Training Accuracy: 91.55%\n",
            "Epoch [1491/5000], Training Loss: 0.1777, Training Accuracy: 91.55%\n",
            "Epoch [1492/5000], Training Loss: 0.1777, Training Accuracy: 91.55%\n",
            "Epoch [1493/5000], Training Loss: 0.1777, Training Accuracy: 91.55%\n",
            "Epoch [1494/5000], Training Loss: 0.1777, Training Accuracy: 91.55%\n",
            "Epoch [1495/5000], Training Loss: 0.1777, Training Accuracy: 91.55%\n",
            "Epoch [1496/5000], Training Loss: 0.1777, Training Accuracy: 91.55%\n",
            "Epoch [1497/5000], Training Loss: 0.1777, Training Accuracy: 91.55%\n",
            "Epoch [1498/5000], Training Loss: 0.1777, Training Accuracy: 91.55%\n",
            "Epoch [1499/5000], Training Loss: 0.1777, Training Accuracy: 91.55%\n",
            "Epoch [1500/5000], Training Loss: 0.1777, Training Accuracy: 91.55%\n",
            "Epoch [1501/5000], Training Loss: 0.1777, Training Accuracy: 91.55%\n",
            "Epoch [1502/5000], Training Loss: 0.1777, Training Accuracy: 91.55%\n",
            "Epoch [1503/5000], Training Loss: 0.1777, Training Accuracy: 91.55%\n",
            "Epoch [1504/5000], Training Loss: 0.1777, Training Accuracy: 91.55%\n",
            "Epoch [1505/5000], Training Loss: 0.1777, Training Accuracy: 91.55%\n",
            "Epoch [1506/5000], Training Loss: 0.1777, Training Accuracy: 91.55%\n",
            "Epoch [1507/5000], Training Loss: 0.1776, Training Accuracy: 91.55%\n",
            "Epoch [1508/5000], Training Loss: 0.1776, Training Accuracy: 91.55%\n",
            "Epoch [1509/5000], Training Loss: 0.1776, Training Accuracy: 91.55%\n",
            "Epoch [1510/5000], Training Loss: 0.1776, Training Accuracy: 91.55%\n",
            "Epoch [1511/5000], Training Loss: 0.1776, Training Accuracy: 91.55%\n",
            "Epoch [1512/5000], Training Loss: 0.1776, Training Accuracy: 91.55%\n",
            "Epoch [1513/5000], Training Loss: 0.1776, Training Accuracy: 91.55%\n",
            "Epoch [1514/5000], Training Loss: 0.1776, Training Accuracy: 91.55%\n",
            "Epoch [1515/5000], Training Loss: 0.1776, Training Accuracy: 91.55%\n",
            "Epoch [1516/5000], Training Loss: 0.1776, Training Accuracy: 91.55%\n",
            "Epoch [1517/5000], Training Loss: 0.1776, Training Accuracy: 91.55%\n",
            "Epoch [1518/5000], Training Loss: 0.1776, Training Accuracy: 91.55%\n",
            "Epoch [1519/5000], Training Loss: 0.1776, Training Accuracy: 91.55%\n",
            "Epoch [1520/5000], Training Loss: 0.1776, Training Accuracy: 91.55%\n",
            "Epoch [1521/5000], Training Loss: 0.1776, Training Accuracy: 91.55%\n",
            "Epoch [1522/5000], Training Loss: 0.1776, Training Accuracy: 91.55%\n",
            "Epoch [1523/5000], Training Loss: 0.1776, Training Accuracy: 91.55%\n",
            "Epoch [1524/5000], Training Loss: 0.1776, Training Accuracy: 91.55%\n",
            "Epoch [1525/5000], Training Loss: 0.1776, Training Accuracy: 91.55%\n",
            "Epoch [1526/5000], Training Loss: 0.1776, Training Accuracy: 91.55%\n",
            "Epoch [1527/5000], Training Loss: 0.1776, Training Accuracy: 91.55%\n",
            "Epoch [1528/5000], Training Loss: 0.1776, Training Accuracy: 91.55%\n",
            "Epoch [1529/5000], Training Loss: 0.1775, Training Accuracy: 91.55%\n",
            "Epoch [1530/5000], Training Loss: 0.1775, Training Accuracy: 91.55%\n",
            "Epoch [1531/5000], Training Loss: 0.1775, Training Accuracy: 91.55%\n",
            "Epoch [1532/5000], Training Loss: 0.1775, Training Accuracy: 91.55%\n",
            "Epoch [1533/5000], Training Loss: 0.1775, Training Accuracy: 91.55%\n",
            "Epoch [1534/5000], Training Loss: 0.1775, Training Accuracy: 91.55%\n",
            "Epoch [1535/5000], Training Loss: 0.1775, Training Accuracy: 91.55%\n",
            "Epoch [1536/5000], Training Loss: 0.1775, Training Accuracy: 91.55%\n",
            "Epoch [1537/5000], Training Loss: 0.1775, Training Accuracy: 91.55%\n",
            "Epoch [1538/5000], Training Loss: 0.1775, Training Accuracy: 91.55%\n",
            "Epoch [1539/5000], Training Loss: 0.1775, Training Accuracy: 91.55%\n",
            "Epoch [1540/5000], Training Loss: 0.1775, Training Accuracy: 91.55%\n",
            "Epoch [1541/5000], Training Loss: 0.1775, Training Accuracy: 91.55%\n",
            "Epoch [1542/5000], Training Loss: 0.1775, Training Accuracy: 91.55%\n",
            "Epoch [1543/5000], Training Loss: 0.1775, Training Accuracy: 91.55%\n",
            "Epoch [1544/5000], Training Loss: 0.1775, Training Accuracy: 91.55%\n",
            "Epoch [1545/5000], Training Loss: 0.1775, Training Accuracy: 91.55%\n",
            "Epoch [1546/5000], Training Loss: 0.1775, Training Accuracy: 91.55%\n",
            "Epoch [1547/5000], Training Loss: 0.1775, Training Accuracy: 91.55%\n",
            "Epoch [1548/5000], Training Loss: 0.1775, Training Accuracy: 91.55%\n",
            "Epoch [1549/5000], Training Loss: 0.1775, Training Accuracy: 91.55%\n",
            "Epoch [1550/5000], Training Loss: 0.1775, Training Accuracy: 91.55%\n",
            "Epoch [1551/5000], Training Loss: 0.1774, Training Accuracy: 91.55%\n",
            "Epoch [1552/5000], Training Loss: 0.1774, Training Accuracy: 91.55%\n",
            "Epoch [1553/5000], Training Loss: 0.1774, Training Accuracy: 91.55%\n",
            "Epoch [1554/5000], Training Loss: 0.1774, Training Accuracy: 91.55%\n",
            "Epoch [1555/5000], Training Loss: 0.1774, Training Accuracy: 91.55%\n",
            "Epoch [1556/5000], Training Loss: 0.1774, Training Accuracy: 91.55%\n",
            "Epoch [1557/5000], Training Loss: 0.1774, Training Accuracy: 91.55%\n",
            "Epoch [1558/5000], Training Loss: 0.1774, Training Accuracy: 91.55%\n",
            "Epoch [1559/5000], Training Loss: 0.1774, Training Accuracy: 91.55%\n",
            "Epoch [1560/5000], Training Loss: 0.1774, Training Accuracy: 91.61%\n",
            "Epoch [1561/5000], Training Loss: 0.1774, Training Accuracy: 91.61%\n",
            "Epoch [1562/5000], Training Loss: 0.1774, Training Accuracy: 91.61%\n",
            "Epoch [1563/5000], Training Loss: 0.1774, Training Accuracy: 91.61%\n",
            "Epoch [1564/5000], Training Loss: 0.1774, Training Accuracy: 91.61%\n",
            "Epoch [1565/5000], Training Loss: 0.1774, Training Accuracy: 91.61%\n",
            "Epoch [1566/5000], Training Loss: 0.1774, Training Accuracy: 91.61%\n",
            "Epoch [1567/5000], Training Loss: 0.1774, Training Accuracy: 91.61%\n",
            "Epoch [1568/5000], Training Loss: 0.1774, Training Accuracy: 91.61%\n",
            "Epoch [1569/5000], Training Loss: 0.1774, Training Accuracy: 91.61%\n",
            "Epoch [1570/5000], Training Loss: 0.1774, Training Accuracy: 91.61%\n",
            "Epoch [1571/5000], Training Loss: 0.1774, Training Accuracy: 91.61%\n",
            "Epoch [1572/5000], Training Loss: 0.1774, Training Accuracy: 91.61%\n",
            "Epoch [1573/5000], Training Loss: 0.1774, Training Accuracy: 91.61%\n",
            "Epoch [1574/5000], Training Loss: 0.1774, Training Accuracy: 91.61%\n",
            "Epoch [1575/5000], Training Loss: 0.1773, Training Accuracy: 91.61%\n",
            "Epoch [1576/5000], Training Loss: 0.1773, Training Accuracy: 91.61%\n",
            "Epoch [1577/5000], Training Loss: 0.1773, Training Accuracy: 91.61%\n",
            "Epoch [1578/5000], Training Loss: 0.1773, Training Accuracy: 91.61%\n",
            "Epoch [1579/5000], Training Loss: 0.1773, Training Accuracy: 91.61%\n",
            "Epoch [1580/5000], Training Loss: 0.1773, Training Accuracy: 91.61%\n",
            "Epoch [1581/5000], Training Loss: 0.1773, Training Accuracy: 91.61%\n",
            "Epoch [1582/5000], Training Loss: 0.1773, Training Accuracy: 91.61%\n",
            "Epoch [1583/5000], Training Loss: 0.1773, Training Accuracy: 91.77%\n",
            "Epoch [1584/5000], Training Loss: 0.1773, Training Accuracy: 91.77%\n",
            "Epoch [1585/5000], Training Loss: 0.1773, Training Accuracy: 91.77%\n",
            "Epoch [1586/5000], Training Loss: 0.1773, Training Accuracy: 91.77%\n",
            "Epoch [1587/5000], Training Loss: 0.1773, Training Accuracy: 91.77%\n",
            "Epoch [1588/5000], Training Loss: 0.1773, Training Accuracy: 91.77%\n",
            "Epoch [1589/5000], Training Loss: 0.1773, Training Accuracy: 91.77%\n",
            "Epoch [1590/5000], Training Loss: 0.1773, Training Accuracy: 91.77%\n",
            "Epoch [1591/5000], Training Loss: 0.1773, Training Accuracy: 91.77%\n",
            "Epoch [1592/5000], Training Loss: 0.1773, Training Accuracy: 91.77%\n",
            "Epoch [1593/5000], Training Loss: 0.1773, Training Accuracy: 91.77%\n",
            "Epoch [1594/5000], Training Loss: 0.1773, Training Accuracy: 91.77%\n",
            "Epoch [1595/5000], Training Loss: 0.1773, Training Accuracy: 91.77%\n",
            "Epoch [1596/5000], Training Loss: 0.1773, Training Accuracy: 91.77%\n",
            "Epoch [1597/5000], Training Loss: 0.1773, Training Accuracy: 91.77%\n",
            "Epoch [1598/5000], Training Loss: 0.1773, Training Accuracy: 91.77%\n",
            "Epoch [1599/5000], Training Loss: 0.1772, Training Accuracy: 91.77%\n",
            "Epoch [1600/5000], Training Loss: 0.1772, Training Accuracy: 91.77%\n",
            "Epoch [1601/5000], Training Loss: 0.1772, Training Accuracy: 91.77%\n",
            "Epoch [1602/5000], Training Loss: 0.1772, Training Accuracy: 91.77%\n",
            "Epoch [1603/5000], Training Loss: 0.1772, Training Accuracy: 91.77%\n",
            "Epoch [1604/5000], Training Loss: 0.1772, Training Accuracy: 91.77%\n",
            "Epoch [1605/5000], Training Loss: 0.1772, Training Accuracy: 91.77%\n",
            "Epoch [1606/5000], Training Loss: 0.1772, Training Accuracy: 91.77%\n",
            "Epoch [1607/5000], Training Loss: 0.1772, Training Accuracy: 91.77%\n",
            "Epoch [1608/5000], Training Loss: 0.1772, Training Accuracy: 91.77%\n",
            "Epoch [1609/5000], Training Loss: 0.1772, Training Accuracy: 91.77%\n",
            "Epoch [1610/5000], Training Loss: 0.1772, Training Accuracy: 91.77%\n",
            "Epoch [1611/5000], Training Loss: 0.1772, Training Accuracy: 91.77%\n",
            "Epoch [1612/5000], Training Loss: 0.1772, Training Accuracy: 91.77%\n",
            "Epoch [1613/5000], Training Loss: 0.1772, Training Accuracy: 91.77%\n",
            "Epoch [1614/5000], Training Loss: 0.1772, Training Accuracy: 91.77%\n",
            "Epoch [1615/5000], Training Loss: 0.1772, Training Accuracy: 91.77%\n",
            "Epoch [1616/5000], Training Loss: 0.1772, Training Accuracy: 91.77%\n",
            "Epoch [1617/5000], Training Loss: 0.1772, Training Accuracy: 91.77%\n",
            "Epoch [1618/5000], Training Loss: 0.1772, Training Accuracy: 91.77%\n",
            "Epoch [1619/5000], Training Loss: 0.1772, Training Accuracy: 91.77%\n",
            "Epoch [1620/5000], Training Loss: 0.1772, Training Accuracy: 91.77%\n",
            "Epoch [1621/5000], Training Loss: 0.1772, Training Accuracy: 91.77%\n",
            "Epoch [1622/5000], Training Loss: 0.1772, Training Accuracy: 91.77%\n",
            "Epoch [1623/5000], Training Loss: 0.1772, Training Accuracy: 91.77%\n",
            "Epoch [1624/5000], Training Loss: 0.1772, Training Accuracy: 91.77%\n",
            "Epoch [1625/5000], Training Loss: 0.1771, Training Accuracy: 91.77%\n",
            "Epoch [1626/5000], Training Loss: 0.1771, Training Accuracy: 91.77%\n",
            "Epoch [1627/5000], Training Loss: 0.1771, Training Accuracy: 91.77%\n",
            "Epoch [1628/5000], Training Loss: 0.1771, Training Accuracy: 91.77%\n",
            "Epoch [1629/5000], Training Loss: 0.1771, Training Accuracy: 91.77%\n",
            "Epoch [1630/5000], Training Loss: 0.1771, Training Accuracy: 91.77%\n",
            "Epoch [1631/5000], Training Loss: 0.1771, Training Accuracy: 91.77%\n",
            "Epoch [1632/5000], Training Loss: 0.1771, Training Accuracy: 91.77%\n",
            "Epoch [1633/5000], Training Loss: 0.1771, Training Accuracy: 91.77%\n",
            "Epoch [1634/5000], Training Loss: 0.1771, Training Accuracy: 91.77%\n",
            "Epoch [1635/5000], Training Loss: 0.1771, Training Accuracy: 91.77%\n",
            "Epoch [1636/5000], Training Loss: 0.1771, Training Accuracy: 91.77%\n",
            "Epoch [1637/5000], Training Loss: 0.1771, Training Accuracy: 91.77%\n",
            "Epoch [1638/5000], Training Loss: 0.1771, Training Accuracy: 91.77%\n",
            "Epoch [1639/5000], Training Loss: 0.1771, Training Accuracy: 91.77%\n",
            "Epoch [1640/5000], Training Loss: 0.1771, Training Accuracy: 91.77%\n",
            "Epoch [1641/5000], Training Loss: 0.1771, Training Accuracy: 91.77%\n",
            "Epoch [1642/5000], Training Loss: 0.1771, Training Accuracy: 91.77%\n",
            "Epoch [1643/5000], Training Loss: 0.1771, Training Accuracy: 91.77%\n",
            "Epoch [1644/5000], Training Loss: 0.1771, Training Accuracy: 91.77%\n",
            "Epoch [1645/5000], Training Loss: 0.1771, Training Accuracy: 91.77%\n",
            "Epoch [1646/5000], Training Loss: 0.1771, Training Accuracy: 91.77%\n",
            "Epoch [1647/5000], Training Loss: 0.1771, Training Accuracy: 91.77%\n",
            "Epoch [1648/5000], Training Loss: 0.1771, Training Accuracy: 91.77%\n",
            "Epoch [1649/5000], Training Loss: 0.1771, Training Accuracy: 91.77%\n",
            "Epoch [1650/5000], Training Loss: 0.1771, Training Accuracy: 91.77%\n",
            "Epoch [1651/5000], Training Loss: 0.1771, Training Accuracy: 91.77%\n",
            "Epoch [1652/5000], Training Loss: 0.1770, Training Accuracy: 91.77%\n",
            "Epoch [1653/5000], Training Loss: 0.1770, Training Accuracy: 91.77%\n",
            "Epoch [1654/5000], Training Loss: 0.1770, Training Accuracy: 91.77%\n",
            "Epoch [1655/5000], Training Loss: 0.1770, Training Accuracy: 91.77%\n",
            "Epoch [1656/5000], Training Loss: 0.1770, Training Accuracy: 91.77%\n",
            "Epoch [1657/5000], Training Loss: 0.1770, Training Accuracy: 91.77%\n",
            "Epoch [1658/5000], Training Loss: 0.1770, Training Accuracy: 91.77%\n",
            "Epoch [1659/5000], Training Loss: 0.1770, Training Accuracy: 91.77%\n",
            "Epoch [1660/5000], Training Loss: 0.1770, Training Accuracy: 91.77%\n",
            "Epoch [1661/5000], Training Loss: 0.1770, Training Accuracy: 91.77%\n",
            "Epoch [1662/5000], Training Loss: 0.1770, Training Accuracy: 91.77%\n",
            "Epoch [1663/5000], Training Loss: 0.1770, Training Accuracy: 91.77%\n",
            "Epoch [1664/5000], Training Loss: 0.1770, Training Accuracy: 91.77%\n",
            "Epoch [1665/5000], Training Loss: 0.1770, Training Accuracy: 91.77%\n",
            "Epoch [1666/5000], Training Loss: 0.1770, Training Accuracy: 91.77%\n",
            "Epoch [1667/5000], Training Loss: 0.1770, Training Accuracy: 91.77%\n",
            "Epoch [1668/5000], Training Loss: 0.1770, Training Accuracy: 91.77%\n",
            "Epoch [1669/5000], Training Loss: 0.1770, Training Accuracy: 91.77%\n",
            "Epoch [1670/5000], Training Loss: 0.1770, Training Accuracy: 91.77%\n",
            "Epoch [1671/5000], Training Loss: 0.1770, Training Accuracy: 91.77%\n",
            "Epoch [1672/5000], Training Loss: 0.1770, Training Accuracy: 91.77%\n",
            "Epoch [1673/5000], Training Loss: 0.1770, Training Accuracy: 91.77%\n",
            "Epoch [1674/5000], Training Loss: 0.1770, Training Accuracy: 91.77%\n",
            "Epoch [1675/5000], Training Loss: 0.1770, Training Accuracy: 91.77%\n",
            "Epoch [1676/5000], Training Loss: 0.1770, Training Accuracy: 91.77%\n",
            "Epoch [1677/5000], Training Loss: 0.1770, Training Accuracy: 91.77%\n",
            "Epoch [1678/5000], Training Loss: 0.1770, Training Accuracy: 91.77%\n",
            "Epoch [1679/5000], Training Loss: 0.1769, Training Accuracy: 91.77%\n",
            "Epoch [1680/5000], Training Loss: 0.1769, Training Accuracy: 91.77%\n",
            "Epoch [1681/5000], Training Loss: 0.1769, Training Accuracy: 91.77%\n",
            "Epoch [1682/5000], Training Loss: 0.1769, Training Accuracy: 91.77%\n",
            "Epoch [1683/5000], Training Loss: 0.1769, Training Accuracy: 91.77%\n",
            "Epoch [1684/5000], Training Loss: 0.1769, Training Accuracy: 91.77%\n",
            "Epoch [1685/5000], Training Loss: 0.1769, Training Accuracy: 91.77%\n",
            "Epoch [1686/5000], Training Loss: 0.1769, Training Accuracy: 91.77%\n",
            "Epoch [1687/5000], Training Loss: 0.1769, Training Accuracy: 91.77%\n",
            "Epoch [1688/5000], Training Loss: 0.1769, Training Accuracy: 91.77%\n",
            "Epoch [1689/5000], Training Loss: 0.1769, Training Accuracy: 91.77%\n",
            "Epoch [1690/5000], Training Loss: 0.1769, Training Accuracy: 91.77%\n",
            "Epoch [1691/5000], Training Loss: 0.1769, Training Accuracy: 91.77%\n",
            "Epoch [1692/5000], Training Loss: 0.1769, Training Accuracy: 91.77%\n",
            "Epoch [1693/5000], Training Loss: 0.1769, Training Accuracy: 91.77%\n",
            "Epoch [1694/5000], Training Loss: 0.1769, Training Accuracy: 91.77%\n",
            "Epoch [1695/5000], Training Loss: 0.1769, Training Accuracy: 91.77%\n",
            "Epoch [1696/5000], Training Loss: 0.1769, Training Accuracy: 91.77%\n",
            "Epoch [1697/5000], Training Loss: 0.1769, Training Accuracy: 91.77%\n",
            "Epoch [1698/5000], Training Loss: 0.1769, Training Accuracy: 91.77%\n",
            "Epoch [1699/5000], Training Loss: 0.1769, Training Accuracy: 91.77%\n",
            "Epoch [1700/5000], Training Loss: 0.1769, Training Accuracy: 91.77%\n",
            "Epoch [1701/5000], Training Loss: 0.1769, Training Accuracy: 91.77%\n",
            "Epoch [1702/5000], Training Loss: 0.1769, Training Accuracy: 91.77%\n",
            "Epoch [1703/5000], Training Loss: 0.1769, Training Accuracy: 91.77%\n",
            "Epoch [1704/5000], Training Loss: 0.1769, Training Accuracy: 91.77%\n",
            "Epoch [1705/5000], Training Loss: 0.1769, Training Accuracy: 91.77%\n",
            "Epoch [1706/5000], Training Loss: 0.1769, Training Accuracy: 91.77%\n",
            "Epoch [1707/5000], Training Loss: 0.1769, Training Accuracy: 91.77%\n",
            "Epoch [1708/5000], Training Loss: 0.1769, Training Accuracy: 91.77%\n",
            "Epoch [1709/5000], Training Loss: 0.1768, Training Accuracy: 91.77%\n",
            "Epoch [1710/5000], Training Loss: 0.1768, Training Accuracy: 91.77%\n",
            "Epoch [1711/5000], Training Loss: 0.1768, Training Accuracy: 91.77%\n",
            "Epoch [1712/5000], Training Loss: 0.1768, Training Accuracy: 91.77%\n",
            "Epoch [1713/5000], Training Loss: 0.1768, Training Accuracy: 91.77%\n",
            "Epoch [1714/5000], Training Loss: 0.1768, Training Accuracy: 91.77%\n",
            "Epoch [1715/5000], Training Loss: 0.1768, Training Accuracy: 91.77%\n",
            "Epoch [1716/5000], Training Loss: 0.1768, Training Accuracy: 91.77%\n",
            "Epoch [1717/5000], Training Loss: 0.1768, Training Accuracy: 91.77%\n",
            "Epoch [1718/5000], Training Loss: 0.1768, Training Accuracy: 91.77%\n",
            "Epoch [1719/5000], Training Loss: 0.1768, Training Accuracy: 91.77%\n",
            "Epoch [1720/5000], Training Loss: 0.1768, Training Accuracy: 91.77%\n",
            "Epoch [1721/5000], Training Loss: 0.1768, Training Accuracy: 91.77%\n",
            "Epoch [1722/5000], Training Loss: 0.1768, Training Accuracy: 91.77%\n",
            "Epoch [1723/5000], Training Loss: 0.1768, Training Accuracy: 91.77%\n",
            "Epoch [1724/5000], Training Loss: 0.1768, Training Accuracy: 91.77%\n",
            "Epoch [1725/5000], Training Loss: 0.1768, Training Accuracy: 91.77%\n",
            "Epoch [1726/5000], Training Loss: 0.1768, Training Accuracy: 91.77%\n",
            "Epoch [1727/5000], Training Loss: 0.1768, Training Accuracy: 91.77%\n",
            "Epoch [1728/5000], Training Loss: 0.1768, Training Accuracy: 91.77%\n",
            "Epoch [1729/5000], Training Loss: 0.1768, Training Accuracy: 91.77%\n",
            "Epoch [1730/5000], Training Loss: 0.1768, Training Accuracy: 91.77%\n",
            "Epoch [1731/5000], Training Loss: 0.1768, Training Accuracy: 91.77%\n",
            "Epoch [1732/5000], Training Loss: 0.1768, Training Accuracy: 91.77%\n",
            "Epoch [1733/5000], Training Loss: 0.1768, Training Accuracy: 91.77%\n",
            "Epoch [1734/5000], Training Loss: 0.1768, Training Accuracy: 91.77%\n",
            "Epoch [1735/5000], Training Loss: 0.1768, Training Accuracy: 91.77%\n",
            "Epoch [1736/5000], Training Loss: 0.1768, Training Accuracy: 91.77%\n",
            "Epoch [1737/5000], Training Loss: 0.1768, Training Accuracy: 91.77%\n",
            "Epoch [1738/5000], Training Loss: 0.1768, Training Accuracy: 91.77%\n",
            "Epoch [1739/5000], Training Loss: 0.1767, Training Accuracy: 91.77%\n",
            "Epoch [1740/5000], Training Loss: 0.1767, Training Accuracy: 91.77%\n",
            "Epoch [1741/5000], Training Loss: 0.1767, Training Accuracy: 91.77%\n",
            "Epoch [1742/5000], Training Loss: 0.1767, Training Accuracy: 91.77%\n",
            "Epoch [1743/5000], Training Loss: 0.1767, Training Accuracy: 91.77%\n",
            "Epoch [1744/5000], Training Loss: 0.1767, Training Accuracy: 91.77%\n",
            "Epoch [1745/5000], Training Loss: 0.1767, Training Accuracy: 91.77%\n",
            "Epoch [1746/5000], Training Loss: 0.1767, Training Accuracy: 91.77%\n",
            "Epoch [1747/5000], Training Loss: 0.1767, Training Accuracy: 91.77%\n",
            "Epoch [1748/5000], Training Loss: 0.1767, Training Accuracy: 91.77%\n",
            "Epoch [1749/5000], Training Loss: 0.1767, Training Accuracy: 91.77%\n",
            "Epoch [1750/5000], Training Loss: 0.1767, Training Accuracy: 91.77%\n",
            "Epoch [1751/5000], Training Loss: 0.1767, Training Accuracy: 91.77%\n",
            "Epoch [1752/5000], Training Loss: 0.1767, Training Accuracy: 91.77%\n",
            "Epoch [1753/5000], Training Loss: 0.1767, Training Accuracy: 91.77%\n",
            "Epoch [1754/5000], Training Loss: 0.1767, Training Accuracy: 91.77%\n",
            "Epoch [1755/5000], Training Loss: 0.1767, Training Accuracy: 91.77%\n",
            "Epoch [1756/5000], Training Loss: 0.1767, Training Accuracy: 91.77%\n",
            "Epoch [1757/5000], Training Loss: 0.1767, Training Accuracy: 91.77%\n",
            "Epoch [1758/5000], Training Loss: 0.1767, Training Accuracy: 91.77%\n",
            "Epoch [1759/5000], Training Loss: 0.1767, Training Accuracy: 91.77%\n",
            "Epoch [1760/5000], Training Loss: 0.1767, Training Accuracy: 91.77%\n",
            "Epoch [1761/5000], Training Loss: 0.1767, Training Accuracy: 91.77%\n",
            "Epoch [1762/5000], Training Loss: 0.1767, Training Accuracy: 91.77%\n",
            "Epoch [1763/5000], Training Loss: 0.1767, Training Accuracy: 91.77%\n",
            "Epoch [1764/5000], Training Loss: 0.1767, Training Accuracy: 91.77%\n",
            "Epoch [1765/5000], Training Loss: 0.1767, Training Accuracy: 91.77%\n",
            "Epoch [1766/5000], Training Loss: 0.1767, Training Accuracy: 91.77%\n",
            "Epoch [1767/5000], Training Loss: 0.1767, Training Accuracy: 91.77%\n",
            "Epoch [1768/5000], Training Loss: 0.1767, Training Accuracy: 91.77%\n",
            "Epoch [1769/5000], Training Loss: 0.1767, Training Accuracy: 91.77%\n",
            "Epoch [1770/5000], Training Loss: 0.1767, Training Accuracy: 91.77%\n",
            "Epoch [1771/5000], Training Loss: 0.1767, Training Accuracy: 91.77%\n",
            "Epoch [1772/5000], Training Loss: 0.1766, Training Accuracy: 91.77%\n",
            "Epoch [1773/5000], Training Loss: 0.1766, Training Accuracy: 91.77%\n",
            "Epoch [1774/5000], Training Loss: 0.1766, Training Accuracy: 91.77%\n",
            "Epoch [1775/5000], Training Loss: 0.1766, Training Accuracy: 91.77%\n",
            "Epoch [1776/5000], Training Loss: 0.1766, Training Accuracy: 91.77%\n",
            "Epoch [1777/5000], Training Loss: 0.1766, Training Accuracy: 91.77%\n",
            "Epoch [1778/5000], Training Loss: 0.1766, Training Accuracy: 91.77%\n",
            "Epoch [1779/5000], Training Loss: 0.1766, Training Accuracy: 91.77%\n",
            "Epoch [1780/5000], Training Loss: 0.1766, Training Accuracy: 91.77%\n",
            "Epoch [1781/5000], Training Loss: 0.1766, Training Accuracy: 91.77%\n",
            "Epoch [1782/5000], Training Loss: 0.1766, Training Accuracy: 91.77%\n",
            "Epoch [1783/5000], Training Loss: 0.1766, Training Accuracy: 91.77%\n",
            "Epoch [1784/5000], Training Loss: 0.1766, Training Accuracy: 91.77%\n",
            "Epoch [1785/5000], Training Loss: 0.1766, Training Accuracy: 91.77%\n",
            "Epoch [1786/5000], Training Loss: 0.1766, Training Accuracy: 91.77%\n",
            "Epoch [1787/5000], Training Loss: 0.1766, Training Accuracy: 91.77%\n",
            "Epoch [1788/5000], Training Loss: 0.1766, Training Accuracy: 91.77%\n",
            "Epoch [1789/5000], Training Loss: 0.1766, Training Accuracy: 91.77%\n",
            "Epoch [1790/5000], Training Loss: 0.1766, Training Accuracy: 91.77%\n",
            "Epoch [1791/5000], Training Loss: 0.1766, Training Accuracy: 91.77%\n",
            "Epoch [1792/5000], Training Loss: 0.1766, Training Accuracy: 91.77%\n",
            "Epoch [1793/5000], Training Loss: 0.1766, Training Accuracy: 91.77%\n",
            "Epoch [1794/5000], Training Loss: 0.1766, Training Accuracy: 91.77%\n",
            "Epoch [1795/5000], Training Loss: 0.1766, Training Accuracy: 91.77%\n",
            "Epoch [1796/5000], Training Loss: 0.1766, Training Accuracy: 91.77%\n",
            "Epoch [1797/5000], Training Loss: 0.1766, Training Accuracy: 91.77%\n",
            "Epoch [1798/5000], Training Loss: 0.1766, Training Accuracy: 91.77%\n",
            "Epoch [1799/5000], Training Loss: 0.1766, Training Accuracy: 91.77%\n",
            "Epoch [1800/5000], Training Loss: 0.1766, Training Accuracy: 91.77%\n",
            "Epoch [1801/5000], Training Loss: 0.1766, Training Accuracy: 91.77%\n",
            "Epoch [1802/5000], Training Loss: 0.1766, Training Accuracy: 91.77%\n",
            "Epoch [1803/5000], Training Loss: 0.1766, Training Accuracy: 91.77%\n",
            "Epoch [1804/5000], Training Loss: 0.1766, Training Accuracy: 91.77%\n",
            "Epoch [1805/5000], Training Loss: 0.1766, Training Accuracy: 91.77%\n",
            "Epoch [1806/5000], Training Loss: 0.1765, Training Accuracy: 91.77%\n",
            "Epoch [1807/5000], Training Loss: 0.1765, Training Accuracy: 91.77%\n",
            "Epoch [1808/5000], Training Loss: 0.1765, Training Accuracy: 91.77%\n",
            "Epoch [1809/5000], Training Loss: 0.1765, Training Accuracy: 91.77%\n",
            "Epoch [1810/5000], Training Loss: 0.1765, Training Accuracy: 91.77%\n",
            "Epoch [1811/5000], Training Loss: 0.1765, Training Accuracy: 91.77%\n",
            "Epoch [1812/5000], Training Loss: 0.1765, Training Accuracy: 91.77%\n",
            "Epoch [1813/5000], Training Loss: 0.1765, Training Accuracy: 91.77%\n",
            "Epoch [1814/5000], Training Loss: 0.1765, Training Accuracy: 91.77%\n",
            "Epoch [1815/5000], Training Loss: 0.1765, Training Accuracy: 91.77%\n",
            "Epoch [1816/5000], Training Loss: 0.1765, Training Accuracy: 91.77%\n",
            "Epoch [1817/5000], Training Loss: 0.1765, Training Accuracy: 91.77%\n",
            "Epoch [1818/5000], Training Loss: 0.1765, Training Accuracy: 91.77%\n",
            "Epoch [1819/5000], Training Loss: 0.1765, Training Accuracy: 91.77%\n",
            "Epoch [1820/5000], Training Loss: 0.1765, Training Accuracy: 91.77%\n",
            "Epoch [1821/5000], Training Loss: 0.1765, Training Accuracy: 91.77%\n",
            "Epoch [1822/5000], Training Loss: 0.1765, Training Accuracy: 91.77%\n",
            "Epoch [1823/5000], Training Loss: 0.1765, Training Accuracy: 91.77%\n",
            "Epoch [1824/5000], Training Loss: 0.1765, Training Accuracy: 91.77%\n",
            "Epoch [1825/5000], Training Loss: 0.1765, Training Accuracy: 91.77%\n",
            "Epoch [1826/5000], Training Loss: 0.1765, Training Accuracy: 91.77%\n",
            "Epoch [1827/5000], Training Loss: 0.1765, Training Accuracy: 91.77%\n",
            "Epoch [1828/5000], Training Loss: 0.1765, Training Accuracy: 91.77%\n",
            "Epoch [1829/5000], Training Loss: 0.1765, Training Accuracy: 91.77%\n",
            "Epoch [1830/5000], Training Loss: 0.1765, Training Accuracy: 91.77%\n",
            "Epoch [1831/5000], Training Loss: 0.1765, Training Accuracy: 91.77%\n",
            "Epoch [1832/5000], Training Loss: 0.1765, Training Accuracy: 91.77%\n",
            "Epoch [1833/5000], Training Loss: 0.1765, Training Accuracy: 91.77%\n",
            "Epoch [1834/5000], Training Loss: 0.1765, Training Accuracy: 91.77%\n",
            "Epoch [1835/5000], Training Loss: 0.1765, Training Accuracy: 91.77%\n",
            "Epoch [1836/5000], Training Loss: 0.1765, Training Accuracy: 91.77%\n",
            "Epoch [1837/5000], Training Loss: 0.1765, Training Accuracy: 91.77%\n",
            "Epoch [1838/5000], Training Loss: 0.1765, Training Accuracy: 91.77%\n",
            "Epoch [1839/5000], Training Loss: 0.1765, Training Accuracy: 91.77%\n",
            "Epoch [1840/5000], Training Loss: 0.1765, Training Accuracy: 91.77%\n",
            "Epoch [1841/5000], Training Loss: 0.1765, Training Accuracy: 91.77%\n",
            "Epoch [1842/5000], Training Loss: 0.1764, Training Accuracy: 91.77%\n",
            "Epoch [1843/5000], Training Loss: 0.1764, Training Accuracy: 91.77%\n",
            "Epoch [1844/5000], Training Loss: 0.1764, Training Accuracy: 91.77%\n",
            "Epoch [1845/5000], Training Loss: 0.1764, Training Accuracy: 91.77%\n",
            "Epoch [1846/5000], Training Loss: 0.1764, Training Accuracy: 91.77%\n",
            "Epoch [1847/5000], Training Loss: 0.1764, Training Accuracy: 91.77%\n",
            "Epoch [1848/5000], Training Loss: 0.1764, Training Accuracy: 91.77%\n",
            "Epoch [1849/5000], Training Loss: 0.1764, Training Accuracy: 91.77%\n",
            "Epoch [1850/5000], Training Loss: 0.1764, Training Accuracy: 91.77%\n",
            "Epoch [1851/5000], Training Loss: 0.1764, Training Accuracy: 91.77%\n",
            "Epoch [1852/5000], Training Loss: 0.1764, Training Accuracy: 91.77%\n",
            "Epoch [1853/5000], Training Loss: 0.1764, Training Accuracy: 91.77%\n",
            "Epoch [1854/5000], Training Loss: 0.1764, Training Accuracy: 91.77%\n",
            "Epoch [1855/5000], Training Loss: 0.1764, Training Accuracy: 91.77%\n",
            "Epoch [1856/5000], Training Loss: 0.1764, Training Accuracy: 91.77%\n",
            "Epoch [1857/5000], Training Loss: 0.1764, Training Accuracy: 91.77%\n",
            "Epoch [1858/5000], Training Loss: 0.1764, Training Accuracy: 91.77%\n",
            "Epoch [1859/5000], Training Loss: 0.1764, Training Accuracy: 91.77%\n",
            "Epoch [1860/5000], Training Loss: 0.1764, Training Accuracy: 91.77%\n",
            "Epoch [1861/5000], Training Loss: 0.1764, Training Accuracy: 91.77%\n",
            "Epoch [1862/5000], Training Loss: 0.1764, Training Accuracy: 91.77%\n",
            "Epoch [1863/5000], Training Loss: 0.1764, Training Accuracy: 91.77%\n",
            "Epoch [1864/5000], Training Loss: 0.1764, Training Accuracy: 91.77%\n",
            "Epoch [1865/5000], Training Loss: 0.1764, Training Accuracy: 91.77%\n",
            "Epoch [1866/5000], Training Loss: 0.1764, Training Accuracy: 91.77%\n",
            "Epoch [1867/5000], Training Loss: 0.1764, Training Accuracy: 91.77%\n",
            "Epoch [1868/5000], Training Loss: 0.1764, Training Accuracy: 91.77%\n",
            "Epoch [1869/5000], Training Loss: 0.1764, Training Accuracy: 91.77%\n",
            "Epoch [1870/5000], Training Loss: 0.1764, Training Accuracy: 91.77%\n",
            "Epoch [1871/5000], Training Loss: 0.1764, Training Accuracy: 91.77%\n",
            "Epoch [1872/5000], Training Loss: 0.1764, Training Accuracy: 91.77%\n",
            "Epoch [1873/5000], Training Loss: 0.1764, Training Accuracy: 91.77%\n",
            "Epoch [1874/5000], Training Loss: 0.1764, Training Accuracy: 91.77%\n",
            "Epoch [1875/5000], Training Loss: 0.1764, Training Accuracy: 91.77%\n",
            "Epoch [1876/5000], Training Loss: 0.1764, Training Accuracy: 91.77%\n",
            "Epoch [1877/5000], Training Loss: 0.1764, Training Accuracy: 91.77%\n",
            "Epoch [1878/5000], Training Loss: 0.1764, Training Accuracy: 91.77%\n",
            "Epoch [1879/5000], Training Loss: 0.1764, Training Accuracy: 91.77%\n",
            "Epoch [1880/5000], Training Loss: 0.1763, Training Accuracy: 91.77%\n",
            "Epoch [1881/5000], Training Loss: 0.1763, Training Accuracy: 91.77%\n",
            "Epoch [1882/5000], Training Loss: 0.1763, Training Accuracy: 91.77%\n",
            "Epoch [1883/5000], Training Loss: 0.1763, Training Accuracy: 91.77%\n",
            "Epoch [1884/5000], Training Loss: 0.1763, Training Accuracy: 91.77%\n",
            "Epoch [1885/5000], Training Loss: 0.1763, Training Accuracy: 91.77%\n",
            "Epoch [1886/5000], Training Loss: 0.1763, Training Accuracy: 91.77%\n",
            "Epoch [1887/5000], Training Loss: 0.1763, Training Accuracy: 91.77%\n",
            "Epoch [1888/5000], Training Loss: 0.1763, Training Accuracy: 91.77%\n",
            "Epoch [1889/5000], Training Loss: 0.1763, Training Accuracy: 91.77%\n",
            "Epoch [1890/5000], Training Loss: 0.1763, Training Accuracy: 91.77%\n",
            "Epoch [1891/5000], Training Loss: 0.1763, Training Accuracy: 91.77%\n",
            "Epoch [1892/5000], Training Loss: 0.1763, Training Accuracy: 91.77%\n",
            "Epoch [1893/5000], Training Loss: 0.1763, Training Accuracy: 91.77%\n",
            "Epoch [1894/5000], Training Loss: 0.1763, Training Accuracy: 91.77%\n",
            "Epoch [1895/5000], Training Loss: 0.1763, Training Accuracy: 91.77%\n",
            "Epoch [1896/5000], Training Loss: 0.1763, Training Accuracy: 91.77%\n",
            "Epoch [1897/5000], Training Loss: 0.1763, Training Accuracy: 91.77%\n",
            "Epoch [1898/5000], Training Loss: 0.1763, Training Accuracy: 91.77%\n",
            "Epoch [1899/5000], Training Loss: 0.1763, Training Accuracy: 91.77%\n",
            "Epoch [1900/5000], Training Loss: 0.1763, Training Accuracy: 91.77%\n",
            "Epoch [1901/5000], Training Loss: 0.1763, Training Accuracy: 91.77%\n",
            "Epoch [1902/5000], Training Loss: 0.1763, Training Accuracy: 91.77%\n",
            "Epoch [1903/5000], Training Loss: 0.1763, Training Accuracy: 91.77%\n",
            "Epoch [1904/5000], Training Loss: 0.1763, Training Accuracy: 91.77%\n",
            "Epoch [1905/5000], Training Loss: 0.1763, Training Accuracy: 91.77%\n",
            "Epoch [1906/5000], Training Loss: 0.1763, Training Accuracy: 91.77%\n",
            "Epoch [1907/5000], Training Loss: 0.1763, Training Accuracy: 91.77%\n",
            "Epoch [1908/5000], Training Loss: 0.1763, Training Accuracy: 91.77%\n",
            "Epoch [1909/5000], Training Loss: 0.1763, Training Accuracy: 91.77%\n",
            "Epoch [1910/5000], Training Loss: 0.1763, Training Accuracy: 91.77%\n",
            "Epoch [1911/5000], Training Loss: 0.1763, Training Accuracy: 91.77%\n",
            "Epoch [1912/5000], Training Loss: 0.1763, Training Accuracy: 91.77%\n",
            "Epoch [1913/5000], Training Loss: 0.1763, Training Accuracy: 91.77%\n",
            "Epoch [1914/5000], Training Loss: 0.1763, Training Accuracy: 91.77%\n",
            "Epoch [1915/5000], Training Loss: 0.1763, Training Accuracy: 91.77%\n",
            "Epoch [1916/5000], Training Loss: 0.1763, Training Accuracy: 91.77%\n",
            "Epoch [1917/5000], Training Loss: 0.1763, Training Accuracy: 91.77%\n",
            "Epoch [1918/5000], Training Loss: 0.1763, Training Accuracy: 91.77%\n",
            "Epoch [1919/5000], Training Loss: 0.1763, Training Accuracy: 91.77%\n",
            "Epoch [1920/5000], Training Loss: 0.1762, Training Accuracy: 91.77%\n",
            "Epoch [1921/5000], Training Loss: 0.1762, Training Accuracy: 91.77%\n",
            "Epoch [1922/5000], Training Loss: 0.1762, Training Accuracy: 91.77%\n",
            "Epoch [1923/5000], Training Loss: 0.1762, Training Accuracy: 91.77%\n",
            "Epoch [1924/5000], Training Loss: 0.1762, Training Accuracy: 91.77%\n",
            "Epoch [1925/5000], Training Loss: 0.1762, Training Accuracy: 91.77%\n",
            "Epoch [1926/5000], Training Loss: 0.1762, Training Accuracy: 91.77%\n",
            "Epoch [1927/5000], Training Loss: 0.1762, Training Accuracy: 91.77%\n",
            "Epoch [1928/5000], Training Loss: 0.1762, Training Accuracy: 91.77%\n",
            "Epoch [1929/5000], Training Loss: 0.1762, Training Accuracy: 91.77%\n",
            "Epoch [1930/5000], Training Loss: 0.1762, Training Accuracy: 91.77%\n",
            "Epoch [1931/5000], Training Loss: 0.1762, Training Accuracy: 91.77%\n",
            "Epoch [1932/5000], Training Loss: 0.1762, Training Accuracy: 91.77%\n",
            "Epoch [1933/5000], Training Loss: 0.1762, Training Accuracy: 91.77%\n",
            "Epoch [1934/5000], Training Loss: 0.1762, Training Accuracy: 91.77%\n",
            "Epoch [1935/5000], Training Loss: 0.1762, Training Accuracy: 91.77%\n",
            "Epoch [1936/5000], Training Loss: 0.1762, Training Accuracy: 91.77%\n",
            "Epoch [1937/5000], Training Loss: 0.1762, Training Accuracy: 91.77%\n",
            "Epoch [1938/5000], Training Loss: 0.1762, Training Accuracy: 91.77%\n",
            "Epoch [1939/5000], Training Loss: 0.1762, Training Accuracy: 91.77%\n",
            "Epoch [1940/5000], Training Loss: 0.1762, Training Accuracy: 91.77%\n",
            "Epoch [1941/5000], Training Loss: 0.1762, Training Accuracy: 91.77%\n",
            "Epoch [1942/5000], Training Loss: 0.1762, Training Accuracy: 91.77%\n",
            "Epoch [1943/5000], Training Loss: 0.1762, Training Accuracy: 91.77%\n",
            "Epoch [1944/5000], Training Loss: 0.1762, Training Accuracy: 91.77%\n",
            "Epoch [1945/5000], Training Loss: 0.1762, Training Accuracy: 91.77%\n",
            "Epoch [1946/5000], Training Loss: 0.1762, Training Accuracy: 91.77%\n",
            "Epoch [1947/5000], Training Loss: 0.1762, Training Accuracy: 91.77%\n",
            "Epoch [1948/5000], Training Loss: 0.1762, Training Accuracy: 91.77%\n",
            "Epoch [1949/5000], Training Loss: 0.1762, Training Accuracy: 91.77%\n",
            "Epoch [1950/5000], Training Loss: 0.1762, Training Accuracy: 91.77%\n",
            "Epoch [1951/5000], Training Loss: 0.1762, Training Accuracy: 91.77%\n",
            "Epoch [1952/5000], Training Loss: 0.1762, Training Accuracy: 91.77%\n",
            "Epoch [1953/5000], Training Loss: 0.1762, Training Accuracy: 91.77%\n",
            "Epoch [1954/5000], Training Loss: 0.1762, Training Accuracy: 91.77%\n",
            "Epoch [1955/5000], Training Loss: 0.1762, Training Accuracy: 91.77%\n",
            "Epoch [1956/5000], Training Loss: 0.1762, Training Accuracy: 91.77%\n",
            "Epoch [1957/5000], Training Loss: 0.1762, Training Accuracy: 91.77%\n",
            "Epoch [1958/5000], Training Loss: 0.1762, Training Accuracy: 91.77%\n",
            "Epoch [1959/5000], Training Loss: 0.1762, Training Accuracy: 91.77%\n",
            "Epoch [1960/5000], Training Loss: 0.1762, Training Accuracy: 91.77%\n",
            "Epoch [1961/5000], Training Loss: 0.1762, Training Accuracy: 91.77%\n",
            "Epoch [1962/5000], Training Loss: 0.1762, Training Accuracy: 91.77%\n",
            "Epoch [1963/5000], Training Loss: 0.1761, Training Accuracy: 91.77%\n",
            "Epoch [1964/5000], Training Loss: 0.1761, Training Accuracy: 91.77%\n",
            "Epoch [1965/5000], Training Loss: 0.1761, Training Accuracy: 91.77%\n",
            "Epoch [1966/5000], Training Loss: 0.1761, Training Accuracy: 91.77%\n",
            "Epoch [1967/5000], Training Loss: 0.1761, Training Accuracy: 91.77%\n",
            "Epoch [1968/5000], Training Loss: 0.1761, Training Accuracy: 91.77%\n",
            "Epoch [1969/5000], Training Loss: 0.1761, Training Accuracy: 91.77%\n",
            "Epoch [1970/5000], Training Loss: 0.1761, Training Accuracy: 91.77%\n",
            "Epoch [1971/5000], Training Loss: 0.1761, Training Accuracy: 91.77%\n",
            "Epoch [1972/5000], Training Loss: 0.1761, Training Accuracy: 91.77%\n",
            "Epoch [1973/5000], Training Loss: 0.1761, Training Accuracy: 91.77%\n",
            "Epoch [1974/5000], Training Loss: 0.1761, Training Accuracy: 91.77%\n",
            "Epoch [1975/5000], Training Loss: 0.1761, Training Accuracy: 91.77%\n",
            "Epoch [1976/5000], Training Loss: 0.1761, Training Accuracy: 91.77%\n",
            "Epoch [1977/5000], Training Loss: 0.1761, Training Accuracy: 91.77%\n",
            "Epoch [1978/5000], Training Loss: 0.1761, Training Accuracy: 91.77%\n",
            "Epoch [1979/5000], Training Loss: 0.1761, Training Accuracy: 91.77%\n",
            "Epoch [1980/5000], Training Loss: 0.1761, Training Accuracy: 91.77%\n",
            "Epoch [1981/5000], Training Loss: 0.1761, Training Accuracy: 91.77%\n",
            "Epoch [1982/5000], Training Loss: 0.1761, Training Accuracy: 91.77%\n",
            "Epoch [1983/5000], Training Loss: 0.1761, Training Accuracy: 91.77%\n",
            "Epoch [1984/5000], Training Loss: 0.1761, Training Accuracy: 91.77%\n",
            "Epoch [1985/5000], Training Loss: 0.1761, Training Accuracy: 91.77%\n",
            "Epoch [1986/5000], Training Loss: 0.1761, Training Accuracy: 91.77%\n",
            "Epoch [1987/5000], Training Loss: 0.1761, Training Accuracy: 91.77%\n",
            "Epoch [1988/5000], Training Loss: 0.1761, Training Accuracy: 91.77%\n",
            "Epoch [1989/5000], Training Loss: 0.1761, Training Accuracy: 91.77%\n",
            "Epoch [1990/5000], Training Loss: 0.1761, Training Accuracy: 91.77%\n",
            "Epoch [1991/5000], Training Loss: 0.1761, Training Accuracy: 91.77%\n",
            "Epoch [1992/5000], Training Loss: 0.1761, Training Accuracy: 91.77%\n",
            "Epoch [1993/5000], Training Loss: 0.1761, Training Accuracy: 91.77%\n",
            "Epoch [1994/5000], Training Loss: 0.1761, Training Accuracy: 91.77%\n",
            "Epoch [1995/5000], Training Loss: 0.1761, Training Accuracy: 91.77%\n",
            "Epoch [1996/5000], Training Loss: 0.1761, Training Accuracy: 91.77%\n",
            "Epoch [1997/5000], Training Loss: 0.1761, Training Accuracy: 91.77%\n",
            "Epoch [1998/5000], Training Loss: 0.1761, Training Accuracy: 91.77%\n",
            "Epoch [1999/5000], Training Loss: 0.1761, Training Accuracy: 91.77%\n",
            "Epoch [2000/5000], Training Loss: 0.1761, Training Accuracy: 91.77%\n",
            "Epoch [2001/5000], Training Loss: 0.1761, Training Accuracy: 91.77%\n",
            "Epoch [2002/5000], Training Loss: 0.1761, Training Accuracy: 91.77%\n",
            "Epoch [2003/5000], Training Loss: 0.1761, Training Accuracy: 91.77%\n",
            "Epoch [2004/5000], Training Loss: 0.1761, Training Accuracy: 91.77%\n",
            "Epoch [2005/5000], Training Loss: 0.1761, Training Accuracy: 91.77%\n",
            "Epoch [2006/5000], Training Loss: 0.1761, Training Accuracy: 91.77%\n",
            "Epoch [2007/5000], Training Loss: 0.1761, Training Accuracy: 91.77%\n",
            "Epoch [2008/5000], Training Loss: 0.1761, Training Accuracy: 91.77%\n",
            "Epoch [2009/5000], Training Loss: 0.1760, Training Accuracy: 91.77%\n",
            "Epoch [2010/5000], Training Loss: 0.1760, Training Accuracy: 91.77%\n",
            "Epoch [2011/5000], Training Loss: 0.1760, Training Accuracy: 91.77%\n",
            "Epoch [2012/5000], Training Loss: 0.1760, Training Accuracy: 91.77%\n",
            "Epoch [2013/5000], Training Loss: 0.1760, Training Accuracy: 91.77%\n",
            "Epoch [2014/5000], Training Loss: 0.1760, Training Accuracy: 91.77%\n",
            "Epoch [2015/5000], Training Loss: 0.1760, Training Accuracy: 91.77%\n",
            "Epoch [2016/5000], Training Loss: 0.1760, Training Accuracy: 91.77%\n",
            "Epoch [2017/5000], Training Loss: 0.1760, Training Accuracy: 91.77%\n",
            "Epoch [2018/5000], Training Loss: 0.1760, Training Accuracy: 91.77%\n",
            "Epoch [2019/5000], Training Loss: 0.1760, Training Accuracy: 91.77%\n",
            "Epoch [2020/5000], Training Loss: 0.1760, Training Accuracy: 91.77%\n",
            "Epoch [2021/5000], Training Loss: 0.1760, Training Accuracy: 91.77%\n",
            "Epoch [2022/5000], Training Loss: 0.1760, Training Accuracy: 91.77%\n",
            "Epoch [2023/5000], Training Loss: 0.1760, Training Accuracy: 91.77%\n",
            "Epoch [2024/5000], Training Loss: 0.1760, Training Accuracy: 91.77%\n",
            "Epoch [2025/5000], Training Loss: 0.1760, Training Accuracy: 91.77%\n",
            "Epoch [2026/5000], Training Loss: 0.1760, Training Accuracy: 91.77%\n",
            "Epoch [2027/5000], Training Loss: 0.1760, Training Accuracy: 91.77%\n",
            "Epoch [2028/5000], Training Loss: 0.1760, Training Accuracy: 91.77%\n",
            "Epoch [2029/5000], Training Loss: 0.1760, Training Accuracy: 91.77%\n",
            "Epoch [2030/5000], Training Loss: 0.1760, Training Accuracy: 91.77%\n",
            "Epoch [2031/5000], Training Loss: 0.1760, Training Accuracy: 91.77%\n",
            "Epoch [2032/5000], Training Loss: 0.1760, Training Accuracy: 91.77%\n",
            "Epoch [2033/5000], Training Loss: 0.1760, Training Accuracy: 91.77%\n",
            "Epoch [2034/5000], Training Loss: 0.1760, Training Accuracy: 91.77%\n",
            "Epoch [2035/5000], Training Loss: 0.1760, Training Accuracy: 91.77%\n",
            "Epoch [2036/5000], Training Loss: 0.1760, Training Accuracy: 91.77%\n",
            "Epoch [2037/5000], Training Loss: 0.1760, Training Accuracy: 91.77%\n",
            "Epoch [2038/5000], Training Loss: 0.1760, Training Accuracy: 91.78%\n",
            "Epoch [2039/5000], Training Loss: 0.1760, Training Accuracy: 91.78%\n",
            "Epoch [2040/5000], Training Loss: 0.1760, Training Accuracy: 91.78%\n",
            "Epoch [2041/5000], Training Loss: 0.1760, Training Accuracy: 91.78%\n",
            "Epoch [2042/5000], Training Loss: 0.1760, Training Accuracy: 91.78%\n",
            "Epoch [2043/5000], Training Loss: 0.1760, Training Accuracy: 91.78%\n",
            "Epoch [2044/5000], Training Loss: 0.1760, Training Accuracy: 91.78%\n",
            "Epoch [2045/5000], Training Loss: 0.1760, Training Accuracy: 91.78%\n",
            "Epoch [2046/5000], Training Loss: 0.1760, Training Accuracy: 91.78%\n",
            "Epoch [2047/5000], Training Loss: 0.1760, Training Accuracy: 91.78%\n",
            "Epoch [2048/5000], Training Loss: 0.1760, Training Accuracy: 91.78%\n",
            "Epoch [2049/5000], Training Loss: 0.1760, Training Accuracy: 91.78%\n",
            "Epoch [2050/5000], Training Loss: 0.1760, Training Accuracy: 91.78%\n",
            "Epoch [2051/5000], Training Loss: 0.1760, Training Accuracy: 91.78%\n",
            "Epoch [2052/5000], Training Loss: 0.1760, Training Accuracy: 91.78%\n",
            "Epoch [2053/5000], Training Loss: 0.1760, Training Accuracy: 91.78%\n",
            "Epoch [2054/5000], Training Loss: 0.1760, Training Accuracy: 91.78%\n",
            "Epoch [2055/5000], Training Loss: 0.1760, Training Accuracy: 91.78%\n",
            "Epoch [2056/5000], Training Loss: 0.1760, Training Accuracy: 91.78%\n",
            "Epoch [2057/5000], Training Loss: 0.1760, Training Accuracy: 91.78%\n",
            "Epoch [2058/5000], Training Loss: 0.1759, Training Accuracy: 91.78%\n",
            "Epoch [2059/5000], Training Loss: 0.1759, Training Accuracy: 91.78%\n",
            "Epoch [2060/5000], Training Loss: 0.1759, Training Accuracy: 91.78%\n",
            "Epoch [2061/5000], Training Loss: 0.1759, Training Accuracy: 91.78%\n",
            "Epoch [2062/5000], Training Loss: 0.1759, Training Accuracy: 91.78%\n",
            "Epoch [2063/5000], Training Loss: 0.1759, Training Accuracy: 91.78%\n",
            "Epoch [2064/5000], Training Loss: 0.1759, Training Accuracy: 91.78%\n",
            "Epoch [2065/5000], Training Loss: 0.1759, Training Accuracy: 91.78%\n",
            "Epoch [2066/5000], Training Loss: 0.1759, Training Accuracy: 91.78%\n",
            "Epoch [2067/5000], Training Loss: 0.1759, Training Accuracy: 91.78%\n",
            "Epoch [2068/5000], Training Loss: 0.1759, Training Accuracy: 91.78%\n",
            "Epoch [2069/5000], Training Loss: 0.1759, Training Accuracy: 91.78%\n",
            "Epoch [2070/5000], Training Loss: 0.1759, Training Accuracy: 91.78%\n",
            "Epoch [2071/5000], Training Loss: 0.1759, Training Accuracy: 91.78%\n",
            "Epoch [2072/5000], Training Loss: 0.1759, Training Accuracy: 91.78%\n",
            "Epoch [2073/5000], Training Loss: 0.1759, Training Accuracy: 91.78%\n",
            "Epoch [2074/5000], Training Loss: 0.1759, Training Accuracy: 91.78%\n",
            "Epoch [2075/5000], Training Loss: 0.1759, Training Accuracy: 91.78%\n",
            "Epoch [2076/5000], Training Loss: 0.1759, Training Accuracy: 91.78%\n",
            "Epoch [2077/5000], Training Loss: 0.1759, Training Accuracy: 91.78%\n",
            "Epoch [2078/5000], Training Loss: 0.1759, Training Accuracy: 91.78%\n",
            "Epoch [2079/5000], Training Loss: 0.1759, Training Accuracy: 91.78%\n",
            "Epoch [2080/5000], Training Loss: 0.1759, Training Accuracy: 91.78%\n",
            "Epoch [2081/5000], Training Loss: 0.1759, Training Accuracy: 91.78%\n",
            "Epoch [2082/5000], Training Loss: 0.1759, Training Accuracy: 91.78%\n",
            "Epoch [2083/5000], Training Loss: 0.1759, Training Accuracy: 91.78%\n",
            "Epoch [2084/5000], Training Loss: 0.1759, Training Accuracy: 91.78%\n",
            "Epoch [2085/5000], Training Loss: 0.1759, Training Accuracy: 91.78%\n",
            "Epoch [2086/5000], Training Loss: 0.1759, Training Accuracy: 91.78%\n",
            "Epoch [2087/5000], Training Loss: 0.1759, Training Accuracy: 91.78%\n",
            "Epoch [2088/5000], Training Loss: 0.1759, Training Accuracy: 91.78%\n",
            "Epoch [2089/5000], Training Loss: 0.1759, Training Accuracy: 91.78%\n",
            "Epoch [2090/5000], Training Loss: 0.1759, Training Accuracy: 91.78%\n",
            "Epoch [2091/5000], Training Loss: 0.1759, Training Accuracy: 91.78%\n",
            "Epoch [2092/5000], Training Loss: 0.1759, Training Accuracy: 91.78%\n",
            "Epoch [2093/5000], Training Loss: 0.1759, Training Accuracy: 91.78%\n",
            "Epoch [2094/5000], Training Loss: 0.1759, Training Accuracy: 91.78%\n",
            "Epoch [2095/5000], Training Loss: 0.1759, Training Accuracy: 91.78%\n",
            "Epoch [2096/5000], Training Loss: 0.1759, Training Accuracy: 91.78%\n",
            "Epoch [2097/5000], Training Loss: 0.1759, Training Accuracy: 91.78%\n",
            "Epoch [2098/5000], Training Loss: 0.1759, Training Accuracy: 91.78%\n",
            "Epoch [2099/5000], Training Loss: 0.1759, Training Accuracy: 91.78%\n",
            "Epoch [2100/5000], Training Loss: 0.1759, Training Accuracy: 91.78%\n",
            "Epoch [2101/5000], Training Loss: 0.1759, Training Accuracy: 91.78%\n",
            "Epoch [2102/5000], Training Loss: 0.1759, Training Accuracy: 91.78%\n",
            "Epoch [2103/5000], Training Loss: 0.1759, Training Accuracy: 91.78%\n",
            "Epoch [2104/5000], Training Loss: 0.1759, Training Accuracy: 91.78%\n",
            "Epoch [2105/5000], Training Loss: 0.1759, Training Accuracy: 91.78%\n",
            "Epoch [2106/5000], Training Loss: 0.1759, Training Accuracy: 91.78%\n",
            "Epoch [2107/5000], Training Loss: 0.1759, Training Accuracy: 91.78%\n",
            "Epoch [2108/5000], Training Loss: 0.1759, Training Accuracy: 91.78%\n",
            "Epoch [2109/5000], Training Loss: 0.1759, Training Accuracy: 91.78%\n",
            "Epoch [2110/5000], Training Loss: 0.1759, Training Accuracy: 91.78%\n",
            "Epoch [2111/5000], Training Loss: 0.1758, Training Accuracy: 91.78%\n",
            "Epoch [2112/5000], Training Loss: 0.1758, Training Accuracy: 91.78%\n",
            "Epoch [2113/5000], Training Loss: 0.1758, Training Accuracy: 91.78%\n",
            "Epoch [2114/5000], Training Loss: 0.1758, Training Accuracy: 91.78%\n",
            "Epoch [2115/5000], Training Loss: 0.1758, Training Accuracy: 91.78%\n",
            "Epoch [2116/5000], Training Loss: 0.1758, Training Accuracy: 91.78%\n",
            "Epoch [2117/5000], Training Loss: 0.1758, Training Accuracy: 91.78%\n",
            "Epoch [2118/5000], Training Loss: 0.1758, Training Accuracy: 91.78%\n",
            "Epoch [2119/5000], Training Loss: 0.1758, Training Accuracy: 91.78%\n",
            "Epoch [2120/5000], Training Loss: 0.1758, Training Accuracy: 91.78%\n",
            "Epoch [2121/5000], Training Loss: 0.1758, Training Accuracy: 91.78%\n",
            "Epoch [2122/5000], Training Loss: 0.1758, Training Accuracy: 91.78%\n",
            "Epoch [2123/5000], Training Loss: 0.1758, Training Accuracy: 91.78%\n",
            "Epoch [2124/5000], Training Loss: 0.1758, Training Accuracy: 91.78%\n",
            "Epoch [2125/5000], Training Loss: 0.1758, Training Accuracy: 91.78%\n",
            "Epoch [2126/5000], Training Loss: 0.1758, Training Accuracy: 91.78%\n",
            "Epoch [2127/5000], Training Loss: 0.1758, Training Accuracy: 91.78%\n",
            "Epoch [2128/5000], Training Loss: 0.1758, Training Accuracy: 91.78%\n",
            "Epoch [2129/5000], Training Loss: 0.1758, Training Accuracy: 91.78%\n",
            "Epoch [2130/5000], Training Loss: 0.1758, Training Accuracy: 91.78%\n",
            "Epoch [2131/5000], Training Loss: 0.1758, Training Accuracy: 91.78%\n",
            "Epoch [2132/5000], Training Loss: 0.1758, Training Accuracy: 91.78%\n",
            "Epoch [2133/5000], Training Loss: 0.1758, Training Accuracy: 91.78%\n",
            "Epoch [2134/5000], Training Loss: 0.1758, Training Accuracy: 91.78%\n",
            "Epoch [2135/5000], Training Loss: 0.1758, Training Accuracy: 91.78%\n",
            "Epoch [2136/5000], Training Loss: 0.1758, Training Accuracy: 91.78%\n",
            "Epoch [2137/5000], Training Loss: 0.1758, Training Accuracy: 91.78%\n",
            "Epoch [2138/5000], Training Loss: 0.1758, Training Accuracy: 91.78%\n",
            "Epoch [2139/5000], Training Loss: 0.1758, Training Accuracy: 91.78%\n",
            "Epoch [2140/5000], Training Loss: 0.1758, Training Accuracy: 91.78%\n",
            "Epoch [2141/5000], Training Loss: 0.1758, Training Accuracy: 91.78%\n",
            "Epoch [2142/5000], Training Loss: 0.1758, Training Accuracy: 91.78%\n",
            "Epoch [2143/5000], Training Loss: 0.1758, Training Accuracy: 91.78%\n",
            "Epoch [2144/5000], Training Loss: 0.1758, Training Accuracy: 91.78%\n",
            "Epoch [2145/5000], Training Loss: 0.1758, Training Accuracy: 91.78%\n",
            "Epoch [2146/5000], Training Loss: 0.1758, Training Accuracy: 91.78%\n",
            "Epoch [2147/5000], Training Loss: 0.1758, Training Accuracy: 91.78%\n",
            "Epoch [2148/5000], Training Loss: 0.1758, Training Accuracy: 91.78%\n",
            "Epoch [2149/5000], Training Loss: 0.1758, Training Accuracy: 91.78%\n",
            "Epoch [2150/5000], Training Loss: 0.1758, Training Accuracy: 91.78%\n",
            "Epoch [2151/5000], Training Loss: 0.1758, Training Accuracy: 91.78%\n",
            "Epoch [2152/5000], Training Loss: 0.1758, Training Accuracy: 91.78%\n",
            "Epoch [2153/5000], Training Loss: 0.1758, Training Accuracy: 91.78%\n",
            "Epoch [2154/5000], Training Loss: 0.1758, Training Accuracy: 91.78%\n",
            "Epoch [2155/5000], Training Loss: 0.1758, Training Accuracy: 91.78%\n",
            "Epoch [2156/5000], Training Loss: 0.1758, Training Accuracy: 91.78%\n",
            "Epoch [2157/5000], Training Loss: 0.1758, Training Accuracy: 91.78%\n",
            "Epoch [2158/5000], Training Loss: 0.1758, Training Accuracy: 91.78%\n",
            "Epoch [2159/5000], Training Loss: 0.1758, Training Accuracy: 91.78%\n",
            "Epoch [2160/5000], Training Loss: 0.1758, Training Accuracy: 91.78%\n",
            "Epoch [2161/5000], Training Loss: 0.1758, Training Accuracy: 91.78%\n",
            "Epoch [2162/5000], Training Loss: 0.1758, Training Accuracy: 91.78%\n",
            "Epoch [2163/5000], Training Loss: 0.1758, Training Accuracy: 91.78%\n",
            "Epoch [2164/5000], Training Loss: 0.1758, Training Accuracy: 91.78%\n",
            "Epoch [2165/5000], Training Loss: 0.1758, Training Accuracy: 91.78%\n",
            "Epoch [2166/5000], Training Loss: 0.1758, Training Accuracy: 91.78%\n",
            "Epoch [2167/5000], Training Loss: 0.1758, Training Accuracy: 91.78%\n",
            "Epoch [2168/5000], Training Loss: 0.1757, Training Accuracy: 91.78%\n",
            "Epoch [2169/5000], Training Loss: 0.1757, Training Accuracy: 91.78%\n",
            "Epoch [2170/5000], Training Loss: 0.1757, Training Accuracy: 91.78%\n",
            "Epoch [2171/5000], Training Loss: 0.1757, Training Accuracy: 91.78%\n",
            "Epoch [2172/5000], Training Loss: 0.1757, Training Accuracy: 91.78%\n",
            "Epoch [2173/5000], Training Loss: 0.1757, Training Accuracy: 91.78%\n",
            "Epoch [2174/5000], Training Loss: 0.1757, Training Accuracy: 91.78%\n",
            "Epoch [2175/5000], Training Loss: 0.1757, Training Accuracy: 91.78%\n",
            "Epoch [2176/5000], Training Loss: 0.1757, Training Accuracy: 91.78%\n",
            "Epoch [2177/5000], Training Loss: 0.1757, Training Accuracy: 91.78%\n",
            "Epoch [2178/5000], Training Loss: 0.1757, Training Accuracy: 91.78%\n",
            "Epoch [2179/5000], Training Loss: 0.1757, Training Accuracy: 91.78%\n",
            "Epoch [2180/5000], Training Loss: 0.1757, Training Accuracy: 91.78%\n",
            "Epoch [2181/5000], Training Loss: 0.1757, Training Accuracy: 91.78%\n",
            "Epoch [2182/5000], Training Loss: 0.1757, Training Accuracy: 91.78%\n",
            "Epoch [2183/5000], Training Loss: 0.1757, Training Accuracy: 91.78%\n",
            "Epoch [2184/5000], Training Loss: 0.1757, Training Accuracy: 91.78%\n",
            "Epoch [2185/5000], Training Loss: 0.1757, Training Accuracy: 91.78%\n",
            "Epoch [2186/5000], Training Loss: 0.1757, Training Accuracy: 91.78%\n",
            "Epoch [2187/5000], Training Loss: 0.1757, Training Accuracy: 91.78%\n",
            "Epoch [2188/5000], Training Loss: 0.1757, Training Accuracy: 91.78%\n",
            "Epoch [2189/5000], Training Loss: 0.1757, Training Accuracy: 91.78%\n",
            "Epoch [2190/5000], Training Loss: 0.1757, Training Accuracy: 91.78%\n",
            "Epoch [2191/5000], Training Loss: 0.1757, Training Accuracy: 91.78%\n",
            "Epoch [2192/5000], Training Loss: 0.1757, Training Accuracy: 91.78%\n",
            "Epoch [2193/5000], Training Loss: 0.1757, Training Accuracy: 91.78%\n",
            "Epoch [2194/5000], Training Loss: 0.1757, Training Accuracy: 91.78%\n",
            "Epoch [2195/5000], Training Loss: 0.1757, Training Accuracy: 91.78%\n",
            "Epoch [2196/5000], Training Loss: 0.1757, Training Accuracy: 91.78%\n",
            "Epoch [2197/5000], Training Loss: 0.1757, Training Accuracy: 91.78%\n",
            "Epoch [2198/5000], Training Loss: 0.1757, Training Accuracy: 91.78%\n",
            "Epoch [2199/5000], Training Loss: 0.1757, Training Accuracy: 91.78%\n",
            "Epoch [2200/5000], Training Loss: 0.1757, Training Accuracy: 91.78%\n",
            "Epoch [2201/5000], Training Loss: 0.1757, Training Accuracy: 91.78%\n",
            "Epoch [2202/5000], Training Loss: 0.1757, Training Accuracy: 91.78%\n",
            "Epoch [2203/5000], Training Loss: 0.1757, Training Accuracy: 91.78%\n",
            "Epoch [2204/5000], Training Loss: 0.1757, Training Accuracy: 91.78%\n",
            "Epoch [2205/5000], Training Loss: 0.1757, Training Accuracy: 91.78%\n",
            "Epoch [2206/5000], Training Loss: 0.1757, Training Accuracy: 91.78%\n",
            "Epoch [2207/5000], Training Loss: 0.1757, Training Accuracy: 91.78%\n",
            "Epoch [2208/5000], Training Loss: 0.1757, Training Accuracy: 91.78%\n",
            "Epoch [2209/5000], Training Loss: 0.1757, Training Accuracy: 91.78%\n",
            "Epoch [2210/5000], Training Loss: 0.1757, Training Accuracy: 91.78%\n",
            "Epoch [2211/5000], Training Loss: 0.1757, Training Accuracy: 91.78%\n",
            "Epoch [2212/5000], Training Loss: 0.1757, Training Accuracy: 91.78%\n",
            "Epoch [2213/5000], Training Loss: 0.1757, Training Accuracy: 91.78%\n",
            "Epoch [2214/5000], Training Loss: 0.1757, Training Accuracy: 91.78%\n",
            "Epoch [2215/5000], Training Loss: 0.1757, Training Accuracy: 91.78%\n",
            "Epoch [2216/5000], Training Loss: 0.1757, Training Accuracy: 91.78%\n",
            "Epoch [2217/5000], Training Loss: 0.1757, Training Accuracy: 91.78%\n",
            "Epoch [2218/5000], Training Loss: 0.1757, Training Accuracy: 91.78%\n",
            "Epoch [2219/5000], Training Loss: 0.1757, Training Accuracy: 91.78%\n",
            "Epoch [2220/5000], Training Loss: 0.1757, Training Accuracy: 91.78%\n",
            "Epoch [2221/5000], Training Loss: 0.1757, Training Accuracy: 91.78%\n",
            "Epoch [2222/5000], Training Loss: 0.1757, Training Accuracy: 91.78%\n",
            "Epoch [2223/5000], Training Loss: 0.1757, Training Accuracy: 91.78%\n",
            "Epoch [2224/5000], Training Loss: 0.1757, Training Accuracy: 91.78%\n",
            "Epoch [2225/5000], Training Loss: 0.1757, Training Accuracy: 91.78%\n",
            "Epoch [2226/5000], Training Loss: 0.1757, Training Accuracy: 91.78%\n",
            "Epoch [2227/5000], Training Loss: 0.1757, Training Accuracy: 91.78%\n",
            "Epoch [2228/5000], Training Loss: 0.1757, Training Accuracy: 91.78%\n",
            "Epoch [2229/5000], Training Loss: 0.1757, Training Accuracy: 91.78%\n",
            "Epoch [2230/5000], Training Loss: 0.1757, Training Accuracy: 91.78%\n",
            "Epoch [2231/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2232/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2233/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2234/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2235/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2236/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2237/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2238/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2239/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2240/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2241/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2242/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2243/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2244/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2245/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2246/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2247/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2248/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2249/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2250/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2251/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2252/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2253/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2254/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2255/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2256/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2257/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2258/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2259/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2260/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2261/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2262/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2263/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2264/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2265/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2266/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2267/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2268/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2269/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2270/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2271/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2272/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2273/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2274/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2275/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2276/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2277/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2278/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2279/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2280/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2281/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2282/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2283/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2284/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2285/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2286/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2287/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2288/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2289/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2290/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2291/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2292/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2293/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2294/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2295/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2296/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2297/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2298/5000], Training Loss: 0.1756, Training Accuracy: 91.78%\n",
            "Epoch [2299/5000], Training Loss: 0.1755, Training Accuracy: 91.78%\n",
            "Epoch [2300/5000], Training Loss: 0.1755, Training Accuracy: 91.78%\n",
            "Epoch [2301/5000], Training Loss: 0.1755, Training Accuracy: 91.78%\n",
            "Epoch [2302/5000], Training Loss: 0.1755, Training Accuracy: 91.78%\n",
            "Epoch [2303/5000], Training Loss: 0.1755, Training Accuracy: 91.78%\n",
            "Epoch [2304/5000], Training Loss: 0.1755, Training Accuracy: 91.78%\n",
            "Epoch [2305/5000], Training Loss: 0.1755, Training Accuracy: 91.78%\n",
            "Epoch [2306/5000], Training Loss: 0.1755, Training Accuracy: 91.78%\n",
            "Epoch [2307/5000], Training Loss: 0.1755, Training Accuracy: 91.78%\n",
            "Epoch [2308/5000], Training Loss: 0.1755, Training Accuracy: 91.78%\n",
            "Epoch [2309/5000], Training Loss: 0.1755, Training Accuracy: 91.78%\n",
            "Epoch [2310/5000], Training Loss: 0.1755, Training Accuracy: 91.78%\n",
            "Epoch [2311/5000], Training Loss: 0.1755, Training Accuracy: 91.78%\n",
            "Epoch [2312/5000], Training Loss: 0.1755, Training Accuracy: 91.78%\n",
            "Epoch [2313/5000], Training Loss: 0.1755, Training Accuracy: 91.78%\n",
            "Epoch [2314/5000], Training Loss: 0.1755, Training Accuracy: 91.78%\n",
            "Epoch [2315/5000], Training Loss: 0.1755, Training Accuracy: 91.78%\n",
            "Epoch [2316/5000], Training Loss: 0.1755, Training Accuracy: 91.78%\n",
            "Epoch [2317/5000], Training Loss: 0.1755, Training Accuracy: 91.78%\n",
            "Epoch [2318/5000], Training Loss: 0.1755, Training Accuracy: 91.78%\n",
            "Epoch [2319/5000], Training Loss: 0.1755, Training Accuracy: 91.78%\n",
            "Epoch [2320/5000], Training Loss: 0.1755, Training Accuracy: 91.78%\n",
            "Epoch [2321/5000], Training Loss: 0.1755, Training Accuracy: 91.78%\n",
            "Epoch [2322/5000], Training Loss: 0.1755, Training Accuracy: 91.78%\n",
            "Epoch [2323/5000], Training Loss: 0.1755, Training Accuracy: 91.78%\n",
            "Epoch [2324/5000], Training Loss: 0.1755, Training Accuracy: 91.78%\n",
            "Epoch [2325/5000], Training Loss: 0.1755, Training Accuracy: 91.78%\n",
            "Epoch [2326/5000], Training Loss: 0.1755, Training Accuracy: 91.78%\n",
            "Epoch [2327/5000], Training Loss: 0.1755, Training Accuracy: 91.62%\n",
            "Epoch [2328/5000], Training Loss: 0.1755, Training Accuracy: 91.62%\n",
            "Epoch [2329/5000], Training Loss: 0.1755, Training Accuracy: 91.62%\n",
            "Epoch [2330/5000], Training Loss: 0.1755, Training Accuracy: 91.62%\n",
            "Epoch [2331/5000], Training Loss: 0.1755, Training Accuracy: 91.62%\n",
            "Epoch [2332/5000], Training Loss: 0.1755, Training Accuracy: 91.62%\n",
            "Epoch [2333/5000], Training Loss: 0.1755, Training Accuracy: 91.62%\n",
            "Epoch [2334/5000], Training Loss: 0.1755, Training Accuracy: 91.62%\n",
            "Epoch [2335/5000], Training Loss: 0.1755, Training Accuracy: 91.62%\n",
            "Epoch [2336/5000], Training Loss: 0.1755, Training Accuracy: 91.62%\n",
            "Epoch [2337/5000], Training Loss: 0.1755, Training Accuracy: 91.62%\n",
            "Epoch [2338/5000], Training Loss: 0.1755, Training Accuracy: 91.62%\n",
            "Epoch [2339/5000], Training Loss: 0.1755, Training Accuracy: 91.62%\n",
            "Epoch [2340/5000], Training Loss: 0.1755, Training Accuracy: 91.62%\n",
            "Epoch [2341/5000], Training Loss: 0.1755, Training Accuracy: 91.62%\n",
            "Epoch [2342/5000], Training Loss: 0.1755, Training Accuracy: 91.62%\n",
            "Epoch [2343/5000], Training Loss: 0.1755, Training Accuracy: 91.62%\n",
            "Epoch [2344/5000], Training Loss: 0.1755, Training Accuracy: 91.62%\n",
            "Epoch [2345/5000], Training Loss: 0.1755, Training Accuracy: 91.62%\n",
            "Epoch [2346/5000], Training Loss: 0.1755, Training Accuracy: 91.62%\n",
            "Epoch [2347/5000], Training Loss: 0.1755, Training Accuracy: 91.62%\n",
            "Epoch [2348/5000], Training Loss: 0.1755, Training Accuracy: 91.62%\n",
            "Epoch [2349/5000], Training Loss: 0.1755, Training Accuracy: 91.62%\n",
            "Epoch [2350/5000], Training Loss: 0.1755, Training Accuracy: 91.62%\n",
            "Epoch [2351/5000], Training Loss: 0.1755, Training Accuracy: 91.62%\n",
            "Epoch [2352/5000], Training Loss: 0.1755, Training Accuracy: 91.62%\n",
            "Epoch [2353/5000], Training Loss: 0.1755, Training Accuracy: 91.62%\n",
            "Epoch [2354/5000], Training Loss: 0.1755, Training Accuracy: 91.62%\n",
            "Epoch [2355/5000], Training Loss: 0.1755, Training Accuracy: 91.62%\n",
            "Epoch [2356/5000], Training Loss: 0.1755, Training Accuracy: 91.62%\n",
            "Epoch [2357/5000], Training Loss: 0.1755, Training Accuracy: 91.62%\n",
            "Epoch [2358/5000], Training Loss: 0.1755, Training Accuracy: 91.62%\n",
            "Epoch [2359/5000], Training Loss: 0.1755, Training Accuracy: 91.62%\n",
            "Epoch [2360/5000], Training Loss: 0.1755, Training Accuracy: 91.62%\n",
            "Epoch [2361/5000], Training Loss: 0.1755, Training Accuracy: 91.62%\n",
            "Epoch [2362/5000], Training Loss: 0.1755, Training Accuracy: 91.62%\n",
            "Epoch [2363/5000], Training Loss: 0.1755, Training Accuracy: 91.62%\n",
            "Epoch [2364/5000], Training Loss: 0.1755, Training Accuracy: 91.62%\n",
            "Epoch [2365/5000], Training Loss: 0.1755, Training Accuracy: 91.62%\n",
            "Epoch [2366/5000], Training Loss: 0.1755, Training Accuracy: 91.62%\n",
            "Epoch [2367/5000], Training Loss: 0.1755, Training Accuracy: 91.62%\n",
            "Epoch [2368/5000], Training Loss: 0.1755, Training Accuracy: 91.62%\n",
            "Epoch [2369/5000], Training Loss: 0.1755, Training Accuracy: 91.62%\n",
            "Epoch [2370/5000], Training Loss: 0.1755, Training Accuracy: 91.62%\n",
            "Epoch [2371/5000], Training Loss: 0.1755, Training Accuracy: 91.62%\n",
            "Epoch [2372/5000], Training Loss: 0.1755, Training Accuracy: 91.62%\n",
            "Epoch [2373/5000], Training Loss: 0.1755, Training Accuracy: 91.62%\n",
            "Epoch [2374/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2375/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2376/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2377/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2378/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2379/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2380/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2381/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2382/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2383/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2384/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2385/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2386/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2387/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2388/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2389/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2390/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2391/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2392/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2393/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2394/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2395/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2396/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2397/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2398/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2399/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2400/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2401/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2402/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2403/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2404/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2405/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2406/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2407/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2408/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2409/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2410/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2411/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2412/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2413/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2414/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2415/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2416/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2417/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2418/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2419/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2420/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2421/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2422/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2423/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2424/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2425/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2426/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2427/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2428/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2429/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2430/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2431/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2432/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2433/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2434/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2435/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2436/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2437/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2438/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2439/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2440/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2441/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2442/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2443/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2444/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2445/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2446/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2447/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2448/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2449/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2450/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2451/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2452/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2453/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2454/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2455/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2456/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2457/5000], Training Loss: 0.1754, Training Accuracy: 91.62%\n",
            "Epoch [2458/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2459/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2460/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2461/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2462/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2463/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2464/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2465/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2466/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2467/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2468/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2469/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2470/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2471/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2472/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2473/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2474/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2475/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2476/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2477/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2478/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2479/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2480/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2481/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2482/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2483/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2484/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2485/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2486/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2487/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2488/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2489/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2490/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2491/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2492/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2493/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2494/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2495/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2496/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2497/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2498/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2499/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2500/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2501/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2502/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2503/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2504/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2505/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2506/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2507/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2508/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2509/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2510/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2511/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2512/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2513/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2514/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2515/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2516/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2517/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2518/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2519/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2520/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2521/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2522/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2523/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2524/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2525/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2526/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2527/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2528/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2529/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2530/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2531/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2532/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2533/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2534/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2535/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2536/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2537/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2538/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2539/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2540/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2541/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2542/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2543/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2544/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2545/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2546/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2547/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2548/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2549/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2550/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2551/5000], Training Loss: 0.1753, Training Accuracy: 91.62%\n",
            "Epoch [2552/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2553/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2554/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2555/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2556/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2557/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2558/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2559/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2560/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2561/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2562/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2563/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2564/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2565/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2566/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2567/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2568/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2569/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2570/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2571/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2572/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2573/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2574/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2575/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2576/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2577/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2578/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2579/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2580/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2581/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2582/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2583/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2584/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2585/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2586/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2587/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2588/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2589/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2590/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2591/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2592/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2593/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2594/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2595/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2596/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2597/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2598/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2599/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2600/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2601/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2602/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2603/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2604/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2605/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2606/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2607/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2608/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2609/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2610/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2611/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2612/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2613/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2614/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2615/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2616/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2617/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2618/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2619/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2620/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2621/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2622/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2623/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2624/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2625/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2626/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2627/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2628/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2629/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2630/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2631/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2632/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2633/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2634/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2635/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2636/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2637/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2638/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2639/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2640/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2641/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2642/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2643/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2644/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2645/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2646/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2647/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2648/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2649/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2650/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2651/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2652/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2653/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2654/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2655/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2656/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2657/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2658/5000], Training Loss: 0.1752, Training Accuracy: 91.62%\n",
            "Epoch [2659/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2660/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2661/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2662/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2663/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2664/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2665/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2666/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2667/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2668/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2669/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2670/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2671/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2672/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2673/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2674/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2675/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2676/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2677/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2678/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2679/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2680/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2681/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2682/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2683/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2684/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2685/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2686/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2687/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2688/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2689/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2690/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2691/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2692/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2693/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2694/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2695/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2696/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2697/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2698/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2699/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2700/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2701/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2702/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2703/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2704/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2705/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2706/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2707/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2708/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2709/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2710/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2711/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2712/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2713/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2714/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2715/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2716/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2717/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2718/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2719/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2720/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2721/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2722/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2723/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2724/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2725/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2726/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2727/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2728/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2729/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2730/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2731/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2732/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2733/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2734/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2735/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2736/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2737/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2738/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2739/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2740/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2741/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2742/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2743/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2744/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2745/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2746/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2747/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2748/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2749/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2750/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2751/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2752/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2753/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2754/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2755/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2756/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2757/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2758/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2759/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2760/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2761/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2762/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2763/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2764/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2765/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2766/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2767/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2768/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2769/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2770/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2771/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2772/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2773/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2774/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2775/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2776/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2777/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2778/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2779/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2780/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2781/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2782/5000], Training Loss: 0.1751, Training Accuracy: 91.62%\n",
            "Epoch [2783/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2784/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2785/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2786/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2787/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2788/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2789/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2790/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2791/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2792/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2793/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2794/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2795/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2796/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2797/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2798/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2799/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2800/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2801/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2802/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2803/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2804/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2805/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2806/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2807/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2808/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2809/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2810/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2811/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2812/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2813/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2814/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2815/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2816/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2817/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2818/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2819/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2820/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2821/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2822/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2823/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2824/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2825/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2826/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2827/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2828/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2829/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2830/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2831/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2832/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2833/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2834/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2835/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2836/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2837/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2838/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2839/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2840/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2841/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2842/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2843/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2844/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2845/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2846/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2847/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2848/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2849/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2850/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2851/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2852/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2853/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2854/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2855/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2856/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2857/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2858/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2859/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2860/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2861/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2862/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2863/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2864/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2865/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2866/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2867/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2868/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2869/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2870/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2871/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2872/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2873/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2874/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2875/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2876/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2877/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2878/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2879/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2880/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2881/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2882/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2883/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2884/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2885/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2886/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2887/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2888/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2889/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2890/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2891/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2892/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2893/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2894/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2895/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2896/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2897/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2898/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2899/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2900/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2901/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2902/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2903/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2904/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2905/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2906/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2907/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2908/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2909/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2910/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2911/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2912/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2913/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2914/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2915/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2916/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2917/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2918/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2919/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2920/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2921/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2922/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2923/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2924/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2925/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2926/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2927/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2928/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2929/5000], Training Loss: 0.1750, Training Accuracy: 91.62%\n",
            "Epoch [2930/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2931/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2932/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2933/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2934/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2935/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2936/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2937/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2938/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2939/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2940/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2941/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2942/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2943/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2944/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2945/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2946/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2947/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2948/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2949/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2950/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2951/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2952/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2953/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2954/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2955/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2956/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2957/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2958/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2959/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2960/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2961/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2962/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2963/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2964/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2965/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2966/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2967/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2968/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2969/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2970/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2971/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2972/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2973/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2974/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2975/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2976/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2977/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2978/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2979/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2980/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2981/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2982/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2983/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2984/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2985/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2986/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2987/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2988/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2989/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2990/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2991/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2992/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2993/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2994/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2995/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2996/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2997/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2998/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [2999/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3000/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3001/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3002/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3003/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3004/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3005/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3006/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3007/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3008/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3009/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3010/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3011/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3012/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3013/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3014/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3015/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3016/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3017/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3018/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3019/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3020/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3021/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3022/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3023/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3024/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3025/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3026/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3027/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3028/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3029/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3030/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3031/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3032/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3033/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3034/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3035/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3036/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3037/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3038/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3039/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3040/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3041/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3042/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3043/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3044/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3045/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3046/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3047/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3048/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3049/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3050/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3051/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3052/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3053/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3054/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3055/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3056/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3057/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3058/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3059/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3060/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3061/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3062/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3063/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3064/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3065/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3066/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3067/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3068/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3069/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3070/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3071/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3072/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3073/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3074/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3075/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3076/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3077/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3078/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3079/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3080/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3081/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3082/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3083/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3084/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3085/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3086/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3087/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3088/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3089/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3090/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3091/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3092/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3093/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3094/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3095/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3096/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3097/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3098/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3099/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3100/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3101/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3102/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3103/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3104/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3105/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3106/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3107/5000], Training Loss: 0.1749, Training Accuracy: 91.62%\n",
            "Epoch [3108/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3109/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3110/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3111/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3112/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3113/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3114/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3115/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3116/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3117/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3118/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3119/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3120/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3121/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3122/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3123/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3124/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3125/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3126/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3127/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3128/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3129/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3130/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3131/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3132/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3133/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3134/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3135/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3136/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3137/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3138/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3139/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3140/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3141/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3142/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3143/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3144/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3145/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3146/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3147/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3148/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3149/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3150/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3151/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3152/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3153/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3154/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3155/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3156/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3157/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3158/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3159/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3160/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3161/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3162/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3163/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3164/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3165/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3166/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3167/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3168/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3169/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3170/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3171/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3172/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3173/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3174/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3175/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3176/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3177/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3178/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3179/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3180/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3181/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3182/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3183/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3184/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3185/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3186/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3187/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3188/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3189/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3190/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3191/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3192/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3193/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3194/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3195/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3196/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3197/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3198/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3199/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3200/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3201/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3202/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3203/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3204/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3205/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3206/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3207/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3208/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3209/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3210/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3211/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3212/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3213/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3214/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3215/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3216/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3217/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3218/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3219/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3220/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3221/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3222/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3223/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3224/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3225/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3226/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3227/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3228/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3229/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3230/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3231/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3232/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3233/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3234/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3235/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3236/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3237/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3238/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3239/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3240/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3241/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3242/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3243/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3244/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3245/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3246/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3247/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3248/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3249/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3250/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3251/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3252/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3253/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3254/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3255/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3256/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3257/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3258/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3259/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3260/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3261/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3262/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3263/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3264/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3265/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3266/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3267/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3268/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3269/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3270/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3271/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3272/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3273/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3274/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3275/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3276/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3277/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3278/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3279/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3280/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3281/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3282/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3283/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3284/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3285/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3286/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3287/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3288/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3289/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3290/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3291/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3292/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3293/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3294/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3295/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3296/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3297/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3298/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3299/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3300/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3301/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3302/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3303/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3304/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3305/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3306/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3307/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3308/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3309/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3310/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3311/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3312/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3313/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3314/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3315/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3316/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3317/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3318/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3319/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3320/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3321/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3322/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3323/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3324/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3325/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3326/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3327/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3328/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3329/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3330/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3331/5000], Training Loss: 0.1748, Training Accuracy: 91.62%\n",
            "Epoch [3332/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3333/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3334/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3335/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3336/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3337/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3338/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3339/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3340/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3341/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3342/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3343/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3344/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3345/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3346/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3347/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3348/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3349/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3350/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3351/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3352/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3353/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3354/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3355/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3356/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3357/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3358/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3359/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3360/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3361/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3362/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3363/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3364/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3365/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3366/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3367/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3368/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3369/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3370/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3371/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3372/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3373/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3374/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3375/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3376/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3377/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3378/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3379/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3380/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3381/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3382/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3383/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3384/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3385/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3386/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3387/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3388/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3389/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3390/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3391/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3392/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3393/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3394/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3395/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3396/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3397/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3398/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3399/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3400/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3401/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3402/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3403/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3404/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3405/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3406/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3407/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3408/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3409/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3410/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3411/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3412/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3413/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3414/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3415/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3416/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3417/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3418/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3419/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3420/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3421/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3422/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3423/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3424/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3425/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3426/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3427/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3428/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3429/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3430/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3431/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3432/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3433/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3434/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3435/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3436/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3437/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3438/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3439/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3440/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3441/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3442/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3443/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3444/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3445/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3446/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3447/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3448/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3449/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3450/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3451/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3452/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3453/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3454/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3455/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3456/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3457/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3458/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3459/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3460/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3461/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3462/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3463/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3464/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3465/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3466/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3467/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3468/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3469/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3470/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3471/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3472/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3473/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3474/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3475/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3476/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3477/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3478/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3479/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3480/5000], Training Loss: 0.1747, Training Accuracy: 91.62%\n",
            "Epoch [3481/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3482/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3483/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3484/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3485/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3486/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3487/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3488/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3489/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3490/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3491/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3492/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3493/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3494/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3495/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3496/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3497/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3498/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3499/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3500/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3501/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3502/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3503/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3504/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3505/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3506/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3507/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3508/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3509/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3510/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3511/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3512/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3513/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3514/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3515/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3516/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3517/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3518/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3519/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3520/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3521/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3522/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3523/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3524/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3525/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3526/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3527/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3528/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3529/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3530/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3531/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3532/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3533/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3534/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3535/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3536/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3537/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3538/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3539/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3540/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3541/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3542/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3543/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3544/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3545/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3546/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3547/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3548/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3549/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3550/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3551/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3552/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3553/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3554/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3555/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3556/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3557/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3558/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3559/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3560/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3561/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3562/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3563/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3564/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3565/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3566/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3567/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3568/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3569/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3570/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3571/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3572/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3573/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3574/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3575/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3576/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3577/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3578/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3579/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3580/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3581/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3582/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3583/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3584/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3585/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3586/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3587/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3588/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3589/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3590/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3591/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3592/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3593/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3594/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3595/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3596/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3597/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3598/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3599/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3600/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3601/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3602/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3603/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3604/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3605/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3606/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3607/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3608/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3609/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3610/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3611/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3612/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3613/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3614/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3615/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3616/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3617/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3618/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3619/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3620/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3621/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3622/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3623/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3624/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3625/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3626/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3627/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3628/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3629/5000], Training Loss: 0.1747, Training Accuracy: 91.68%\n",
            "Epoch [3630/5000], Training Loss: 0.1747, Training Accuracy: 91.74%\n",
            "Epoch [3631/5000], Training Loss: 0.1747, Training Accuracy: 91.74%\n",
            "Epoch [3632/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3633/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3634/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3635/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3636/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3637/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3638/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3639/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3640/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3641/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3642/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3643/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3644/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3645/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3646/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3647/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3648/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3649/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3650/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3651/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3652/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3653/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3654/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3655/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3656/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3657/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3658/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3659/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3660/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3661/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3662/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3663/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3664/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3665/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3666/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3667/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3668/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3669/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3670/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3671/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3672/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3673/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3674/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3675/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3676/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3677/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3678/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3679/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3680/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3681/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3682/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3683/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3684/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3685/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3686/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3687/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3688/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3689/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3690/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3691/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3692/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3693/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3694/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3695/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3696/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3697/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3698/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3699/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3700/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3701/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3702/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3703/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3704/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3705/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3706/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3707/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3708/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3709/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3710/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3711/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3712/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3713/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3714/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3715/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3716/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3717/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3718/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3719/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3720/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3721/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3722/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3723/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3724/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3725/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3726/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3727/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3728/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3729/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3730/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3731/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3732/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3733/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3734/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3735/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3736/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3737/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3738/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3739/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3740/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3741/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3742/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3743/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3744/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3745/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3746/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3747/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3748/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3749/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3750/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3751/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3752/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3753/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3754/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3755/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3756/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3757/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3758/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3759/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3760/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3761/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3762/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3763/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3764/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3765/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3766/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3767/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3768/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3769/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3770/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3771/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3772/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3773/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3774/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3775/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3776/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3777/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3778/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3779/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3780/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3781/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3782/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3783/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3784/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3785/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3786/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3787/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3788/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3789/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3790/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3791/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3792/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3793/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3794/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3795/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3796/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3797/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3798/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3799/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3800/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3801/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3802/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3803/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3804/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3805/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3806/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3807/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3808/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3809/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3810/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3811/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3812/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3813/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3814/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3815/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3816/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3817/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3818/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3819/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3820/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3821/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3822/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3823/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3824/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3825/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3826/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3827/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3828/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3829/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3830/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3831/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3832/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3833/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3834/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3835/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3836/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3837/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3838/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3839/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3840/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3841/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3842/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3843/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3844/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3845/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3846/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3847/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3848/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3849/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3850/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3851/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3852/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3853/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3854/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3855/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3856/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3857/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3858/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3859/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3860/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3861/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3862/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3863/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3864/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3865/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3866/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3867/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3868/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3869/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3870/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3871/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3872/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3873/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3874/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3875/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3876/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3877/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3878/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3879/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3880/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3881/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3882/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3883/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3884/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3885/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3886/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3887/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3888/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3889/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3890/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3891/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3892/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3893/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3894/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3895/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3896/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3897/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3898/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3899/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3900/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3901/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3902/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3903/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3904/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3905/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3906/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3907/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3908/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3909/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3910/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3911/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3912/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3913/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3914/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3915/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3916/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3917/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3918/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3919/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3920/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3921/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3922/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3923/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3924/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3925/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3926/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3927/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3928/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3929/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3930/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3931/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3932/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3933/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3934/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3935/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3936/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3937/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3938/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3939/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3940/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3941/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3942/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3943/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3944/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3945/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3946/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3947/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3948/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3949/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3950/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3951/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3952/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3953/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3954/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3955/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3956/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3957/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3958/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3959/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3960/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3961/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3962/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3963/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3964/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3965/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3966/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3967/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3968/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3969/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3970/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3971/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3972/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3973/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3974/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3975/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3976/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3977/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3978/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3979/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3980/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3981/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3982/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3983/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3984/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3985/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3986/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3987/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3988/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3989/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3990/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3991/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3992/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3993/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3994/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3995/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3996/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3997/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3998/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [3999/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4000/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4001/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4002/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4003/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4004/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4005/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4006/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4007/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4008/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4009/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4010/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4011/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4012/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4013/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4014/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4015/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4016/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4017/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4018/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4019/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4020/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4021/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4022/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4023/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4024/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4025/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4026/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4027/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4028/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4029/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4030/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4031/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4032/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4033/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4034/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4035/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4036/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4037/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4038/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4039/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4040/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4041/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4042/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4043/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4044/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4045/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4046/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4047/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4048/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4049/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4050/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4051/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4052/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4053/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4054/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4055/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4056/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4057/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4058/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4059/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4060/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4061/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4062/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4063/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4064/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4065/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4066/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4067/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4068/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4069/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4070/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4071/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4072/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4073/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4074/5000], Training Loss: 0.1746, Training Accuracy: 91.74%\n",
            "Epoch [4075/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4076/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4077/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4078/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4079/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4080/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4081/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4082/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4083/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4084/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4085/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4086/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4087/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4088/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4089/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4090/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4091/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4092/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4093/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4094/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4095/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4096/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4097/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4098/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4099/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4100/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4101/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4102/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4103/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4104/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4105/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4106/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4107/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4108/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4109/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4110/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4111/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4112/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4113/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4114/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4115/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4116/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4117/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4118/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4119/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4120/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4121/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4122/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4123/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4124/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4125/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4126/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4127/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4128/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4129/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4130/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4131/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4132/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4133/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4134/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4135/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4136/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4137/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4138/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4139/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4140/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4141/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4142/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4143/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4144/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4145/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4146/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4147/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4148/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4149/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4150/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4151/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4152/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4153/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4154/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4155/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4156/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4157/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4158/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4159/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4160/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4161/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4162/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4163/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4164/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4165/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4166/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4167/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4168/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4169/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4170/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4171/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4172/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4173/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4174/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4175/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4176/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4177/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4178/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4179/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4180/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4181/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4182/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4183/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4184/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4185/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4186/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4187/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4188/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4189/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4190/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4191/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4192/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4193/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4194/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4195/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4196/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4197/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4198/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4199/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4200/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4201/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4202/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4203/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4204/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4205/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4206/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4207/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4208/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4209/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4210/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4211/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4212/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4213/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4214/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4215/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4216/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4217/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4218/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4219/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4220/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4221/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4222/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4223/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4224/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4225/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4226/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4227/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4228/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4229/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4230/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4231/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4232/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4233/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4234/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4235/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4236/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4237/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4238/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4239/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4240/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4241/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4242/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4243/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4244/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4245/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4246/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4247/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4248/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4249/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4250/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4251/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4252/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4253/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4254/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4255/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4256/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4257/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4258/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4259/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4260/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4261/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4262/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4263/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4264/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4265/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4266/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4267/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4268/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4269/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4270/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4271/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4272/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4273/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4274/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4275/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4276/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4277/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4278/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4279/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4280/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4281/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4282/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4283/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4284/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4285/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4286/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4287/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4288/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4289/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4290/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4291/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4292/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4293/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4294/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4295/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4296/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4297/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4298/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4299/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4300/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4301/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4302/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4303/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4304/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4305/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4306/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4307/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4308/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4309/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4310/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4311/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4312/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4313/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4314/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4315/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4316/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4317/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4318/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4319/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4320/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4321/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4322/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4323/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4324/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4325/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4326/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4327/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4328/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4329/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4330/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4331/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4332/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4333/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4334/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4335/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4336/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4337/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4338/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4339/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4340/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4341/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4342/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4343/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4344/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4345/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4346/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4347/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4348/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4349/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4350/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4351/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4352/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4353/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4354/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4355/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4356/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4357/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4358/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4359/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4360/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4361/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4362/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4363/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4364/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4365/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4366/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4367/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4368/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4369/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4370/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4371/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4372/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4373/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4374/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4375/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4376/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4377/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4378/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4379/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4380/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4381/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4382/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4383/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4384/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4385/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4386/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4387/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4388/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4389/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4390/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4391/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4392/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4393/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4394/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4395/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4396/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4397/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4398/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4399/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4400/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4401/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4402/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4403/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4404/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4405/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4406/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4407/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4408/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4409/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4410/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4411/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4412/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4413/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4414/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4415/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4416/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4417/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4418/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4419/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4420/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4421/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4422/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4423/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4424/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4425/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4426/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4427/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4428/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4429/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4430/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4431/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4432/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4433/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4434/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4435/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4436/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4437/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4438/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4439/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4440/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4441/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4442/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4443/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4444/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4445/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4446/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4447/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4448/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4449/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4450/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4451/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4452/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4453/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4454/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4455/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4456/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4457/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4458/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4459/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4460/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4461/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4462/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4463/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4464/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4465/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4466/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4467/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4468/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4469/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4470/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4471/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4472/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4473/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4474/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4475/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4476/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4477/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4478/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4479/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4480/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4481/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4482/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4483/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4484/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4485/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4486/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4487/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4488/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4489/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4490/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4491/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4492/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4493/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4494/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4495/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4496/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4497/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4498/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4499/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4500/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4501/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4502/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4503/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4504/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4505/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4506/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4507/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4508/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4509/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4510/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4511/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4512/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4513/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4514/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4515/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4516/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4517/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4518/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4519/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4520/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4521/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4522/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4523/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4524/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4525/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4526/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4527/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4528/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4529/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4530/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4531/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4532/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4533/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4534/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4535/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4536/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4537/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4538/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4539/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4540/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4541/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4542/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4543/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4544/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4545/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4546/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4547/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4548/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4549/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4550/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4551/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4552/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4553/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4554/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4555/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4556/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4557/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4558/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4559/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4560/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4561/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4562/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4563/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4564/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4565/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4566/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4567/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4568/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4569/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4570/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4571/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4572/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4573/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4574/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4575/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4576/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4577/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4578/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4579/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4580/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4581/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4582/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4583/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4584/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4585/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4586/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4587/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4588/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4589/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4590/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4591/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4592/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4593/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4594/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4595/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4596/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4597/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4598/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4599/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4600/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4601/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4602/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4603/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4604/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4605/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4606/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4607/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4608/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4609/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4610/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4611/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4612/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4613/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4614/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4615/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4616/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4617/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4618/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4619/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4620/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4621/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4622/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4623/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4624/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4625/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4626/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4627/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4628/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4629/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4630/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4631/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4632/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4633/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4634/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4635/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4636/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4637/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4638/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4639/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4640/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4641/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4642/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4643/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4644/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4645/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4646/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4647/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4648/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4649/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4650/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4651/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4652/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4653/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4654/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4655/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4656/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4657/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4658/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4659/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4660/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4661/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4662/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4663/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4664/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4665/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4666/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4667/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4668/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4669/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4670/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4671/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4672/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4673/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4674/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4675/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4676/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4677/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4678/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4679/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4680/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4681/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4682/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4683/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4684/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4685/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4686/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4687/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4688/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4689/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4690/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4691/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4692/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4693/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4694/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4695/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4696/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4697/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4698/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4699/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4700/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4701/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4702/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4703/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4704/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4705/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4706/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4707/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4708/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4709/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4710/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4711/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4712/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4713/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4714/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4715/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4716/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4717/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4718/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4719/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4720/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4721/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4722/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4723/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4724/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4725/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4726/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4727/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4728/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4729/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4730/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4731/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4732/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4733/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4734/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4735/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4736/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4737/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4738/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4739/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4740/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4741/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4742/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4743/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4744/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4745/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4746/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4747/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4748/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4749/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4750/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4751/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4752/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4753/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4754/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4755/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4756/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4757/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4758/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4759/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4760/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4761/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4762/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4763/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4764/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4765/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4766/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4767/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4768/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4769/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4770/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4771/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4772/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4773/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4774/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4775/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4776/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4777/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4778/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4779/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4780/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4781/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4782/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4783/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4784/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4785/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4786/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4787/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4788/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4789/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4790/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4791/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4792/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4793/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4794/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4795/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4796/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4797/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4798/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4799/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4800/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4801/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4802/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4803/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4804/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4805/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4806/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4807/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4808/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4809/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4810/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4811/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4812/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4813/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4814/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4815/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4816/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4817/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4818/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4819/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4820/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4821/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4822/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4823/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4824/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4825/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4826/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4827/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4828/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4829/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4830/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4831/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4832/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4833/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4834/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4835/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4836/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4837/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4838/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4839/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4840/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4841/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4842/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4843/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4844/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4845/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4846/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4847/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4848/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4849/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4850/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4851/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4852/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4853/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4854/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4855/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4856/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4857/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4858/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4859/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4860/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4861/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4862/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4863/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4864/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4865/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4866/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4867/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4868/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4869/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4870/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4871/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4872/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4873/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4874/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4875/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4876/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4877/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4878/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4879/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4880/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4881/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4882/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4883/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4884/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4885/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4886/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4887/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4888/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4889/5000], Training Loss: 0.1745, Training Accuracy: 91.74%\n",
            "Epoch [4890/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4891/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4892/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4893/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4894/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4895/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4896/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4897/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4898/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4899/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4900/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4901/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4902/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4903/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4904/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4905/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4906/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4907/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4908/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4909/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4910/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4911/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4912/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4913/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4914/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4915/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4916/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4917/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4918/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4919/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4920/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4921/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4922/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4923/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4924/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4925/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4926/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4927/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4928/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4929/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4930/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4931/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4932/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4933/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4934/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4935/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4936/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4937/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4938/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4939/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4940/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4941/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4942/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4943/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4944/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4945/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4946/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4947/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4948/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4949/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4950/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4951/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4952/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4953/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4954/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4955/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4956/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4957/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4958/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4959/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4960/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4961/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4962/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4963/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4964/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4965/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4966/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4967/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4968/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4969/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4970/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4971/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4972/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4973/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4974/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4975/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4976/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4977/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4978/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4979/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4980/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4981/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4982/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4983/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4984/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4985/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4986/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4987/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4988/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4989/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4990/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4991/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4992/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4993/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4994/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4995/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4996/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4997/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4998/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [4999/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Epoch [5000/5000], Training Loss: 0.1744, Training Accuracy: 91.74%\n",
            "Accuracy on the test dataset: 91.80%\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x400 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Preprocessing and Logistic Regression Model\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Loading, preprocessing, and Logistic Regression Model\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Preprocessing URL data\n",
        "# Assuming 'data' is your DataFrame\n",
        "data['num_dots'] = data['url'].apply(lambda x: str(x).count('.'))\n",
        "threshold_dots = 3\n",
        "data['is_bad_url'] = data['num_dots'] > threshold_dots\n",
        "\n",
        "# Additional preprocessing for URL length\n",
        "data['url_len'] = data['url'].apply(lambda x: len(str(x)))\n",
        "threshold_length = 90\n",
        "data['is_bad_url'] = (data['url_len'] > threshold_length) | data['is_bad_url']\n",
        "\n",
        "# Features: url length and number of dots\n",
        "features = data[['url_len', 'num_dots']]\n",
        "labels = data['is_bad_url']\n",
        "\n",
        "# Encode Labels using Label Encoding\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(labels)\n",
        "\n",
        "# Convert to float and normalize to the range [0, 1]\n",
        "normalized_labels = encoded_labels.astype(float) / max(encoded_labels)\n",
        "\n",
        "# Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, normalized_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize Features using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)  # Ensure it's a column vector\n",
        "\n",
        "# Define Logistic Regression Model\n",
        "class LogisticRegressionModel(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(LogisticRegressionModel, self).__init__()\n",
        "        self.linear = nn.Linear(in_features=input_size, out_features=1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "model = LogisticRegressionModel(input_size=X_train.shape[1])\n",
        "\n",
        "# Define Loss Function and Optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1, weight_decay=0.0001)\n",
        "\n",
        "# Train the Model\n",
        "epochs = 5000\n",
        "training_losses = []  # initialize the list\n",
        "accuracy_list = []  # initialize the list for accuracy\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Forward pass\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Store the training loss for the current epoch\n",
        "    training_losses.append(loss.item())\n",
        "\n",
        "    # Calculate training accuracy\n",
        "    predictions_binary = (outputs >= 0.5).float()\n",
        "    accuracy_train = (predictions_binary == y_train_tensor).float().mean().item()\n",
        "    accuracy_list.append(accuracy_train)\n",
        "\n",
        "    # Print the current epoch, training loss, and accuracy\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Training Loss: {training_losses[-1]:.4f}, Training Accuracy: {accuracy_train * 100:.2f}%')\n",
        "\n",
        "# Evaluate on Test Data\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    predictions = model(X_test_tensor)\n",
        "    predictions_binary = (predictions >= 0.5).float()\n",
        "    accuracy = (predictions_binary == y_test_tensor).float().mean().item()\n",
        "\n",
        "print(f\"Accuracy on the test dataset: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Plot the training loss and accuracy\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Plot the training loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(1, epochs + 1), training_losses, label='Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss Over Epochs')\n",
        "plt.legend()\n",
        "\n",
        "# Plot the training accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(1, epochs + 1), accuracy_list, label='Training Accuracy', color='green')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training Accuracy Over Epochs')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "7PS-sxsp399j",
        "-zJmwfoT8Z3e",
        "369S9ZD12Zhh"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
